\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[utf8]{inputenc}
\pagenumbering{arabic}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{mathtools}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}

\input{../../style/common}

\tcbset{enhanced}

%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}

\font \sfbold=cmssbx10
\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}

\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
% \pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
	{\sf \bf \huge Exercise Collection -- #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\exlect}
  {\color{black} \hrule \section{Lecture exercises}}
  
\newcommand{\exexams}
  {\color{black} \hrule \section{Further exercises}}
  % rename so it is not immediately clear these are from past exams
  
\newcommand{\exinspo}
  {\color{black} \hrule \section{Ideas \& exercises from other sources}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1} 
	\noindent}
	{\vspace{0.5cm}}
	
\newenvironment{aufgabeexam}[3] % semester, first or second, question number
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1, #2, question #3}
	\noindent}
	{\vspace{1.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\color{gray} \refstepcounter{loes}\textbf{Solution \arabic{loes}:}
	\\ \noindent}
	{\bigskip}

\setcounter{secnumdepth}{0}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-svm.tex}
\input{../../latex-math/ml-gp.tex}

\kopf{Gaussian Processes}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{Bayesian Linear Model}{


In the Bayesian linear model, we assume that the data follows the following law:
%
$$
y = \fx + \epsilon = \thetab^T \xv + \epsilon , 
$$
%
where $\varepsilon \sim \mathcal{N}(0,\sigma^2)$ and independent of $\xv.$
%
On the data-level this corresponds to
%
\begin{eqnarray*}
%	
	\yi &=& \fxi + \epsi = \thetab^T \xi + \epsi, \quad \text{for } i \in \{1, \ldots, n\}
%	
\end{eqnarray*}
%
where $\epsi \sim \mathcal{N}(0, \sigma^2)$ are iid and all independent of the $\xi$'s.
%
In the Bayesian perspective it is assumed that the parameter vector $\thetab$ is stochastic and follows a distribution.

Assume we are interested in the so-called maximum a posteriori estimate of $\thetab,$ which is defined by
%
$$		\thetabh = \argmax_{\thetab} p(\thetab | \Xmat, \yv).	$$
%
\begin{enumerate}
%	
  \item Show that if we choose a uniform distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
%  
	$$  q(\thetab)  \propto 1, $$
%  
	then the  maximum a posteriori estimate coincides with the empirical risk minimizer for the L2-loss (over the linear models).
%
  \item Show that if we choose a Gaussian distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
  %  
  $$  q(\thetab)  \propto  \exp\biggl[-\frac{1}{2\tau^2}\thetab^\top\thetab\biggr], \qquad \tau>0, $$
  %  
  then the maximum a posteriori estimate coincides for a specific choice of $\tau$ with the regularized empirical risk minimizer for the L2-loss with L2 penalty (over the linear models), i.e., the Ridge regression.
%  
  \item Show that if we choose a Laplace distribution over the parameter vectors $\thetab$ as the prior belief, i.e.,
%  
	$$  q(\thetab)  \propto  \exp\biggl[-\frac{\sum_{i=1}^p |\thetab_i|}{\tau} \biggr], \qquad \tau>0, $$
	%  
	then the maximum a posteriori estimate coincides for a specific choice of $\tau$ with the regularized empirical risk minimizer for the L2-loss with L1 penalty (over the linear models), i.e., the Lasso regression.
\end{enumerate}
%
}

\dlz

\aufgabe{Covariance Functions}{

Consider $\Xspace = \R.$
%
The following plot shows eight functions 
%
\begin{align*}
	f:\ &\Xspace \to \R\\
		&x \mapsto f(x)
\end{align*}
%
randomly drawn from eight different Gaussian processes, each of which has a mean function of zero. 
%
\begin{figure}[h]
%	
	\centering
	\includegraphics[width=0.99\linewidth]{figure/Gaussian_processes}
%
\end{figure}

%
The covariance functions are one of those listed below.
%
Indicate which of the functions above is most likely to have been drawn from the Gaussian process $\mathcal{GP}\left(0, k(x,x') \right)$ with that covariance function.

\begin{enumerate}
%	
	\item $k(x,x') = \mathds{1}_{[ x = x' ]}   .$
%	
	\item []
	%	
	\item $k(x,x') = x\cdot x'   .$
	%	
	\item []
%		
	\item $k(x,x') = 0.5 \cdot x^2\cdot (x')^2   .$
	%	
	\item []
%		
	\item $k(x,x') = 0.5^2 \cdot \exp\left(- \frac{(x-x')^2}{0.5^2}\right)  .$
	%	
	\item []
%
	\item $k(x,x') = \cos(x-x')  .$
	%	
	\item []
	%
	\item $k(x,x') = 8^2 \cdot \exp\left(- \frac{(x-x')^2}{5}\right)  .$
	%	
	\item []
	%
	\item $k(x,x') = 25 + 25 \cdot x \cdot x' + 0.25 \cdot \exp\left(- \frac{(x-x')^2}{0.1^2}\right).$
	%	
	\item []
%	
	\item $k(x,x') = 2\cdot x^2 \cdot (x')^2+2\cdot \exp\left(- \frac{(x-x')^2}{0.1^2}\right).$
%	
	\item []
%	
%	
\end{enumerate}
}

\dlz

\aufgabe{Gaussian Posterior Process}{

Assume your data follows the following law: $$\bm{y} = \bm{f} + \bm{\varepsilon}, \quad \bm{\varepsilon} \sim \mathcal{N}(\bm{0},\sigma^2 \bm{I}),$$
  with $\bm{f} = f(\bm{x}) \in \mathbb{R}^n$ being a realization of a Gaussian process (GP), for which we a priori assume $$f(\bm{x}) \sim \mathcal{GP}(m(\bm{x}),k(\bm{x},\bm{x}^\prime)).$$ $\bm{x}$ here only consists of 1 feature that is observed for $n$ data points.
  \begin{enumerate}
  \item Derive / define the prior distribution of $\bm{f}$.
  \item Derive the posterior distribution $\bm{f}|\bm{y}$.
  \item Derive the posterior predictive distribution $y_* | x_*, \bm{x}, \bm{y}$ for a new sample $x_*$ from the same data-generating process.
\item Implement the GP with squared exponential kernel, zero mean function and $\ls = 1$ from scratch for $n=2$ observations $(\bm{y},\bm{x})$. 
Do this as efficiently as possible by explicitly calculating all expensive computations by hand. Do the same for the posterior predictive distribution of $y_*$. Test your implementation using simulated data.
\end{enumerate}
}

\dlz

\aufgabe{Gaussian Processes Prediction}{

Let $\Xspace = \R$ and assume the following statistical model
%
$$
y = f(x) + \eps, \qquad \eps \sim\mathcal{N}\left(0, \sigma^2\right),
$$
%
where $f(x) \in \mathcal{GP}\left(0, k(x,x') \right).$
%
Suppose the covariance function of the GP is 
%
$$  k(x,x') = \mathds{1}_{[|x-x'|<1]} \cdot (1-|x-x'|)$$
%
and we have seen the training data:
%

\begin{center}
	\begin{tabular}{c|cc}
		$i$ & $\xi$ & $\yi$  \\
		\hline
		1 & 1.6 & 3.0 \\
		\hline
		2 & 2.8 & 3.3 \\
		\hline
		3 & 0.5 & 2.0 \\
		\hline
		4 & 3.9 & 2.7 \\
	\end{tabular}
\end{center}

As a test input we observe $x_* = 1.2.$
%
Recall that the predictive distribution for $f(x_*)$ is
%
  \begin{eqnarray*}
	f(x_*) ~|~  \Xmat, \bm{y}, x_* \sim \mathcal{N}(m_{\text{post}}, k_\text{post}).
\end{eqnarray*}
%
with 
%
\begin{eqnarray*}
%	
	m_{\text{post}} &=& \bm{K}_*^{T} \left(\Kmat+ \sigma^2 \cdot \id\right)^{-1}\bm{y} \\
%	
	k_\text{post} &=& K_{**} - \bm{K}_*^T \left(\Kmat + \sigma^2 \cdot \id\right)^{-1} \bm{K}_*,
%	
\end{eqnarray*}
%
Here, $\Kmat = \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}$, $\bm{K}_* = \left(k\left(x_*, \xi[1]\right), ..., k\left(x_*, \xi[n]\right)\right)^\top$ and $ K_{**}\ = k(x_*, x_*)$. 
%

\begin{enumerate}
%	
	\item Compute the predictive mean $	m_{\text{post}}.$

%
	\item Compute the predictive variance $k_\text{post}.$

%
	\item Repeat the calculations from (a) and (b) by using as the test input $x_* = \xi$ for each $i=1,2,3,4,$ respectively. 

	\item Based on your calculations so far, try to sketch the posterior Gaussian process. 

	\item If the nugget $\sigma^2$ would be zero, how would the posterior Gaussian process (roughly) look like?

\end{enumerate}
}

\dlz


% % ------------------------------------------------------------------------------
% % PAST EXAMS
% % ------------------------------------------------------------------------------
% 
% \dlz
% \exexams
% \lz
% 
% \aufgabeexam{WS2020/21}{first}{1}{
% foo
% }
% 
% \dlz
% \loesung{
% bar
% }
% 
% % ------------------------------------------------------------------------------
% % INSPO
% % ------------------------------------------------------------------------------
% 
% \dlz
% \exinspo
\end{document}
