\documentclass[a4paper,10pt,oneside]{article}
\usepackage{a4wide,amsmath,amsfonts,german}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{algorithm,algorithmic-local}
\addtolength{\topmargin}{-2cm} \addtolength{\textheight}{3cm}



\begin{document}
\section*{STATG019 2015: Exercise Sheet 2}
{\em Regarding the content of the second lecture. The content is ICA-relevant but your solution does formally not contribute to the grade.}

\begin{enumerate}

\item {\bf The Support Vector Machine: cross-validation}
\begin{enumerate}
\item For the \texttt{iris} data sets in R (available in vanilla R as variable in the workspace): use \texttt{ksvm} to obtain predictions on a 33\% test set. Use the Gauss kernel, manually trying widths 0.001,0.01, 0.05, 0.1, 1, and the soft margin support vector machine with regularization constants 5 and 0.01.
\item Write a cross-validation script that computes RMSE and MAE, including sample standard deviation, by 100 iterations of leave-one-out cross-validation. I.e., 100 times, the SVM is trained on all data point but one and evaluated on the one. RMSE and MAE, and standard deviations, are computed from the sample of 100 residuals.
\item Use the cross-validation script to select the kernel width among those in (a) and the regularization constant among 0.01, 0.1, 1, 5, 10, 100, by estimating RMSE and MAE on a 66\% training set and picking the pair of values with the smalles RMSE or MAE. 
\item Try out the nu-SVC and the two types of multiclass-SVC with suitable constants.
\item Use cross-validation on the training set to obtain optimal parameters for the nu-SVC and multi-class SVC. Compare then the MSE and MAE (plus standard deviation) of the four methods on the 33\% test set.
\end{enumerate}

\item {\bf The Support Vector Machine: SPAM}
\begin{enumerate}
\item Type \texttt{data(spam)} to load the Hewlett-Packard spam data set, where the task is to classify e-mails to spam or no-spam. Read \texttt{help(spam)} to learn more about the data set. Note that for both, the \texttt{kernlab} package needs to be loaded.
\item Use leave-one-out cross-validation on a 80\% training set and the soft-margin SVC to obtain a classifier. Evaluate the classifier by computing RMSE and MAE, plus standard deviations, on the 20\% test set.
\item Compare against nu-SVC and bound-constraint SVC in, trained in the same way.
\item For the three final classifiers, output the support vectors. Do kernel PCA, with the same kernel, on spam and non-spam; plot the support vectors in the first two principal components.
\item Make a data frame \texttt{nospam} containing a random sub-sample of 95\% of the non-spam e-mails. Learn a model with the one-class SVM, where you use 5\% of the spam e-mails to cross-validate the kernel width and regularization parameter. Evaluate the one-class SVM by obtaining MAE and RMSE (plus standard deviations) on the remaining 95\% of spam mails. Compare to a soft-margin SVM which was trained in a comparable way on the same 95\% non-spam and 5\% spam e-mails.
\end{enumerate}


\item {\bf Lagrange Duality}
\begin{enumerate}
\item Prove the proposition on page 8.
\item Dualize the linearly constrained program in example 1 on page 9, filling in potentially missing steps.
\item Dualize the dual program in example 1 on page 9, and compare the dual of the dual to the primal.
\item Dualize and kernelize the SVM program in example 2 on page 10, filling in potentially missing steps.
\item Choose any of the other non-multi-class SVM programs on page 14 (soft-margin SVC will be easiest). Derive the Lagrange dual program. Kernelize both the primal and the dual program, and compare.
\item Dualize the program described in ``naive kernelization'' on page 6 twice. Compare to the standard form of the kernel support vector machine.
\end{enumerate}

\newpage
\item {\bf Classification Surfaces}
\begin{enumerate}
\item For the homogenous/inhomogenous polynomial kernel of degree $d$: show that the separating manifold $\mathcal{M}$ on page 13 is of the form $\mathcal{M}=\mbox{V}(f):=\{x\in\mathbb{R}^n\;:\; f(x) = 0\}$ for some homogenous/general polynomial $f$ of degree $d$ in $n$ variables.
\item Show, as a converse to (a), that any such polynomial can occur as a separating manifold obtained from a hard-margin SVM, for a suitable collection of data points and labels in $\mathbb{R}^n$ (if you like, you can try to show this for the soft-margin SVM as well, though that may be harder).
\end{enumerate}


\item {\bf Support Vector Regression}
\begin{enumerate}
\item Use support vector regression with cross-validation (see 1 and 2) to obtain a regressor predicting $y$ from $x$ in
 \texttt{
 x <- seq(-20,20,0.1)
 y <- sin(x)/x + rnorm(401,sd=0.03)}
\item Use support vector regression with cross-validation to predict housing prices in Boston (\texttt{Boston} in \texttt{MASS} package) or the income of San Francisco shopping mall customers (\texttt{income} in \texttt{kernlab} package). To access the data sets, you will need to type \texttt{data(Boston)} or \texttt{data(income)} with the respective package loaded.
    Compare the effect of choosing different kernels and regularisation.
\end{enumerate}


\end{enumerate}
\end{document}


