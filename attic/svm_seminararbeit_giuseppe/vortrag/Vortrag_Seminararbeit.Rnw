\documentclass{beamer}
\usepackage{caption}
\usepackage[german]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
%\usepackage{ngerman}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{picins}
\usepackage{floatflt}
\usepackage{lastpage}
\usepackage{url}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{lastpage}
%\pagestyle{fancy}
%\cfoot{\thepage\ of \pageref{LastPage}}

%\linespread{1.5}
%\graphicspath{{Y:/Graphiken/Endpraesentation/}}
\usetheme{Montpellier}
\useinnertheme{rounded}
\useoutertheme{tree}
\usepackage{setspace}
\usepackage{cancel}
\definecolor{hell}{rgb}{0.8,0.8,0.8}
%\definecolor{Red}{rgb}{1,0,0}
%\newcommand{\colorcancel}[2]{\renewcommand{\CancelColor}{\color{#2}}\cancel{#1}}

\setstretch{1,0}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\mini}{\operatornamewithlimits{minimiere}}
\newcommand{\maxi}{\operatornamewithlimits{maximiere}}
\setbeamercovered{transparent}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}
{
\hfill \insertframenumber \,/\,\pageref{Last} \hspace{5pt}%
\vspace{5pt}
} 
\title[Support Vector Machines]{Support Vector Machines \\ \small{- Seminar: Regularisierungstechniken und strukturierte Regression -}}
%\author{Berger Moritz, Casalicchio Giuseppe}
\author{Giuseppe Casalicchio\\
Betreuer: Wolfgang Pößnecker
}
\institute{Institut für Statistik, LMU München}
\date{15. Januar 2013}
\begin{document}

%\SweaveOpts{concordance=TRUE}
%\setlength{\evensidemargin}{0.4cm}
%\setlength{\oddsidemargin}{1.9cm}
<<set-knitr-options, echo = FALSE, results='hide', message=FALSE>>=
opts_knit$set(child.path = "./input")
opts_knit$set(fig.path = "./figure")
opts_chunk$set(echo = TRUE)
opts_chunk$set(concordance=TRUE)

library(svmpath)
library(scatterplot3d)
library(MASS)
library(e1071)
library(plotrix)
library(shape)

plot.svmpath <- function (x, step = max(x$Step), Size = 60, elbow.show = TRUE, 
          support.show = TRUE, fill=TRUE, ...) 
{
  object = x
  if (!missing(step)) {
    maxstep = max(object$Step)
    if (step > maxstep) {
      warning(paste("maximim step of", format(trunc(maxstep)), 
                    "used"))
      step = maxstep
    }
    if (step <= 0) {
      warning("minimim step of 1 used")
      step = 1
    }
  }
  f = predict(object, lambda = object$lambda[step], type = "function")
  x = object$x
  y = object$y
  Elbow = object$Elbow[[step]]
  stats <- StatPath(y, f, Elbow)
  alpha = object$alpha[, step]
  alpha0 = object$alpha0[step]
  lambda = object$lambda[step]
  linear.plot = object$linear
  if (dim(x)[2] > 2) {
    warning("plot.svm intended for two-dimensional x; only first two dimensions used")
    x <- x[, 1:2]
  }
  n <- length(y)
  ss <- abs(diff(range(x[, 2])))
  soffset <- x * 0
  soffset[, 2] <- ss/50
  x <- x[, 1:2]
  if (!linear.plot) {
    kernel.function = object$kernel
    param.kernel = object$param.kernel
    xr <- apply(x, 2, range)
    xg <- apply(xr, 2, function(x, Size) seq(from = x[1], 
                                             to = x[2], length = Size), Size)
    xG <- data.matrix(expand.grid(as.list(data.frame(xg))))
    Knew <- kernel.function(x, xG, param.kernel = param.kernel, 
                            ...)
    fhat <- ((alpha * y) %*% Knew + alpha0)/lambda
    fhat <- matrix(fhat, Size, Size)
  }
  CVal <- format(round(1/lambda, 4), digits = 4)
  SumEpsVal <- format(round(stats$margin, 2), digits = 7)
  dotts = list(...)
  plotargs = list(x = x[, ], type = "n", ..., xlab = "X1", 
                  ylab = "X2", main = bquote(paste(C == .(CVal), "   " , sum(xi[i]) == .(SumEpsVal) )) )
  # bquote(paste("C:", .(CVal), "\n", sum(xi), .(SumEpsVal) ))
                  
#      paste("C: ", format(1/lambda, digits = 5),
#            " Step:", format(step, digits = 3), 
#            " Errors:", format(round(stats$error), digits = 3), 
#            " Elbow Size:", format(round(stats$selbow), digits = 2), 
#            "\n Sum Eps:", format(round(stats$margin, 2), digits = 7))
                  
  plotargs[names(dotts)] = NULL
  plotargs[names(dotts)] = dotts
  do.call("plot", plotargs, quote=TRUE)
  pointid <- seq(y)
  support = (y * f <= 1) | match(pointid, Elbow, FALSE)
  if(fill){
    dat2 <- object$x
    xgrid<-seq(min(dat2[,1])-1,max(dat2[,1])+1, length=200)
    ygrid<-seq(min(dat2[,2])-1,max(dat2[,2])+1, length=200)
    grid1<-expand.grid(xgrid,ygrid)
    pred1 <- predict(object, newx=as.matrix(grid1),
                     lambda = object$lambda[step], type = "class")
    grid.fill(xgrid,ygrid,pred1)
  }
  if (!support.show) 
    points(x, col = y + 3, pch = "*", cex = 2)
  else {
    points(x[support, ], col = y[support] + 3, pch = 42, 
           cex = 3)
    
    pts.plus <- y[!support]==1
    pts.minus <- y[!support]==-1
    
    pts.plus[!pts.plus] <- NA
    pts.minus[!pts.minus] <- NA
    
    pts.plus[pts.plus] <- "+"
    pts.minus[pts.minus] <- "-"
    
    points(x[!support, ], col = y[!support] + 3, pch = pts.minus, #10, 
           cex = 2.5)
    points(x[!support, ], col = y[!support] + 3, pch = pts.plus, #10, 
           cex = 2.5)
  }
  if (n < 15) 
    text((x - soffset), labels = paste(pointid), col = y + 
      3)
  if (n < 15 && length(Elbow)) 
    text((x - soffset)[Elbow, ], labels = paste(pointid[Elbow]), 
         col = 3)
  if (linear.plot) {
    beta <- (alpha * y) %*% x
    abline(-alpha0/beta[2], -beta[1]/beta[2], col = 1, lwd = 4)
    abline(lambda/beta[2] - alpha0/beta[2], -beta[1]/beta[2], 
           col = 1, lwd = 4, lty = 2)
    abline(-lambda/beta[2] - alpha0/beta[2], -beta[1]/beta[2], 
           col = 1, lwd = 4, lty = 2)
  }
  else {
    contour(xg[, 1], xg[, 2], fhat, levels = 0, add = TRUE, 
            labels = "", col = 1, lwd = 4)
    contour(xg[, 1], xg[, 2], fhat, levels = c(-1, 1), add = TRUE, 
            labels = c("", ""), col = 1, lwd = 4, lty = 2)
  }
  if (elbow.show) 
    points(x[Elbow, ], col = y[Elbow] + 3, pch = 20, cex = 3.5)
  invisible(list(x = x, y = y, Elbow = Elbow, alpha = alpha, 
                 alpha0 = alpha0, support = pointid[support], step = step, 
                 error = stats$error, sum.eps = stats$margin))
}

orthogonal <- function(svmpath, from.hyperplane=FALSE, adjust=c(0.15,0.5),
                       label=expression(frac(1,"||w||")),...){
  x = svmpath$x
  y = svmpath$y
  step = svmpath$Step[length(svmpath$Step)]
  alpha = svmpath$alpha[, step]
  alpha0 = svmpath$alpha0[step]
  lambda = svmpath$lambda[step]
  beta <- (alpha * y) %*% x
  scale <- diff(range(x[,1]))/diff(range(x[,2]))
  plotit<-plot(svmpath, ...)
  pointIndex <- plotit$support[!plotit$support%in%plotit$Elbow]
  if(from.hyperplane){
      hyperplane <- c(- alpha0/beta[2], -beta[1]/beta[2])
      pointIndex <- plotit$Elbow
  }
  
  pointClass <- y[pointIndex]
  
  # senkrechte
  m <- -1/((-beta[1]/beta[2])*scale*scale)
  pointCoord <- (x[pointIndex,1:2]) #x[pointIndex,1:2]
  
  ortho <- t(apply(pointCoord,1,function(X){c(X[2]-m*X[1],m)})) #c(pointCoord[2]-m*pointCoord[1],m)
  xiCoord <- cbind(pointCoord[,1]+0.3,(pointCoord[,1])*ortho[,2]+ortho[,1])
  
  if(!from.hyperplane){
  upper <- c(lambda/beta[2] - alpha0/beta[2], -beta[1]/beta[2])
  lower <- c(-lambda/beta[2] - alpha0/beta[2], -beta[1]/beta[2])
  
  # Schnittpunkt upper und orthogonal
  cutUpX <- apply(ortho[pointClass==1,], 1, function(X){(X[1]-upper[1])/(upper[2]-X[2])}) #(ortho[1]-upper[1])/(upper[2]-ortho[2])
  cutUpY <- upper[1] + upper[2]*cutUpX
  
  cutLoX <- apply(ortho[pointClass==-1,], 1,function(X){(X[1]-lower[1])/(lower[2]-X[2])}) #(ortho[1]-lower[1])/(lower[2]-ortho[2])
  cutLoY <- lower[1] + lower[2]*cutLoX
  
  listeUp <- list(pointCoord[pointClass==1,],cbind(cutUpX,cutUpY))
  listeLo <- list(pointCoord[pointClass==-1,],cbind(cutLoX,cutLoY))
    for(i in 1:sum(pointClass==1)){
      lines(rbind(listeUp[[1]][i,],listeUp[[2]][i,]), lwd=3)
    }
    for(j in 1:sum(pointClass==-1)){
      lines(rbind(listeLo[[1]][j,],listeLo[[2]][j,]), lwd=3)
    }
    #text(xiCoord,labels=expression(xi[i]^"*"), cex=2)
    for(i in 1:5) 
      text(xiCoord[i,1], xiCoord[i,2], substitute(list(xi)[list(x)], 
                                                  list(x=i)), cex=2)
  }else{
      cutHypX <- (ortho[1,1]-hyperplane[1])/(hyperplane[2]-ortho[1,2]) 
      cutHypY <- hyperplane[1] + hyperplane[2]*cutHypX
      listeHyp <- list(pointCoord[1,],cbind(cutHypX,cutHypY))
      lines(rbind(listeHyp[[1]],listeHyp[[2]]), lwd=3)
      mitte <- apply(rbind(listeHyp[[1]],listeHyp[[2]]), 2,mean)
      text(mitte[1]+adjust[1], mitte[2]+adjust[2], labels=label, cex=2)#, srt=atan((m*scale))*180/pi)
  }
  invisible(plotit)
}

grid.points<-function(pred, grid, ...){
  col <- pred
  col[col=="1"] <- rgb(0, 0, 1, 0.25)
  col[col=="-1"] <- rgb(1, 0, 0, 0.25)
  points(grid,col=col, pch=15, ...)
}

grid.fill <- function(x, y, pred){
  z<-matrix(pred,nrow=length(x),ncol=length(y))
  image(x,y,z, add=TRUE, 
        col=c(rgb(1, 0, 0, 0.25),rgb(0, 0, 1, 0.25)))
}


@

\begin{frame}
\titlepage
\end{frame}
\frame
{
  \frametitle{Gliederung}
	\tableofcontents
}
\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Gliederung}
       \tableofcontents[currentsection]
   \end{frame}
}

\section{Grundidee}
\frame{
\frametitle{Grundidee}
%\framesubtitle{Grundidee}
%\centering
\begin{minipage}{\textwidth}
\hspace{-10px}\textbf{Ausgangslage}:

$N$ Trainingsdaten $(\mathbf{x}_1^{\top}, y_1), \hdots, (\mathbf{x}_N^{\top}, y_N)$ mit

\begin{tabular}{lll}
&$\mathbf{x}_i^{\top} \in \mathbb{R}^p$: & Merkmalsvektor mit $p$ Variablen\\
&$y_i \in \{-1,1\}$: &Klassenzugehörigkeit der $i$-ten Beobachtung
\end{tabular}

\vspace{1.5em}

\hspace{-10px}\textbf{Ziel}:

optimale Zuordnung neuer Daten $\mathbf{x}_{neu}$ in die Klasse $y_{neu} \in \{-1,1\}$. \newline

\hspace{-10px}\textbf{Grundidee}:

%Suche Entscheidungsfunktion $f: \mathbb{R}^p \rightarrow \{-1,1\}$, sodass $f(\mathbf{x_i})= y_i$.
Finde eine Hyperebene, die die Daten möglichst gut in zwei Klassen trennt.

\end{minipage}
}

\section{Linear trennbare Daten}
\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Hyperebene}

%\begin{table}
\begin{tabular}{ll}
\textbf{Gesucht:} &Entscheidungsfunktion $f: \mathbb{R}^p \rightarrow \{-1,1\}$, sodass\\
                  &$f(\mathbf{x}_i)=y_i \hspace{20pt} \forall i = 1, \hdots, N$ \\
\textbf{Frage:}   &Wie wählt man die Hyperebene aus?
\end{tabular}
%\end{table}

<<trennlinie, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, fig.width=9, fig.height=3>>=

set.seed(123)
clust1 <- cbind(mvrnorm(15, mu=c(1,5), Sigma=matrix(c(0.5,0,0,0.5), ncol=2)), Class=1)
clust2 <- cbind(mvrnorm(15, mu=c(5,1), Sigma=matrix(c(0.5,0,0,0.5), ncol=2)), Class=-1)

dat <- rbind(clust1,clust2)
colnames(dat) <- c("Var1","Var2","Class")
dat<-as.data.frame(dat)
dat$Class <- as.factor(dat$Class)
col=as.character(dat$Class)
col[col=="1"] <- 4
col[col=="-1"] <- 2

# dat2
dat2<-dat
#dat2$Class[24] <- "1"
dat2$Class[1] <- "-1"
dat2$Class[3] <- "-1"

xgrid<-seq(min(dat2[,1])-1,max(dat2[,1])+1, length=100)
ygrid<-seq(min(dat2[,2])-1,max(dat2[,2])+1, length=100)
grid1<-expand.grid(xgrid,ygrid)

svmpath1 <- svmpath(as.matrix(dat[,1:2]),as.numeric(as.character(dat[,"Class"])))

object <- svmpath1
step <- max(svmpath1$Step)
alpha = object$alpha[, step]
alpha0 = object$alpha0[step]
beta <- (alpha * object$y) %*% object$x
lambda = object$lambda[step]

par(mfrow=c(1,3), mar=c(1,0.5,1,0.5))
#layout(matrix(c(1,1,2,2,0,3,3,0),2,4,byrow=TRUE), TRUE)
plusminus <- col
plusminus[plusminus=="4"] <- "+"
plusminus[plusminus=="2"] <- "-"
plot(dat$Var1,dat$Var2, col=col, axes=F, xlab="", ylab="", pch=plusminus, cex=2)
box()
#abline(v=2.95, col=3, lwd=3)
grid1<-expand.grid(xgrid,ygrid)
pred1<-40*grid1[,1]-100
grid.fill(xgrid,ygrid, grid1[,2]>=pred1)
abline(coef=c(-100, 40), col=1, lwd=4)

plot(dat$Var1,dat$Var2, col=col, axes=F, xlab="", ylab="", pch=plusminus, cex=2)
box()
pred2<-0.05*grid1[,1]+2.5
grid.fill(xgrid,ygrid, grid1[,2]>=pred2)
abline(coef=c(2.5,0.05), col=1, lwd=4)

#plot.svmpath(svmpath1, main="", axes=F, support.show=F)
plot(dat$Var1,dat$Var2, col=col, axes=F, xlab="", ylab="", pch=plusminus, cex=2)
pred3<-(-beta[1]/beta[2])*grid1[,1] -alpha0/beta[2] 
grid.fill(xgrid,ygrid, grid1[,2]>=pred3)
abline(-alpha0/beta[2], -beta[1]/beta[2], col = 1, lwd = 4)
box()
@


}



\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Hyperebene}
Eine Hyperebene trennt einen $p$-dimensionalen Variablenraum in zwei Unterräume und 
hat selbst die Dimension $(p-1)$: %ist gegeben durch: %, hat selbst die Dimension 

\begin{equation*} % bedeutet, dass der Abstand der Hyperebene zu x = 0 ist (also durch den Punkt verläuft)
\{\mathbf{x} \in \mathbb{R}^p \; | \; \mathbf{w}^\top \mathbf{x} + b = 0\} %, \; \forall \mathbf{x} \in \mathbb{R}^p
\end{equation*}


mit
\begin{tabular}{lll}
&$\mathbf{w} \in \mathbb{R}^p$: & Vektor orthogonal zur Hyperebene\\
&$b \in \mathbb{R}$: & Verschiebung (vom Ursprung)
\end{tabular}

\vspace{15px}

\begin{equation*} 
\text{Notation zum Skalarprodukt: } \mathbf{w}^\top \mathbf{x} = \langle \mathbf{w},\mathbf{x} \rangle = \sum_{i=1}^{p} w_i x_i 
\end{equation*}

}

\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Hyperebene}
%TODO: Beispiel Hyperebene
%Beispiel Hyperebene im $\mathbb{R}^1$ (eindimensionalen Variablenraum):
%Beispiel Hyperebene im $\mathbb{R}^2$:
%Hyperebenen werden von genau einem Basisvektor weniger als der Gesamtraum aufgespannt.
%Die Hyperebene \fbox{$x_2 = -x_1 + 1 \; \Leftrightarrow x_1 + x_2 - 1 = 0$} mit $ \mathbf{w}^\top = (1,1)$ ist eine Gerade in einem zweidimensionalen Gesamtraum (links).
%Beispiel: Sei $\mathbf{x} = (x_1 , x_2)$ eine Beobachtung im zweidimensionalen Variablenraum.
%\begin{table}
\begin{tabular}{ll}
\textbf{Beispiel:} & $x_2 = -x_1 + 1 \; \Leftrightarrow x_1 + x_2 - 1 = 0$ \\
                   & mit $ \mathbf{w}^\top = (1,1)$ und $b=-1$
\end{tabular}
%\end{table}

$\rightarrow$ Gerade im 2-dimensionalen Variablenraum (links).

<<hyper, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, fig.align='center', fig.width=8, fig.height=4>>=
par(mfrow=c(1,2), mar=c(3,2,2,2))
x <- -3:4
y <- -x +1
plot(x,y, type="l", xlim=c(-3,3), ylim=c(-3,3), main="Gerade im 2-dim",
     axes=F,xlab="", ylab="", lwd=2, col=3)
abline(h=0, lwd=3)
abline(v=0, lwd=3)
text(3,-0.25,expression(x[1]), cex=1)
text(0.3,3,expression(x[2]), cex=1)
text(1,-0.25,1, cex=1)
text(0.2,1,1, cex=1)
text(-0.0,3.2, ">",srt=90, pch=17)
text(3.2,0, ">", pch=17)
for(i in x){
  points(i,0, pch=3)
  points(0,i, pch=3)
}

x <- seq(0,10, length.out=100)
y <- -x + 1
d1 <- expand.grid(x=x, y=y)
x <- d1[,1]
y <- d1[,2]
z <- y

# x <- seq(-3,3, length.out=100)
# y <- seq(-3,3, length.out=100) 
# d1 <- expand.grid(x=x, y=y)
# x <- d1[,1]
# y <- d1[,2]
# z <- y

scatterplot3d(x, y, z, color=3, xlab=expression(x[1]), ylab=expression(x[2]), main="Ebene im 3-dim",
              zlab=expression(x[3]), col.axis="black",col.grid=3, pch=15, type="p", mar=c(3,2,2,2))

@
%\vspace{-20pt}


}


\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Hyperebene}

%\hfill
%\begin{floatingfigure}[l]{5cm}
\begin{columns}
\begin{column}[T]{0.5\textwidth}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{figure/hyperplane2}
\end{figure}
%\end{floatingfigure}
\end{column}
\begin{column}[T]{0.5\textwidth}

\begin{itemize}
\item Punkte ``unterhalb'' der Hyperebene $\rightarrow -1$
\item Punkte ``überhalb'' der Hyperebene $\rightarrow +1$
\end{itemize}

\end{column}
\end{columns}



%\vspace{-5px}

%\begin{minipage}[t]{0.4\textwidth}
\begin{table}
\begin{center}
\begin{tabular}{ccc}
Klassifiziere in Klasse ${\color{red}-1}$ & falls & ${\color{red}\mathbf{w}^\top \mathbf{x} + b< 0}$\\
Klassifiziere in Klasse ${\color{blue}+1}$ & falls & ${\color{blue}\mathbf{w}^\top \mathbf{x} + b> 0}$\\
\end{tabular}
\end{center}
\end{table}

\begin{center}
$\Rightarrow$ Verwende Entscheidungsfunktion $ f(\mathbf{x}) = \sgn(\mathbf{w}^\top \mathbf{x} + b)$
\end{center}
% \begin{align*}
% f(\mathbf{x}) &= \sgn(\mathbf{w}^\top \mathbf{x} + b) = 
% \begin{cases}
%   {\color{red}-1} & \text{falls } \; {\color{red}\mathbf{w}^\top \mathbf{x} + b< 0}, \\
% ~~\, 0 & \text{falls } \; \mathbf{w}^\top \mathbf{x} + b = 0, \\
%   {\color{blue}+1} & \text{falls } \; {\color{blue}\mathbf{w}^\top \mathbf{x} + b> 0}. 
% \end{cases}
% \end{align*}
%\end{minipage}
}

\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Hyperebene}
%\begin{itemize}

%\item[1.] 
Die Hyperebene, kann gleichermaßen durch alle Paare $\{\lambda \mathbf{w},\lambda b\}, \; \lambda \in \mathbb{R}^+$ dargestellt werden:

\begin{align*}
%\{\mathbf{x} \in \mathbb{R}^p \; | \; 
&\mathbf{w}^\top \mathbf{x} + b = 0  \\
%\{\mathbf{x} \in \mathbb{R}^p \; | \; 
\Leftrightarrow &\lambda \mathbf{w}^\top \mathbf{x} + \lambda b = 0, \; \text{für} \; \lambda \in \mathbb{R}^+
\end{align*}

%\item[2.] $f(\mathbf{x}) = \sgn(\mathbf{w}^\top \mathbf{x} + b)$ invariant gegenüber Skalierung:

%\begin{equation*}
%\mathbf{w} \rightarrow \lambda \mathbf{w}, b \rightarrow \lambda b, \; \text{für} \; \lambda \in \mathbb{R}^+
%\end{equation*}

%\end{itemize}

\textbf{Problem:} Keine eindeutige Beschreibung der Hyperebene
}


\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Hyperebene}

\textbf{Lösung:} Einführung einer kanonischen Hyperebene, für die gilt:

\begin{equation*}
\min_{i=1, \hdots, N} | \mathbf{w}^\top \mathbf{x}_i + b | = 1,
\end{equation*}

d.h. nähester Punkt zur Hyperebene hat funktionalen Abstand 1. %http://scienceray.com/physics/how-to-measure-a-functional-distance/

%Für eine trennende Hyperebene in kanonischer Form gilt:
\begin{columns}
\begin{column}[T]{0.5\textwidth}
Es gilt:
\begin{align*}
{\color{blue}\mathbf{w}^\top \mathbf{x}_i + b \geq +1 \; \text{ für } \; y_i = +1} \\
{\color{red}\mathbf{w}^\top \mathbf{x}_i + b \leq -1 \; \text{ für } \; y_i = -1}
\end{align*}

beziehungsweise
\begin{equation*}
y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \;\;  \forall i= 1, \hdots, N
\end{equation*}
\end{column}
\begin{column}[T]{0.5\textwidth}
<<kanonische, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, out.width='0.8\\textwidth', fig.align='center'>>=
m <- -beta[1]/beta[2]
x <- seq(-10,10, by=0.1)
y <- m*x - alpha0/beta[2]
scale <- diff(range(dat[,1]))/diff(range(dat[,2]))

par(mar=c(0.5,0.5,0.5,0.5))
#m4<-plot(svmpath1, main="", axes=F, xlab="", ylab="")
m4<-orthogonal(svmpath1,from.hyperplane=TRUE,main="", 
               label="1",axes=F, xlab="", ylab="")
box()
pred3<-(-beta[1]/beta[2])*grid1[,1] -alpha0/beta[2] 
#grid.fill(xgrid,ygrid, grid1[,2]>=pred3)

x2 <- 3
y2 <- m*x2 -lambda/beta[2] - alpha0/beta[2] - 1
text(x2,y2,labels=expression(paste(w^T, x + b == -1)), col=1,
     cex=2, srt=atan((m*scale))*180/pi) 

x3 <- 0.5
y3 <- m*x3 +lambda/beta[2] - alpha0/beta[2] + 1
text(x3,y3,labels=expression(paste(w^T, x + b == +1)), col=1,
     cex=2, srt=atan((m*scale))*180/pi) 
#polygon(c(min(x), x, max(x)), c(min(y), y, min(y)),  col = rgb(1,0,0,0.25)) 
#polygon(c(min(x), x, min(x)), c(min(y), y, max(y)),  col = rgb(0,0,1,0.25)) 


@
\end{column}


\end{columns}
}


\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Margin}

Euklidischer Abstand eines Punktes $\mathbf{x}_i$ zur Hyperebene $(\mathbf{w},b)$ durch normieren mit der Vektorlänge $||\mathbf{w}||$ bestimmbar:%. Der Abstand von $\mathbf{x}_i$ zur Hyperebene $(\mathbf{w},b)$ beträgt dann:

\begin{equation*}
d((\mathbf{w},b), \mathbf{x}_i) = \frac{y_i (\mathbf{w}^\top \mathbf{x}_i + b)}{||\mathbf{w}||} \geq \frac{1}{||\mathbf{w}||}
\end{equation*}

\textbf{Gesucht:} Hyperebene mit größtmöglichen euklidischen Abstand zu den nähesten Punkten.\\

\begin{itemize}
\item[$\pmb{\rightarrow}$] Je kleiner $||\mathbf{w}||$, desto größer der euklidische Abstand.
\item[$\pmb{\rightarrow}$] Je größer der euklidische Abstand, desto breiter der Rand (margin).
\end{itemize}
% 
% \begin{equation*}
% \max_{w,b} d((\mathbf{w},b), \mathbf{x}_i) \Leftrightarrow \min_{w,b} ||w||
% \end{equation*}
% 
% mit Nebenbedingung: 
% \begin{equation*}
% y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \;\;  \forall i
% \end{equation*}
}

\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Margin}

\begin{columns}
\begin{column}[T]{0.5\textwidth}
<<margin, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, out.width='\\textwidth', fig.align='center'>>=
m <- -beta[1]/beta[2]
x <- quantile(dat[,1],0.6)
y <- m*x - alpha0/beta[2]
x5 <- 3
y5 <- m*x5 - alpha0/beta[2] + (lambda/beta[2])/2
scale <- diff(range(dat[,1]))/diff(range(dat[,2]))

par(mar=c(0.5,0.5,0.5,0.5))
#m4<-plot(svmpath1, main="", axes=F, xlab="", ylab="")
m4 <- orthogonal(svmpath1, from.hyperplane=TRUE, adjust=c(0.075,0.25), label=1,
                 main="", axes=F, xlab="", ylab="")
box()
text(x5,y5,expression(underline("margin")), col=1,
     srt=atan(-1/(m*scale))*180/pi, cex=3)
#apply(m4$x[m4$support,],1, FUN=function(X){text(X[1],X[2],"bla")})
pred3<-(-beta[1]/beta[2])*grid1[,1] -alpha0/beta[2] 
#grid.fill(xgrid,ygrid, grid1[,2]>=pred3)
legend("bottomleft",c("Support Vektoren","Hyperebene"), bg="lightgray",
       pch=c(NA,NA), pt.cex=3, lty=c(NA,1), lwd=3, cex=1.5, col=c(1,1))
points(c(-0.35,-0.1),c(0.4,0.4), pch=19, col=c(2,4), cex=2.5)

x2 <- 5.8
y2 <- m*x2 -lambda/beta[2] - alpha0/beta[2] + 0.5
#filledrectangle(mid=c(x2,y2), wx=1.2, wy=1.5, col="gray", lcol=1)#, angle=atan((m*scale))*180/pi) 
text(x2,y2,labels=expression(paste(w^T, x + b == -1)), col=1,
     cex=2, srt=atan((m*scale))*180/pi) 

x3 <- 0.5
y3 <- m*x3 +lambda/beta[2] - alpha0/beta[2] + 0.5
#filledrectangle(mid=c(x3,y3), wx=1.2, wy=1.5, col="gray", lcol=1,)
text(x3,y3,labels=expression(paste(w^T, x + b == +1)), col=1,
     cex=2, srt=atan((m*scale))*180/pi) 


@
  \end{column}
  \begin{column}[T]{0.5\textwidth}
  \begin{itemize}
  \item Punkte am nähesten zur Hyperebene haben betragsmäßig einen euklidischen Abstand von $\tfrac{1}{||\mathbf{w}||}$
  \item Margin (Rand) ist $\tfrac{2}{||\mathbf{w}||}$ breit
  \item Alle anderen Punkte liegen jenseits des Randes
  \end{itemize}
  \end{column}
  \end{columns}
}

\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Primäres Optimierungsproblem}

maximiere Rand (margin) $\Leftrightarrow$ minimiere $ ||\mathbf{w}||$ $\Leftrightarrow$ minimiere $\tfrac{1}{2} ||\mathbf{w}||^2$

\begin{gather*}
\min_{\mathbf{w},b} \hspace{8pt} {\color{red}\tfrac{1}{2} ||\mathbf{w}||^2}\\
\text{NB:} \hspace{8pt} {\color{blue}y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1} 
\hspace{8pt} \forall i = 1, \hdots, N
\end{gather*}


%mit Nebenbedingung: 
%\begin{center}
%${\color{blue}y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1} \;\;  \forall i = 1, \hdots, N$
%\end{center}

\textbf{Lösung:} Lagrange Methode mit Lagrange Multiplikatoren $\alpha_i \geq 0, \; \forall i = 1, \hdots, N$

\begin{align*}
L(\mathbf{w},b,\pmb{\alpha}) &= {\color{red}\tfrac{1}{2} ||\mathbf{w}||^2} + \sum_{i=1}^{N} \alpha_i  ({\color{blue}1 - y_i (\mathbf{w}^\top \mathbf{x}_i + b)})\\
&=  {\color{red}\tfrac{1}{2} ||\mathbf{w}||^2} - \sum_{i=1}^{N} \alpha_i  ({\color{blue}y_i (\mathbf{w}^\top \mathbf{x}_i + b) - 1})
\end{align*}

}



\frame{
\frametitle{Linear trennbare Daten}
%\framesubtitle{Vorgehen}
$L(\mathbf{w},b,\pmb{\alpha})$ wird bezüglich 
%\max_{\pmb{\alpha}} \left ( \min_{\mathbf{w}, b} L(\mathbf{w},b,\pmb{\alpha}) \right )

\begin{enumerate}
\label{ableitung}
\item[{\color{blue}1.}] $b$ und $\mathbf{w}$ (Primärvariablen) minimiert $\rightarrow \min_{\mathbf{w}, b} L(\mathbf{w},b,\pmb{\alpha}) $
\item[{\color{blue}2.}] $\pmb{\alpha}$ (duale Variable) maximiert $\rightarrow \max_{\pmb{\alpha}} \left ( \min_{\mathbf{w}, b} L(\mathbf{w},b,\pmb{\alpha}) \right )$
%weil quadratisches Problem (nach unten geöffnete Parabel -> hat maximum)
\end{enumerate}

Zu {\color{blue}1.}:
\begin{align}
\frac{\partial L(\mathbf{w},b,\pmb{\alpha})}{\partial b} = 0          & \Rightarrow \sum_{i=1}^N \alpha_i y_i = 0 \\
\frac{\partial L(\mathbf{w},b,\pmb{\alpha})}{\partial \mathbf{w}} = 0 & \Rightarrow \mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i
\end{align}

$\rightarrow \mathbf{w}$ ist Linearkombination der Trainingsdaten $\mathbf{x}_i$.
}

\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Duales Optimierungsproblem}

Zu {\color{blue}2.}:

Durch Einsetzen der Lösungen (1) und (2) in $L(\mathbf{w},b,\pmb{\alpha}) $ verschwinden die Primärvariablen $\Rightarrow$ duales Optimierungsproblem:

% \begin{align*}
% \max_{\pmb{\alpha}} W(\pmb{\alpha})
% &= \max_{\pmb{\alpha}} \left ( \min_{\mathbf{w}, b} L(\mathbf{w},b,\pmb{\alpha}) \right ) \\
% &= \max_{\pmb{\alpha}} \left ( \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j \right )
% \end{align*}

% NB: \hfill
% %\begin{align*}
% $\alpha_i \geq 0 \hspace{20pt} \sum_{i=1}^N \alpha_i y_i = 0$
% %\end{align*}

\begin{gather*}
\label{primal}
% \begin{aligned}
\max_{\pmb{\alpha}} \left ( \min_{\mathbf{w}, b} L(\mathbf{w},b,\pmb{\alpha}) \right ) = \max_{\pmb{\alpha}} \left ( \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N \alpha_i \alpha_j y_i y_j \mathbf{x}_i^{\top} \mathbf{x}_j \right )\\
\text{NB:} \hspace{15pt} \alpha_i \geq 0, \hspace{20pt} \sum_{i=1}^N \alpha_i y_i = 0  \hspace{20pt} \forall i= 1, \hdots, N
%\end{aligned}
\end{gather*}
\vfill
\hfill {\color{hell}{\tiny\ref{primal2}}}
}

\frame{
\frametitle{Einschub: Lagrange Methode}
%\framesubtitle{Einschub: Lagrange Methode - equality constraint}
\vspace{-10pt}
\begin{columns}
  \begin{column}[T]{0.05\textwidth}
  \end{column}
  
  \begin{column}[T]{0.45\textwidth}
    \begin{gather*}
    \min_{x_1,x_2} x_1^2 + x_2^2
    \end{gather*}
  \end{column}

  \begin{column}[T]{0.45\textwidth}
    \begin{gather*}
                \min_{x_1,x_2} x_1^2 + x_2^2 \\
    \text{NB: }  x_1+x_2=1
    \end{gather*}
	\end{column}
  
  \begin{column}[T]{0.05\textwidth}
  \end{column}
\end{columns}
%TODO: ein Beispiel zu equality constraint optimization und inequality constraint optimitation
<<lagrange, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, fig.width=10, fig.height=5, fig.align='center'>>=
# x1 <- seq(-1,1, length=100)
# x2 <- seq(-1,1, length=100)
# 
# gerade <- -x1 + 1
# 
# #x1^2+x2^2=1
# y1 <- sqrt(1-x2^2)
# y2 <- sqrt(1-x1^2)
# 
# kreise <- function(x1,x2, anzahl){
#   summe <- seq(0,1,length=anzahl)
#   plot(0,0, xlim=c(-2,2), ylim=c(-2,2))
#   abline(h=0)
#   abline(v=0)
#   for(i in 1:length(summe)){
#     y1 <- sqrt(summe[i]-x2^2)
#     y2 <- sqrt(summe[i]-x1^2)
#     lines(x2,y1)
#   }
# }
# 
# plot(x1,gerade, xlim=c(-2,2), ylim=c(-2,2), type="l")
# abline(h=0)
# abline(v=0)
# lines(x2,y1)
# lines(x1,y2)
# lines(x2,-y1)
# lines(x1,-y2)

kreis <- function(x1=seq(-1,1, length=100),x2=seq(-1,1, length=100),length=10){
  plot(x1,x2,type="n",xlab="",ylab="",main="", asp=1,ylim=c(-1,1))
  draw.circle(0,0,seq(1,0,length=length),border="black",
              col=heat.colors(length),lty=1,lwd=1)
  abline(h=0)
  abline(v=0)
}

par(mfrow=c(1,2), mar=c(2,2,1,1), cex=1)
kreis()
points(0,0, cex=2, pch=21, bg="grey")

kreis()
x1 <- seq(-2,2, length=100)
x2 <- -x1 +1
lines(x1,x2, lwd=5)
points(0.5,0.5, cex=2, pch=21, bg="grey")

@


}



\frame{
\frametitle{Einschub: Lagrange Methode}
%\framesubtitle{Einschub: Lagrange Methode - inequality constraint}
\vspace{-10pt}
\begin{columns}
  \begin{column}[T]{0.05\textwidth}
  \end{column}
  
  \begin{column}[T]{0.45\textwidth}
    \begin{gather*}
    \min_{x_1,x_2} x_1^2 + x_2^2 \\
    \text{NB: }  {\color{blue}x_1 \geq -0.5}, \; {\color{blue}x_1 \leq 0.5}
    \end{gather*}
  \end{column}

	\begin{column}[T]{0.45\textwidth}
    \begin{gather*}
                \min_{x_1,x_2} x_1^2 + x_2^2 \\
    \text{NB: }  x_1 \geq 0.2, \; {\color{blue}x_1 \leq 0.5}
    \end{gather*}
	\end{column}
  
  \begin{column}[T]{0.05\textwidth}
  \end{column}
\end{columns}

<<lagrangetwo, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, fig.width=10, fig.height=5, fig.align='center'>>=

par(mfrow=c(1,2), mar=c(2,2,1,1), cex=1)
kreis()
rect(-0.5,2,0.5,-2, col= rgb(0, 0, 0, 0.5))
#abline(h=0.5,lwd=5, col="blue")
#abline(h=-0.5,lwd=5, col="blue")
abline(v=0.5,lwd=5, col="blue")
abline(v=-0.5,lwd=5, col="blue")
points(0,0, cex=2, pch=21, bg="grey")

kreis()
rect(0.2,2,0.5,-2, col= rgb(0, 0, 0, 0.5))
#abline(h=0.5,lwd=5, col="blue")
#abline(h=-0.5,lwd=5, col="blue")
abline(v=0.5,lwd=5, col="blue")
abline(v=0.2,lwd=5)
points(0.2,0, cex=2, pch=21, bg="grey")
legend("bottomleft", lty=1, lwd=5, col=c(1,4), c("aktiv","inaktiv"), bg="gray")

@
}



\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Lagrange Methode}
%$\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$ ist Linearkombination der Trainingsdaten $x_i$ mit  Lagrange Multiplikator $\alpha_i > 0$ ist.

Nach Karush-Kuhn-Tucker (KKT) ist eine weitere Bedingung nötig: %komplementär Bedingung

\begin{align*}
\alpha_i [ y_i (\mathbf{w}^\top \mathbf{x}_i + b) -1 ] = 0 \hspace{8pt} \forall i= 1, \hdots, N
\end{align*}

\begin{columns}
\begin{column}[T]{0.6\textwidth}
\begin{itemize}
%\item[$\pmb{\Rightarrow}$] Für $y_i (\mathbf{w}^\top \mathbf{x}_i + b) > 1$ (funktionale Abstand einer Beobachtung $\mathbf{x}_i$ größer 1) ist $\alpha_i=0$.

\item[$\pmb{\Rightarrow}$] $\alpha_i=0$ für $y_i (\mathbf{w}^\top \mathbf{x}_i + b) > 1$ (funktionale Abstand einer Beobachtung $\mathbf{x}_i$ ist größer 1). \\
$\rightarrow$ inaktive NB

\item[$\pmb{\Rightarrow}$] Es interessieren nur die support Vektoren $\rightarrow y_i (\mathbf{w}^\top \mathbf{x}_i + b) = 1$
%Es interessieren nur die Beobachtungen, für die $y_i (\mathbf{w}^\top \mathbf{x}_i + b) = 1$ gilt (support Vektoren).
\end{itemize}
\end{column}
\begin{column}[T]{0.4\textwidth}
\includegraphics[width=\textwidth]{figure/kanonische} 
\end{column}
\end{columns}
}



\frame{
\frametitle{Linear trennbare Daten}
\framesubtitle{Zusammenfassung}

\begin{enumerate}
\item Lagrangefunktion aufstellen und duale Funktion herleiten
\item Bestimme $\alpha_i$ der support Vektoren durch duales Optimierungsproblem
\item Bestimme ${\color{blue}\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i}$ der kanonischen Hyperebene
\item Bestimme Verschiebung $b = - \tfrac{1}{2} ( \mathbf{w}^{\top} \mathbf{x}^{+} +  \mathbf{w}^{\top} \mathbf{x}^{-}) $ der Hyperebene aus support Vektoren $\mathbf{x}^{+}$ und $\mathbf{x}^{-}$:
\begin{align*}
b + \mathbf{w}^{\top} \mathbf{x}^{+}= +1\\
b + \mathbf{w}^{\top} \mathbf{x}^{-}= -1
\end{align*}
% aus Gleichung $\mathbf{w}^T \mathbf{x}_j + b = y_j, \; y_j \in \{-1,1\}$
\item Entscheidungsfunktion $f(\mathbf{x}) = \sgn \left ( {\color{blue}\mathbf{w}^{\top}} \mathbf{x} + b \right ) =  \sgn \left ( {\color{blue}\sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top} \mathbf{x}  + b \right )$
\end{enumerate}

}

\section{Nicht linear trennbare Daten}
\subsection{Kern Trick}
\frame{
\frametitle{Kern Trick}
\framesubtitle{Idee}
Daten in höher dimensionalen Raum überführen, in dem sie linear Trennbar sind. \newline

\textbf{Beispiel:}
\begin{table}
\centering
\begin{tabular}{rcccc}
$\Phi:$  & $\mathbb{R}^2$   & $\rightarrow$ & $\mathbb{R}^3$ & \\
         & $(x_1,x_2)$      & $\rightarrow$ & $(z_1,z_2,z_3)$ & $= (x_1^2,\sqrt{2}x_1 x_2, x_2^2)$\\
\end{tabular}
\end{table}

%\begin{tabular}{lrlll}
%\Phi: & $\mathbb{R}^2$  & $\rightarrow$ &$\mathbb{R}^3$ &\\
%      &$(x_1,x_2)$      & $\rightarrow$ &$(z_1,z_2,z_3) = (x_1^2,\sqrt{2}x_1 x_2, x_2^2)$&\\
%\end{tabular}
%\end{table}
\vfill
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figure/bild3}
\end{figure}

}

\frame{
\frametitle{Kern Trick}
\framesubtitle{Beispiel}
Die trennende Hyperebene im $\mathbb{R}^3$ hat die Form
\hspace{-10pt}
\begin{align*}
                & \; \mathbf{w}^\top \mathbf{z} + b =  0 \\
\Leftrightarrow & \; w_1 z_1 + w_2 z_2 + w_3 z_3 + b= 0  \\
\Leftrightarrow & \; w_1 x_1^2 + w_2 \sqrt{2}x_1 x_2 + w_3 x_2^2 + b= 0
\end{align*}

$\Rightarrow$ Gleichung einer Ellipse im $\mathbb{R}^2$, da $(z_1,z_2,z_3) = (x_1^2,\sqrt{2}x_1 x_2, x_2^2)$.

\vfill
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figure/bild3}
\end{figure}
}

\frame{
\frametitle{Kern Trick}
\framesubtitle{Kernfunktion}
\begin{align*}
\text{Bisher:} \hspace{20pt} &f(\mathbf{x}) = \sgn \left ( \sum_{i=1}^N y_i \alpha_i {\color{red} \langle \mathbf{x}_i, \mathbf{x} \rangle} + b \right )\\
\text{Jetzt:} \hspace{20pt}  &f(\mathbf{x}) = \sgn \left ( \sum_{i=1}^N y_i \alpha_i {\color{red}\langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}) \rangle} + b \right )
\end{align*}
%Für jede Beobachtung errechnet man mittels $\Phi$ aus den alten Variablen neue Variablen. In diesem neuen Variablenraum sind die Beobachtungen linear trennbar.
%$\Phi$ wird dabei durch eine Kernfunktion festgelegt. Die Kernfunktion verhält sich im überführten höherdimensionalen Raum wie ein Skalarprodukt, d.h.
 \begin{itemize}
 \item $\Phi$ überführt den Variablenraum in einen höherdimensionalen Variablenraum $\mathcal{M}$
 \item Trainingsdaten sind in $\mathcal{M}$ linear trennbar
 \item $\Phi$ wird durch eine Kernfunktion $K(\mathbf{x}_i, \mathbf{x})$ festgelegt
 \item $K$ verhält sich wie ein Skalarprodukt in $\mathcal{M}$:
  
 \begin{equation*}
 K(\mathbf{x}_i, \mathbf{x}) = \langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}) \rangle
 \end{equation*}
 
 \end{itemize}

}


\frame{
\frametitle{Kern Trick}
\framesubtitle{Wichtige Kernfunktionen}

\begin{itemize}
\item Polynomial: $K(\mathbf{x}_i,\mathbf{x}_j) = (c + \langle \mathbf{x}_i, \mathbf{x}_j \rangle)^d $, für $c$ konstant
\item Radial Basis: $K(\mathbf{x}_i,\mathbf{x}_j) = \exp{ \left ( -\tfrac{||\mathbf{x}_i - \mathbf{x}_j ||}{c} \right ) }$ für $c > 0$
\end{itemize}

Beispiel: $\mathbf{x}_i = (x_{i_1}, x_{i_2})$, $c= 0$, $d=2$

\begin{align*}
K({\color{red}\mathbf{x}_1},\mathbf{x}_2) 
           &= ( \langle {\color{red}\mathbf{x}_1}, \mathbf{x}_2 \rangle )^2 = ( \langle {\color{red}(x_{1_1},x_{1_2})}, (x_{2_1},x_{2_2}) \rangle )^2\\
           &= ({\color{red}x_{1_1}}x_{2_1} + {\color{red}x_{1_2}}x_{2_2})^2 \\
           &= ({\color{red}x_{1_1}^2} x_{2_1}^2  + {\color{red}x_{1_2}^2} x_{2_2}^2 + 2{\color{red}x_{1_1} x_{1_2}}x_{2_1}x_{2_2})\\
           &= \langle ({\color{red}x_{1_1}^2}, {\color{red}x_{1_2}^2}, {\color{red} \sqrt{2} x_{1_1} x_{1_2}}),
              (x_{2_1}^2, x_{2_2}^2, \sqrt{2} x_{2_1} x_{2_2}) \rangle \\
           &= \langle \Phi({\color{red}\mathbf{x}_1}), \Phi(\mathbf{x}_2) \rangle
\end{align*}

}

\subsection{Soft Margin}

\frame{
\frametitle{Soft Margin}
\framesubtitle{Idee}

\begin{tabular}{ll}
Bisher: & Einzelne Außreiser beeinflussen Hyperebene (Overfitting)\\
Jetzt:  & Erlaube Fehlklassifizierung, aber bestrafe diese!
\end{tabular}

<<softmargin, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, fig.width=12, fig.height=6, fig.align='center'>>=

svmpath4 <- svmpath(as.matrix(dat2[,1:2]),as.numeric(as.character(dat2[,"Class"])), param.kernel=2)#, kernel=radial.kernel)
svmpath2 <- svmpath(as.matrix(dat2[,1:2]),as.numeric(as.character(dat2[,"Class"])))

par(mfrow=c(1,2), mar=c(0.5,1,1.5,1))
plot(svmpath4, axes=F, main="Kernel-Trick")
box()

pred1 <- predict(svmpath4, newx=as.matrix(grid1[,1:2]), type="class", 
                 lambda=svmpath4$lambda[length(svmpath4$lambda)])

#grid.fill(xgrid,ygrid,pred1)
# col=pred1
# col[col=="1"] <- rgb(0, 0, 1, 0.25)
# col[col=="-1"] <- rgb(1, 0, 0, 0.25)
# points(grid1,col=col, pch=15, cex=1.2)


plot(svmpath2, axes=F, main="Soft Margin")
box()
pred1 <- predict(svmpath2, newx=as.matrix(grid1[,1:2]), type="class", 
                 lambda=svmpath2$lambda[length(svmpath2$lambda)])
#grid.fill(xgrid,ygrid,pred1)
# col=pred1
# col[col=="1"] <- rgb(0, 0, 1, 0.25)
# col[col=="-1"] <- rgb(1, 0, 0, 0.25)
# points(grid1,col=col, pch=15, cex=1.2)
@

}

\frame{
\frametitle{Soft Margin}
\framesubtitle{Erlaube Fehlklassifizierung}
Nebenbedingung durch Schlupfvariablen $\xi_i \geq 0$ lockern, sodass:

\begin{wrapfigure}{r}{0.5\textwidth}

<<softmargin2, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, out.width='0.5\\textwidth', fig.align='center'>>=
par(mar=c(0.5,0.5,0.5,0.5))
orthogonal(svmpath2, axes=F, main="")
box()
# xgrid<-seq(min(dat2[,1]),max(dat2[,1]), length=40)
# ygrid<-seq(min(dat2[,2]),max(dat2[,2]), length=40)
# grid1<-expand.grid(xgrid,ygrid)
pred1 <- predict(svmpath2, newx=as.matrix(grid1[,1:2]), type="class",
                 lambda=svmpath2$lambda[length(svmpath2$lambda)])
#grid.fill(xgrid,ygrid,pred1)
# col=pred1
# col[col=="1"] <- rgb(0, 0, 1, 0.25)
# col[col=="-1"] <- rgb(1, 0, 0, 0.25)
# points(grid1,col=col, pch=15, cex=1.5)
@
\end{wrapfigure}

$$y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i$$ %, \; i=1,\hdots, N$$

\vfill

Die Trainingsdaten sind für:
\begin{itemize}
\item $\xi_i = 0$ richtig klassifiziert %e Trainingsdaten
\item $0<\xi_i \leq 1$ richtig klassifiziert (innerhalb des Randes)
\item $\xi_i > 1$ fehlklassifiziert %e Trainingsdaten
\end{itemize}

$$\xi_i = \max \{0, 1 - y_i ( \mathbf{w}^\top \mathbf{x}_i + b )\}$$

}

\frame{
\frametitle{Soft Margin}
\framesubtitle{Primäres Optimierungsproblem}
$$\min_{\mathbf{w},b, \pmb{\xi}} \hspace{8pt} {\color{red} \tfrac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^N \xi_i}$$
%\begin{align*}
%\min_{\mathbf{w},b, \pmb{\xi}} \hspace{8pt} \tfrac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^N \xi_i
%\end{align*}
\begin{table}
\centering
\begin{tabular}{crll}
NB: & ${\color{blue}y_i (\mathbf{w}^\top \mathbf{x}_i + b)}$ & ${\color{blue}\geq 1 - \xi_i}$ &\\
    & ${\color{blue}\xi_i}$                                  & ${\color{blue}\geq 0}$          & $\forall i = 1, \hdots, N$
\end{tabular}
\end{table}
\hspace{-10pt}\textbf{Kompromiss:}
%\begin{center}

maximiere Rand ($\min \tfrac{1}{2} ||\mathbf{w}||^2$) %\\
$\leftrightarrow$ minimiere Trainingsfehler $\sum_{i=1}^N \xi_i$
%\end{center}

{\small  $$\hspace{-10pt}L(\mathbf{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) =  {\color{red}\tfrac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^N \xi_i} - \sum_{i=1}^{N} \alpha_i  ({\color{blue}y_i (\mathbf{w}^\top \mathbf{x}_i + b) - (1- \xi_i)}) - \sum_{i=1}^N \mu_i {\color{blue}\xi_i} $$}
}


\frame{
\frametitle{Soft Margin}
%\framesubtitle{Optimierungsproblem}
Parameter $C$ kann durch Kreuzvalidierung bestimmt werden und steuert wie stark Trainingsfehler bestraft werden:

\begin{itemize}
\item $C$ groß: korrekte Klassifizierung der Trainingsdaten wichtiger $\rightarrow$ kleiner Rand
\item $C$ klein: breiter Rand wichtiger $\rightarrow$ $\sum_{i=1}^N \xi_i$ größer
%\item $C \rightarrow \infty$:
%\item $C \rightarrow 0$:
\end{itemize}



<<costparam, echo = FALSE, fig.show='asis', results='hide', message=FALSE, warning=FALSE, fig.width=12, fig.height=4, fig.align='center'>>=
dat3<-dat
dat3$Class[17] <- "-1"
svmpath3 <- svmpath(as.matrix(dat3[,1:2]),as.numeric(as.character(dat3[,"Class"])))

step1 <- svmpath3$Step[length(svmpath3$Step)]
step2 <- svmpath3$Step[length(svmpath3$Step)]-2
step3 <- svmpath3$Step[length(svmpath3$Step)]-23

pred1 <- predict(svmpath3, newx=as.matrix(grid1[,1:2]), type="class", lambda=svmpath3$lambda[step1])
pred2 <- predict(svmpath3, newx=as.matrix(grid1[,1:2]), type="class", lambda=svmpath3$lambda[step2])
pred3 <- predict(svmpath3, newx=as.matrix(grid1[,1:2]), type="class", lambda=svmpath3$lambda[step3])

par(mfrow=c(1,3), mar=c(0,0.5,2,0.5))
plot(svmpath3, step1, axes=FALSE, xlab="", ylab="", cex.main=2)
box()
#grid.fill(xgrid,ygrid,pred1)
#grid.points(pred1, grid1, cex=1.3)
plot(svmpath3, step2, axes=FALSE, xlab="", ylab="", cex.main=2)
box()
#grid.points(pred2, grid1, cex=1.3)
#grid.fill(xgrid,ygrid,pred2)
plot(svmpath3, step3, axes=FALSE, xlab="", ylab="", cex.main=2)
box()
#grid.points(pred3, grid1, cex=1.3)
#grid.fill(xgrid,ygrid,pred3)
@
}


\frame{
\frametitle{Soft Margin}
\framesubtitle{Duales Optimierungsproblem}
Minimierung der Lagrangefunktion bezüglich $\mathbf{w},b \text{ und } \pmb{\xi}$ und Einsetzen der Lösungen in das primäre Optimierungsproblem führt zum dualen Optimierungsproblem (vgl. Folie \ref{primal}):

%{\small
\begin{gather*}
\label{primal2}
%\begin{aligned}
\max_{\pmb{\alpha}}  \left ( \min_{\mathbf{w}, b, \pmb{\xi}} L(\mathbf{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) \right )
= \max_{\pmb{\alpha}} \left ( \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N \alpha_i \alpha_j y_i y_j \mathbf{x}_i^{\top} \mathbf{x}_j \right )\\
\text{NB:} \hspace{20pt} 0 \leq \alpha_i \leq C, \hspace{20pt} \sum_{i=1}^N \alpha_i y_i = 0 \hspace{20pt} \forall i= 1, \hdots, N
%\end{aligned}
\end{gather*}
%}

Weiteres Vorgehen analog zum linear trennbaren Fall.
}

%TODO: zwei arten von SV -> auf den Rand und jenseits ihres Randes

\section{Zusammenfassung und Ausblick}

\frame{
\frametitle{Zusammenfassung und Ausblick}
\framesubtitle{Zusammenfassung}

\textbf{Linear trennbare Daten:} 

%{\footnotesize 
\begin{itemize}
\item Aufstellen der Hyperebenengleichung
\item Bestimmung der Hyperebenenparameter durch maximierung des Randes ($\rightarrow$ Lagrange-Methode)
\end{itemize}
%}

\textbf{Nicht linear trennbare Daten:}

\begin{enumerate}
  \item \textbf{Kern Trick:} 
  \begin{itemize} 
    \item In höherdimensionalen Variablenraum überführen, in dem Trainingsdaten linear trennbar sind.
    \item Bestimmung der Hyperebenenparameter analog.
  \end{itemize}
  \item \textbf{Soft Margin:} 
  \begin{itemize} 
    \item Erlaube Fehlklassifikation, aber bestrafe diese.
    \item Gleiches duales Optimierungsproblem mit zusätzlicher NB.\\
    $\rightarrow$ Vorgehensweise wie bei linear trennbaren Daten.
  \end{itemize}
  \item \textbf{Kern Trick und Soft Margin}
\end{enumerate}
}

\frame{
\frametitle{Zusammenfassung und Ausblick}
\framesubtitle{Ausblick}

\begin{itemize}
\item Erweiterung für Regressionsprobleme
\item Erweiterung für mehrkategorialen Response, z.B. durch \textbf{Paarweise Klassifikation:}
\begin{itemize}
\item Bilde Klassifikatoren für jedes mögliche Paar der $K$ Klassen
\item Zuordung neuer Beobachtungen durch Mehrheitsentscheid in die Klasse $k \in \{1, \hdots, K \}$ 
\end{itemize}
\end{itemize}

\textbf{Beispiel:}

Für $K=3$ gibt es $\tfrac{K(K-1)}{2}= 3$ mögliche Paare / Klassifikatoren:

\begin{table}
\centering
\begin{tabular}{c|ccc}
  & Beob. 1 & Beob. 2 \\ \hline
$k \in \{ 1,2 \}$      & 1       & 1   \\
$k \in \{ 1,3 \}$      & 1       & 3   \\
$k \in \{ 2,3 \}$      & 2       & 3   \\ \hline
Mehrheitsentscheid & $\rightarrow$ 1 & $\rightarrow$ 3
\end{tabular}
\end{table}
}

% \frame{
% \frametitle{Diskussion}
% \framesubtitle{Vor- und Nachteile}
% 
% \textbf{Vorteile:}
% \begin{itemize}
% \item Nicht nur für Klassifikation geeignet
% \item Kernfunktion ermöglicht flexiblere Klassifizierung
% \item Teilmenge der Trainingsdaten für Parameterschätzungen nötig (support vectors)
% \end{itemize}
% 
% \textbf{Nachteile:}
% \begin{itemize} 
% \item Wahl geeigneter Kernfunktion
% \item Überlappende
% \end{itemize}
% 
% }

\section{Literaturverzeichnis}

\begin{frame}[allowframebreaks]
  \frametitle<presentation>{References}    
  \begin{thebibliography}{10}   
  
  \beamertemplatebookbibitems
  \bibitem{kernels}
    B. Schölkopf and A. J. Smola
    \newblock {\em Learning with Kernels: Support vector machines, regularization, optimization, and beyond}
    \newblock Massachussetts Institute of Technology, 2002

  \bibitem{elements}
    J. Friedman, T. Hastie and R. Tibshirani
    \newblock The elements of statistical learning
    \newblock {\em Springer Series in Statistics, 2011}
    
    \beamertemplatearticlebibitems
    \bibitem{svmclassreg}
    S.R. Gunn and others
    \newblock Support vector machines for classification and regression
    \newblock {\em ISIS technical report vol. 14, 1998}
    
%  \beamertemplateonlinebibitems
% \setbeamertemplate{bibliography item}{\includegraphics[width=1em]{vid}}
%     \bibitem{coursera}
%     P. Domingos
%     \newblock Machine Learning Course
%     \newblock {\em University of Washington}
%     \url{https://www.coursera.org/course/machlearning}
  
%  \beamertemplatearticlebibitems
%  \beamertemplatearrowbibitems
  \end{thebibliography}
  Rpackage: \texttt{e1071} Funktion \texttt{svm}
\end{frame}

\frame{
\label{Last}

\begin{center}
\Huge Vielen Dank für die Aufmerksamkeit!
\end{center}
}

%\thispagestyle{empty}

\frame{
\frametitle{Anhang}
\framesubtitle{Erweiterung auf Regressionsprobleme}

\begin{columns}
\begin{column}[T]{0.52\textwidth}
%Bestrafung erfolgt linear und ist nur nötig, wenn $\pmb{\xi}_i > 0$

$$\min \hspace{3pt} \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^N {\color{blue}\xi_i}$$
\vspace{-10pt}
\begin{align*}
\xi_i &= \max \{0, 1 - y_i ( \mathbf{w}^\top \mathbf{x}_i + b )\} \\
&= [1 - y_i \underbrace{( \mathbf{w}^\top \mathbf{x}_i + b )}_{f(\mathbf{x}_i)}]_{+} %= [1 - y_i f(\mathbf{x}_i)]_{+}
\end{align*}

Alternative \textbf{Loss + Penalty} Form:

\end{column}
\begin{column}[T]{0.48\textwidth}
\vspace{-40pt}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{figure/softmargin2}
\end{figure}
\end{column}
\end{columns}

$$\min \hspace{3pt}  \sum_{i=1}^N {\color{blue}[1-y_i (\mathbf{w}^\top \mathbf{x}_i + b)]_{+}} + \frac{\lambda}{2} ||\mathbf{w}||^2 \text{ mit } \lambda = 1/C$$

Verlustfunktion: $L(y_i, f(\mathbf{x}_i)) = [1 - y_i f(\mathbf{x}_i)]_{+}$ (hinge loss) \\
$\Rightarrow$ Verwende andere Verlustfunktion für Regressionsprobleme
}

\frame{
\frametitle{Anhang}
\framesubtitle{Vor- und Nachteile}
\textbf{Vorteile:}
\begin{itemize}
\item Nicht nur für Klassifikation geeignet $\rightarrow$ viele Erweiterungen
\item flexiblere Klassifizierung durch Arbeiten in höheren Dimensionen
\item Parameterschätzungen basieren auf Teilmenge der Trainingsdaten (support vectors) $\rightarrow$ schnelle Klassifizierung
%\item Kommt gut mit unbalanzierten Klassen klar
\end{itemize}

\textbf{Nachteile:}
\begin{itemize} 
\item Geeignete Kernfunktion muss empirisch gesucht werden
\item Geeignete wahl für C $\rightarrow$ muss empirisch gesucht werden
\item Erweiterungen teilweise aufwendig oder ineffizient (z.B. Paarweise Klassifikation)
%\item Überführung in höherer Dimension $\rightarrow$ höhe der Dimension?
\end{itemize}
}

\frame{
\frametitle{Anhang}
\framesubtitle{Herleitung duale Funktion bei linear trennbare Daten}
Mit (1) und (2) von Folie \ref{ableitung} lässt sich zeigen:

{\small
\begin{align*}
&L(\mathbf{w},b,\pmb{\alpha}) = \frac{1}{2} \mathbf{w}^\top \mathbf{w} - \sum_{i=1}^N \alpha_i (y_i (\mathbf{w}^\top \mathbf{x}_i +b) - 1) \\
&\stackrel{(2)}= \frac{1}{2} \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j - \sum_{i=1}^N \alpha_i y_i \left ( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j^\top \right ) \mathbf{x}_i - \sum_{i=1}^N b \alpha_i y_i + \sum_{i=1}^N \alpha_i \\
&\stackrel{(1)}= \frac{1}{2} \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j - \sum_{i=1}^N \alpha_i y_i \left ( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j^\top \right ) \mathbf{x}_i + \sum_{i=1}^N \alpha_i \\
&= - \frac{1}{2} \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j + \sum_{i=1}^N \alpha_i 
\end{align*}
}
}

\frame{
\frametitle{Anhang}
\framesubtitle{Herleitung duale Funktion bei nicht linear trennbare Daten}
{\small
\begin{align}
\frac{\partial L(\mathbf{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu})}{\partial \pmb{\xi}} = 0 
\Rightarrow C = \alpha_i + \mu_i \hspace{20pt} \forall i=1,\hdots, N
\end{align}
}
\vspace{-10pt}
{\small
\begin{align*}
&L(\mathbf{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) = \frac{1}{2} \mathbf{w}^\top \mathbf{w} \; {\color{red} + C \sum_{i=1}^N \xi_i} \\
&- \sum_{i=1}^N \alpha_i (y_i (\mathbf{w}^\top \mathbf{x}_i +b) - (1 - \xi_i)) - \sum_{i=1}^N \mu_i \xi_i\\
&\stackrel{(2)}= \frac{1}{2} \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j \; {\color{red} \cancel{ + \sum_{i=1}^N \overbrace{(\alpha_i + \mu_i )}^{(3) \Rightarrow C} \xi_i}} \\
&- \sum_{i=1}^N \alpha_i y_i \left ( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j^\top \right ) \mathbf{x}_i - \cancel{\sum_{i=1}^N b \alpha_i y_i} + \sum_{i=1}^N \alpha_i \; {\color{red}\cancel{ - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \mu_i \xi_i}} \\
%&= \frac{1}{2} \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j - \sum_{i=1}^N \alpha_i y_i \left ( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j^\top \right ) \mathbf{x}_i + \sum_{i=1}^N \alpha_i \\
%&= - \frac{1}{2} \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i^\top \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j + \sum_{i=1}^N \alpha_i 
\end{align*}
}
}

% \frame{
% \frametitle{Anhang}
% \framesubtitle{Kanonische Hyperebene}
% 
% $$\min_{\mathbf{w},b, \pmb{\xi}} \hspace{8pt} \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^N \xi_i$$
% \begin{table}
% \centering
% \begin{tabular}{crll}
% NB: & $y_i (\mathbf{w}^\top \mathbf{x}_i + b)$ & $\geq 1 - \xi_i$ &\\
%     & $\xi_i$                                  & $\geq 0$          & $\forall i = 1, \hdots, N$
% \end{tabular}
% \end{table}
% 
% }

%TODO: - Regression, Vor- und Nachteile

% \frame{
% \frametitle{Anhang}
% \framesubtitle{Erweiterung für Regression}
% 
% Generalisierung ersetze xi durch NB
% 
% }

\end{document} 