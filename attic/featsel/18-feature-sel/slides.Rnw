%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
library(mlr)
#library(FSelector)
library(mlbench)
set_parent("../style/preamble.Rnw")
@
\newcommand{\xjNull}{x_{j_0}}
\newcommand{\xjEins}{x_{j_1}}
\newcommand{\xl}{\mathbf{x}_l}
\newcommand{\pushcode}[1][1]{\hskip\dimexpr#1\algorithmicindent\relax}

\lecturechapter{11}{Feature Selection}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Introduction}

Feature selection deals with
\begin{itemize}
\item evaluating the influence of features on the model
\item techniques for choosing a suitable subset of features
\end{itemize}

% First, we motivate, state different aims of variable selection and differentiate techniques.

\begin{blocki}{Goals of feature selection}
  \item Find the set of features which lead to an optimal prediction performance. This is an optimization problem.
  \item Create models more efficiently and more cost-effectively, since fewer features are used.
  Fewer features in the model $\Longrightarrow$ faster prediction, less disk space, lower costs for ascertaining features on test data.
  \item Understand models better. Specifically, models with fewer features are easier to visualize and reason about.\linebreak \textbf{Attention}: Often a trade-off between model size and prediction performance is evident!
\end{blocki}
\end{vbframe}


\begin{vbframe}{Feature selection overview}
\begin{center}
\includegraphics{figure_man/varsel_overview.png}
\end{center}

\lz

It is the task of statisticians, data analysts and machine learners to filter out the relevant information which is \textbf{useful} for prediction!
\end{vbframe}


\begin{vbframe}{Motivation Feature Selection}
\begin{itemize}
\item The information about the target class is inherent in the features.
\item Naive theoretical view:
  \begin{itemize}
  \item More features
  \item[$\rightarrow$] more information
  \item[$\rightarrow$] more discriminant power
  \item model does not care about irrelevant features anyway (e.g. by estimating a coefficient to be 0)
  \end{itemize}
\item In practice there are many reasons why this is not the case!
\item Moreover, optimizing is (usually) good, so we should optimize the input-coding.
\end{itemize}

\framebreak

\begin{itemize}
  \item In many domains we are confronted with an increasing number of features with many irrelevant or redundant ones and multiple features of low quality!
  % \item This is due to the fact that there is an increasingly strong automation of measuring methods.
  % Especially the automatized collection of information by computers and the availability of information in the world wide web generates data sets with an extremely high dimensionality.
  \item In domains with many features the underlying distribution function can be very complex and hard to estimate.
  \item Irrelevant and redundant features can \enquote{confuse} learners (recall the \textbf{curse of dimensionality}).
  \item Limited training data
  \item Limited computational resources
  \item Often the usual procedures are designed for $n > p$ problems.
  \item Thus, we either need
  \begin{itemize}
    \item to adapt these procedures to high dimensional data (e.g. by regularizing),
    \item entirely new procedures,
    \item or use the preprocessing methods addressed in this lecture.
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Size of data sets}
\begin{itemize}
  \item \textbf{Classical setting}: Up to around $10^2$ features,

    feature selection might be relevant, but most of the time still manageable.
  \item \textbf{Data sets from medium to high dimensionality}:

    Around $10^2$ up to $10^3$ features, basic methods still often work well.
  \item \textbf{High dimensional data}: From $10^3$ to $10^7$ features,

    Examples are e.g. micro-array / gene-expression data and text-categorization (bag-of-words features).

    If there additionally exist only few observations, this scenario is called $p >> n$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Feature extraction vs. selection}

\begin{columns}
%Both graphs taken out from Tim Conrad's presentation for Novisad (see cim2/external_material/tim_conrad_novisad)

\column{0.49\textwidth}
\textbf{Feature Selection}

\medskip

\includegraphics{figure_man/feat_sel.png}

\smallskip

\begin{itemize}
  \item Select $\tilde{p}$ out of

  $p$ features
  \item Creates a subset of the original features
  \item Helps to understand the classification rules
\end{itemize}

\column{0.49\textwidth}
\textbf{Feature extraction}

\medskip

\includegraphics{figure_man/feat_extraction.png}

\smallskip

\begin{itemize}
  \item Maps inputs to $\tilde{p} < p$ new features
  \item Linear and non-linear combinations of the original features
\end{itemize}

\end{columns}

\framebreak

\begin{center}
\includegraphics[width = \textwidth]{figure_man/feat_sel_vs_feat_extraction.png}
\end{center}


Both, feature selection and feature extraction contribute to:

\begin{itemize}
  \item[$\rightarrow$] Dimensionality reduction
  \item[$\rightarrow$] Simplicity of classification rules
\end{itemize}

Feature extraction can be supervised (PLS, SDR) or unsupervised (PCA, MDS, Manifold Learning)
\end{vbframe}



\begin{vbframe}{Objective function}
Given a set of features $\{1, \dots, p\}$ the feature selection problem is to find a subset $S \subset \{ 1, \dots p \}$ that
maximizes the learners ability to classify patterns.

Formally, $\Psi^*$ should maximize some objective function
$\Psi: \Omega \rightarrow \R$:


$$\Psi^* = \argmax_{{S \in \Omega}} \{ \Psi(S) \}$$

\begin{itemize}
\item $\Omega = \{ 0, 1 \}^p$ is the space of all possible feature subsets of $\{ 1, \dots, p \}$.

\item $S$ can either be a subset of features or a bit-vector characterizing this subset. We will switch between those variants and the context will make it clear what case we are referring to.

\item $\Psi$ can be different \enquote{things}:
  \begin{itemize}
    \item The BIC-score in a linear regression model
    \item The filter score of an mRMR algorithm
    \item The cross-validated test error of a learner
  \end{itemize}
\end{itemize}

\framebreak

\begin{blocki}{How difficult is it to solve the introduced optimization problem, hence, to find the optimal feature set?}
  \item The size of our search space (power set!) is $2^p$.
  \item Hence, this is a discrete optimization problem.
  \item Of course, this does not mean that we have to search the space completely, since there exist more efficient search strategies.
  \item Unfortunately, for the general case, it can be shown that this problem will never be perfectly and efficiently solved (NP-hard).
%  \item Formally spoken: One can show that the problem is NP-hard!
%  \item This means that the problem cannot be solved in polynomial (P) time: ${\mathcal{O}} (p^c)$, where $c \in \N$ indicates the degree o the polynom.
% \end{blocki}
% 
% \framebreak
% 
% \begin{blocki}{How difficult is it to solve the introduced optimization problem, hence, to find the optimal feature set?}
%  \item More precisely, the proof demonstrates that this problem cannot be approximated within any constant, unless P = NP.
% 
%   The latter means, that if you find an algorithm that solves a more difficult class of problems (than this optimization problem) in polynomial time, this implies that you found how to solve all easier problems (including our optimization problem) in polynomial time.
% \item \textbf{Attention}: This does not imply that it is useless trying to construct strategies which work in practice!
  \item Thus, our problem now consists of moving through the search space in a smart and efficient way, and thereby finding a particularly good set of features.
\end{blocki}
\end{vbframe}

\begin{vbframe}{Motivating example: Regularization}
\begin{itemize}
  \item In case of $p \gg n$ \enquote{less fitting is better}.
  \item This can be demonstrated with the following simulation study:
  \item Investigation of three different dimensionalities of the input space: $p \in \{ 20, 100, 1000 \}$
  \item Data generation process for $n = 100$:
  \begin{itemize}
    \item $p$ random variables $x$ are sampled from a standard Gaussian distribution with pairwise correlation of 0.2.
    \item $y$ is generated according to the linear model
  $ y = \sum_{j=1}^p \xj \theta_j + \sigma\epsilon $, where
    \begin{itemize}
      \item $\epsilon$ and $\theta$ are also sampled from a standard Gaussian distributions and
      \item $\sigma$ is chosen such that $\var (\E[y|x]) / \sigma^2 = 2$
    \end{itemize}
  \item 100 simulation runs
  \end{itemize}
  \item A ridge regression model with $\lambda \in \{ 0.001, 100, 1000 \}$ is fitted to the simulated data.
\end{itemize}

\framebreak

\begin{itemize}
  \item Boxplots show the relative test error over 100 simulations for the different values of $p$ and for the different regularization parameters $\lambda \in 0.001, 100, 1000$ from left to right.
  \item $\text{Relative test error} = \text{Test error}/ \text{Bayes error }\sigma^2$
\end{itemize}
\begin{center}
\includegraphics{figure_man/tibshirani_fig_18_1.png}

\footnotesize{Hastie, The Elements of Statistical Learning, 2009}
\end{center}
%$\rightarrow$ More regularization for high dimensional data seems reasonable.

% \begin{itemize}
% \item On the x-axis the effective degrees of freedom (averaged over the 100 simulation runs) are displayed:
% $$df(\lambda) = \sum_{j=1}^p \frac{d_j^2}{d^2_j + \lambda}$$
% where $d$ are the singular values of $x$.
% \begin{itemize}
%   \item For $\lambda = 0$ (linear regression): $df(\lambda) = p$.
%   \item For $\lambda \rightarrow \infty$: $df(\lambda) \rightarrow 0$
% \end{itemize}
% \item $\lambda = 0.001$ (20 df) has smallest relative rest error for $p = 20$.
% \item $\lambda = 100$ (35 df) has smallest relative rest error for $p = 100$.
% \item $\lambda = 1000$ (43 df) has smallest relative rest error for $p = 1000$.
% \item[$\Rightarrow$] More regularization for high dimensional data seems reasonable.
% \end{itemize}
\end{vbframe}

\begin{vbframe}{Motivating example: Different methods}
Prediction results of eight different classification methods on microarray data with $| \Dtrain | = 144$, $| \Dtest | = 54$, $p=16 063$ genes and a categorical target, which specifies the type of cancer out of 14 different cancer types.

\begin{center}
\includegraphics{figure_man/tibshirani_tab_18_1.png}

\footnotesize{Hastie, The Elements of Statistical Learning, 2009}
\end{center}
\end{vbframe}

\begin{vbframe}{Motivating example: methods with integrated selection}

\begin{itemize}
\item We simulate data with function \code{sim.data} of the package \pkg{penalizedSVM}.
\item With \code{sim.data} one can simulate micro-array data.
\item Each simulated sample has \code{sg} genes.
\item \code{nsg} genes are relevant and affect the class levels.
\item The other \code{ng-nsg} have no influence.
\item The gene rations are drawn from a multivariate normal distribution.
\end{itemize}

\framebreak

\begin{itemize}
\item Now, we draw 200 observations with 50 features of which 25 are positive and 25 are negative correlated with the category.
\item Furthermore, we create 50 irrelevant features.
\item We compare several classification models regarding the misclassification rate.
\item Since we have relatively few data, we use repeated cross-validation with 10 folds and 10 repetitions.
\end{itemize}

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{rrrrrrr}
      \hline
      & rpart & lda & logreg & nBayes & 7nn  & rForest \\
      \hline
      all feat. & 0.44 & 0.27 & 0.25 & 0.32 & 0.37 & 0.36 \\
      relevant feat. & 0.44 & 0.18 & 0.19 & 0.27 & 0.33 & 0.30 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

The models with integrated selection do not work very well here!

If we knew the relevant features, we would achieve a significant improvement!
\end{vbframe}

\begin{vbframe}{Examples, taken from Guyon (2003)}

\begin{figure}
  \includegraphics[width=9cm]{figure_man/varsel_ex0.png}
\end{figure}

\framebreak

\begin{figure}
  \includegraphics[width=9cm]{figure_man/varsel_ex1.png}
\end{figure}

\framebreak

\begin{figure}
  \includegraphics[width=9cm]{figure_man/varsel_ex2.png}
\end{figure}

\end{vbframe}


\begin{vbframe}{Types of feature selection methods}

In the following, we will get to know three different types of methods for feature selection:

\lz

\begin{itemize}
  \item Filter
  \item Wrapper
  \item Embedded techniques
\end{itemize}

\lz

For each technique we will look at a simple example.

\lz

It should be noted that also complicated combinations of methods can occur in practice!

\end{vbframe}



\begin{vbframe}{Filter}

\begin{itemize}
  \item \textbf{Filter methods} are strongly related to methods for determining variable importance.
  \item Mostly they construct a measure that describes the strength of the (univariate) dependency between a feature and the target variable.
  \item Typically this yields a numeric score for each feature $j$.
  That's what's known as \textbf{variable-ranking}.
  \item In contrast to the variable importance measures we already know, filters are in general independent of a specific classification learner and can be applied generically.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Filter: $\chi^2$-statistic}
\begin{itemize}
  \item Test for independence between the $j$-th feature and the target $y$.
  \item Numeric features or targets need to be discretized.
  \item Hypotheses:

  $H_0: p(x = j, y = k) = p(x = j)\, p(y = k)$, $\forall j = 1, \dots, k_1$

  \noindent\hspace*{6.55cm} $\forall k = 1, \dots, k_2$

  $H_1: \exists \; j, k: p(x = j, y = k) \neq p(x = j)\, p(y = k)$
  \item Calculate the $\chi^2$-statistic for each feature-target combination:
    $$ \chi^2 = \sum_{j = 1}^{k_1} \sum_{k=1}^{k_2} (\frac{e_{jk} - \tilde{e}_{jk}}{\tilde{e}_{jk}}) \;\;\;   \stackrel{H_0}{\underset{approx.}{\sim}} \; \chi^2 ((k_1-1)(k_2-1))$$
  where $e_{jk}$ is the observed relative frequency of pair $(j,k)$ and $\tilde{e}_{jk} = \frac{e_{i \cdot} e_{\cdot j}}{N}$ is the expected relative frequency.
  \item The greater $\chi^2$, the more dependent is the feature-target combination, the more relevant is the feature.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Filter: Pearson \& Spearman correlation}
\textbf{Pearson correlation $r(\xj, y)$: }
\begin{itemize}
  \item For numeric features and targets only.
  \item Most sensitive for linear or monotonic relationships.
  \item $ r(\xj, y) = \frac{\sum_{i=1}^n (\xi_j - \bar{x}_j) (\yi - \bar{y})}{\sqrt{\sum_{i=1}^n (\xi_j - \bar{x}_j)} \sqrt{(\sum_{i=1}^n \yi - \bar{y})}},\qquad -1 \le r \le 1$
\end{itemize}
\textbf{Spearman correlation $r_{SP}(\xj, y)$:}
\begin{itemize}
  \item For features and targets at least on ordinal scale.
  \item Equivalent to Pearson correlation computed on the ranks.
  \item Assesses monotonicity of the dependency relationship.
  % \item Calculate the Spearman correlation coefficient between each feature-target combination:
  % $$ r_{SP} = \frac{\sum (rg(\xi) - \bar{rg}_x) (rg(\yi) - \bar{rg}_y)}{\sqrt{\sum (rg(\xi) - \bar{rg}_x)^2 \sum (rg(\yi) - \bar{rg}_y)^2}}$$
  % $-1 \le r_{SP} \le 1$
  % 
  % $r_{SP} > 0$: positive correlation.
  % 
  % $r_{SP} < 0$: negative correlation.
  % 
  % $r_{SP} = 0$: no correlation.
  % \item A higher score indicates higher relevance of the feature.
\end{itemize}
\lz
Use absolute values $|r(\xj, y)|$ for feature ranking: higher score indicates higher relevance.

\framebreak

Only \textbf{linear} dependency structure, non-linear (non-monotonic) aspects are not captured by $r$:

\lz

\begin{center}
  \includegraphics[width=0.9\textwidth]{figure_man/correlation_example.png}

  \scriptsize{\url{https://en.wikipedia.org/wiki/Pearson\_correlation\_coefficient\#/media/File:Correlation\_examples2.svg}}
\end{center}
% 
% \begin{vbframe}{Filter: Rank correlation}
% \begin{itemize}
%   \item For features and targets at least on ordinal scale.
%   \item Equivalent to Pearson correlation computed on the ranks.
%   \item Assesses monotonicity of the dependency relationship.
  % \item Calculate the Spearman correlation coefficient between each feature-target combination:
  % $$ r_{SP} = \frac{\sum (rg(\xi) - \bar{rg}_x) (rg(\yi) - \bar{rg}_y)}{\sqrt{\sum (rg(\xi) - \bar{rg}_x)^2 \sum (rg(\yi) - \bar{rg}_y)^2}}$$
  % $-1 \le r_{SP} \le 1$
  % 
  % $r_{SP} > 0$: positive correlation.
  % 
  % $r_{SP} < 0$: negative correlation.
  % 
  % $r_{SP} = 0$: no correlation.
  % \item A higher score indicates higher relevance of the feature.
%   \end{itemize}
% \end{vbframe}
\end{vbframe}

\begin{vbframe}{Filter: Distance correlation}
% sources: 
%Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007), Measuring and Testing Dependence by Correlation of Distances, Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
% http://dx.doi.org/10.1214/009053607000000505
%Szekely, G.J. and Rizzo, M.L. (2009), Brownian Distance Covariance, Annals of Applied Statistics, Vol. 3, No. 4, 1236-1265. 
% http://dx.doi.org/10.1214/09-AOAS312 

$$r_D(\xj, y) = \sqrt{\frac{c_{D}^2(\xj, y)}{\sqrt{c_{D}^2(\xj, \xj) c_{D}^2(y, y)}}}:$$
Normed version of \textbf{distance covariance} $c_{D}(\xj, y) = \frac{1}{n^2}\sum^n_{i=1}\sum^n_{k=1} D^{(ik)}^{(ik)}_{\xj} D^{(ik)}_{y}$ for
\begin{itemize}
\item distances of observations: $d\left(\xi_j, x^{(k)}_j\right)$,
\item mean distances $\bar{d}^{(i\cdot)}_{xj} = \tfrac{1}{n} \sum^n_{k=1} d\left(\xi_j, x^{(k)}_j\right)$,
\item and centered pairwise distances
$D^{(ik)}_{x} = d\left(\xi_j, x^{(k)}_j\right) - (\bar{d}^{(i\cdot)}_{xj} + \bar{d}^{(\cdot k)}_{xj} - \bar{d}^{(\cdot \cdot)}_{xj})$
\end{itemize}

\framebreak

Properties:
\begin{itemize}
\item $0 \leq r_D(x, y) \leq 1$
\item $r_D(x, y) = 0$ only if $\xv$ and $y$ are empirically independent (!)
\item $r_D(x, y) = 1$ for exact linear dependencies
\item also assesses strength of \textbf{non-monotonic}, \textbf{non-linear}  dependencies
\item very generally applicable since it only requires distance measures on $\Xspace$ and $\Yspace$: can also be used for ranking multivariate features, feature combinations or non-tabular inputs (text, images, audio, etc.)
\item expensive to compute for large data (i.e., use a subsample)
\end{itemize}

<<corr-example, echo = FALSE, fig.height  = 1.3*5.8, fig.width   = 1.3*8>>=
library(datasauRus)
library(energy)
library(dplyr)
library(tidyr)

cortable <- datasaurus_dozen %>% group_by(dataset) %>% 
  summarize(pearson = cor(x, y), spearman = cor(x,y, method = "spearman"), 
    distance = dcor(x, y), 
    minx = min(x), miny=median(y), maxy= max(y)) %>% 
  gather(key = "method", value = "value", -dataset, -minx, -miny, -maxy) %>% 
  mutate(label = recode(method, pearson = "r", spearman = "r[SP]", 
    distance =  "r[d]"), 
    label = paste(label, round(value, 2), sep = "==")) %>% 
  group_by(dataset) %>% 
  summarize(label = paste("atop(", paste("atop(", label[1], ", ",label[2], ")"),", ", paste("atop(", label[3], ", ",label[4], ")"), ")"), minx=min(minx), maxy=min(maxy), 
    miny = min(miny))
ggplot(datasaurus_dozen, aes(x = x, y = y)) +
    geom_point(alpha = .3) +
    theme(legend.position = "none") +
    facet_wrap( ~ dataset, ncol = 3, labeller = function(...) "") + 
  geom_label(data = cortable, aes(x = 15, y = 50, 
    label = label), alpha = .7, col = "red", label.size = NA, nudge_x = 5, parse = TRUE, size = 5)
@
\end{vbframe}




\begin{vbframe}{Filter: Welch's t-test}
\begin{itemize}
  \item For binary classification with numeric features.
  \item Test for unequal means of the $j$-th feature.
  \item For notational purposes let $\Yspace \in \{ 0, 1\}$. Then the subscript $j_0$ refers to the $j$-th feature where $y = 0$ and $j_1$ where $y = 1$.
  \item Hypotheses:

  $H_0$: $\;\;\mu_{j_0} = \mu_{j_1} $ \qquad vs. \qquad $H_1$: $\;\;\mu_{j_0} \neq \mu_{j_1}$
  \item Calculate Welch's t-statistic for every feature $\xj$
  $$ t_j = \frac{\bar{x}_{j_0} - \bar{x}_{j_1}}{\sqrt{(\frac{S^2_{\xjNull}}{n_0} + \frac{S^2_{\xjEins}}{n_1})}}$$
  where $\bar{x}_{j_0}$, $S^2_{\xjNull}$ and $n_0$ are the sample mean, the population variance and the sample size for y = 0, respectively.
  \item A higher t-score indicates higher relevance of the feature.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Filter: F-Test}
\begin{itemize}
  \item For multiclass classification ($g \ge 2$) and numeric features
  \item Assesses whether the expected values of a feature $\xj$ within the classes of the target differ from each other.
  \item Hypotheses:

  $H_0: \mu_{j_0} = \mu_{j_1} = \dots = \mu_{j_g} \;\;\;\;$ vs. $\;\;\;\;H_1 : \exists \; k,l: \mu_{j_k} \neq \mu_{j_l}$
  \item Calculate the F-statistic for each feature-target combination:
  \begin{align*}
  F &= \frac{\text{between-group variability}}{\text{within-group variability}}\\
  F &= \frac{\sum_{k = 1}^g n_k (\bar{x}_{j_k} - \bar{x_j})^2/(g-1)}{\sum_{k = 1}^g \sum_{i = 1}^{n_k} (x_{j_k}^{(i)} - \bar{x}_{j_k})^2/(n-g)}
  \end{align*}
  where $\bar{x}_{j_k}$ is the sample mean of feature $x_j$ where $y = k$ and $\bar{x_{j}}$ is the overall sample mean of feature $x_j$.
\item A higher F-score indicates higher relevance of the feature.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Filter}
\begin{blocki}{How to combine a filter with a model:}
  \item Calculate filter-values.
  \item Sort features by value.
%  \item Choose $\tilde{p}$ best features.
  \item Train model on $\tilde{p}$ best features.
\end{blocki}

\lz

\begin{blocki}{How to choose $\tilde{p}$:}
  \item Can be prescribed by the application.
  \item Eyeball estimation: Read from filter plots (i.e., Scree plots).
  \item Use resampling.
\end{blocki}

\framebreak

\begin{blocki}{Advantages:}
  \item Easy to calculate.
  \item Typically scales well with number of features $p$.
  \item Mostly interpretable intuitively.
  \item Combination with every model possible.
\end{blocki}

\begin{blocki}{Disadvantages:}
  \item Often univariate (not always), ignores multivariate dependencies.
  \item Redundant features will have similar weights.
  \item Ignores learning algorithm.
\end{blocki}
\end{vbframe}


\begin{vbframe}{Minimum Redundancy Maximum Relevancy}
\begin{itemize}
  \item Most filter type methods select top-ranked features based on a certain filter method ($\chi^2$, rank-correlation, t-test,...) without considering relationships among the features.
  \item Problems:
  \begin{itemize}
  \item Features may be correlated and hence, may cause redundancy.
  \item Selected features cover narrow regions in space.
  \end{itemize}
  \item Goal: In addition to maximum relevancy of the features we want the features to be maximally dissimilar to each other (minimum redundancy).
  \item Features can be either continuous or categorical.
\end{itemize}
\end{vbframe}

\begin{vbframe}{MRMR: Criterion functions}
\begin{itemize}
\item Let $S \subset \{1, \dots, p \}$ be a subset of features we want to find.
\item Minimize redundancy

 $$\min \text{Red}(S), \;\;\;\; \text{Red}(S) = \frac{1}{|S|^2} \sum_{j, l \in S} I_{xx}(\xjb, \xl)$$

\item Maximize relevancy

 $$\max \text{Rel}(S), \;\;\;\; \text{Rel}(S) = \frac{1}{|S|} \sum_{j \in S} I_{xy}(\xjb, \ydat)$$

\item $I_{xx}$ measures the strength of the dependency between two features.
\item $I_{xy}$ measures the strength of the dependency between a feature and the target.
\end{itemize}

\framebreak

\begin{itemize}

\lz

\item Examples for $I_{xx}$:
  \begin{itemize}
  \item Two discrete features: mutual information $I(\xjb, \xl)$
  \item Two numeric features: correlation $|r(\xjb, \xl)|$
  \end{itemize}

\lz

\item Examples for $I_{xy}$:
  \begin{itemize}
  \item Discrete feature, discrete target: mutual information $I(\xjb, \ydat)$
  \item Continuous feature, discrete target: F-statistic $F(\xjb, \ydat)$
  \end{itemize}
\end{itemize}


\framebreak

\begin{itemize}
\item The mRMR feature set is obtained by optimizing the relevancy and the redundancy criterion simultaneously.
\item This requires combining the criteria into a single objective function:
$$\Psi(S) = (\text{Rel}(S) - \text{Red}(S)) \;\;\;\; \text{ or } \;\;\;\; \Psi(S) = (\text{Rel}(S)/\text{Red}(S))$$

%\item Exact solution requires $\mathcal{O}(|\Xspace|^{|S|})$ searches, where $|\Xspace|$ is the number of features in the whole feature set and $|S|$ is the number of features selected.
\item In practice, incremental search methods are used to find near-optimal feature sets defined by $\Psi$:
\item Suppose we already have a feature set with $m-1$ features $S_{m-1}$.
\item Next, we select the $m$-th feature from the set $\bar{S}_{m-1}$ by selecting the feature that maximizes:
$$\max_{j \, \in \, \bar{S}_{m-1}} \Biggl[ I_{xy} (\xjb, \ydat) - \frac{1}{|S_{m-1}|} \sum_{l \, \in \, S_{m-1}} I_{xx}(\xjb, \xl)  \Biggr]$$
\item Complexity of this incremental algorithm is $\mathcal{O}(|p| \cdot| S|)$.
\end{itemize}
\end{vbframe}

\begin{vbframe}{MRMR: Algorithm}

\begin{algorithm}[H]
\footnotesize
  \caption*{MRMR algorithm}
  \begin{algorithmic}[1]
    \State Set $S = \emptyset$, $R = \{ 1, \dots, p \}$
    \State Find the feature with maximum relevancy:
    $$j^* := \argmax_{j} I_{xy} (\xjb, \ydat)$$
    \State Set $S = \{ j^* \}$ and update $R \leftarrow R \setminus \{j^* \}$
    \Repeat
      \State Find feature $\xj$ that maximizes:
      $$\max_{j \, \in \, R} \Biggl[ I_{xy} (\xjb, \ydat) - \frac{1}{|S|} \sum_{l \, \in \, S} I_{xx}(\xjb, \xl)  \Biggr]$$
      \State Update $S \leftarrow S \cup \{j^* \}$ and $R \leftarrow R \setminus \{ j^* \}$
    \Until{Expected number of features have been obtained or some other constraints are satisfied.}
  \end{algorithmic}
\end{algorithm}
\end{vbframe}


\begin{vbframe}{Filter: Practical example}
% <<echo=TRUE>>=
% # Calculate the information gain as filter value
% fv = generateFilterValuesData(iris.task, method = "information.gain")
%
% # We can also pass multiple filter methods at once:
% fv2 = generateFilterValuesData(iris.task,
%   method = c("information.gain", "chi.squared"))
%
% # fv2 is a FilterValues object and fv2$data holds a data frame
% # with the importance values for all features
% fv2$data
% @
%
% <<>>=
% plotFilterValues(fv2) + ggtitle("Filters for the 4 features of iris dataset")
% @
%
%
% \framebreak
%
% <<size="tiny", echo=TRUE>>=
% ## Keep the 2 most important features
% filtered.task = filterFeatures(iris.task, method = "information.gain", abs = 2L)
%
% ## Keep the 25% most important features
% filtered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)
%
% ## Keep all features with importance greater than 0.5
% filtered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5)
% filtered.task
% @
%
% \framebreak

\begin{itemize}
  \item Filter methods are often part of the data preprocessing process, where in a subsequent step a learning method is applied.
  \item In practice we want to automate this, so the feature selection can be part of the validation method of our choice:
\end{itemize}

<<size="tiny", echo=TRUE>>=
lrn = makeFilterWrapper(learner = "classif.kknn",
  fw.method = "anova.test", fw.abs = 2L)
ps = makeParamSet(
  makeDiscreteParam("fw.abs", values = 1:4)
)
ctrl = makeTuneControlGrid()
rdesc = makeResampleDesc("CV", iters = 10L)

res = tuneParams(lrn, task = iris.task, resampling = rdesc, par.set = ps,
  control = ctrl)
res$opt.path
@

\end{vbframe}

\begin{vbframe}{Wrapper}

\begin{itemize}
  \item Wrapper methods emerged from the idea that different sets of features can be optimal for different classification learners.
  \item Given a set of features, we can use the classifier itself to assess their quality.
  \item We could just evaluate on the test set or use resampling techniques to achieve this.
  \item A wrapper is nothing else than a discrete search strategy for $S$ where the cross-validated test error of a learner as a function of $S$ is now the objective criterion.

\end{itemize}


\framebreak

There are a lot of varieties of wrappers. To begin with we have to determine the following components:

\lz

\begin{itemize}
  \item A set of starting values
  \item Operators to create new points out of the given ones
  \item A termination criterion
\end{itemize}

\framebreak

\begin{figure}
  \includegraphics[width=8cm]{figure_man/varsel_space.png}
  \caption{Space of all feature sets for 4 features.
  The indicated relationships between the sets insinuate a greedy search strategy, which either adds or removes a feature.}
  % Übersetzung von:
  % Raum aller Feature-Mengen bei 4 Features. Die eingezeichnete Nachbarschaftsbeziehung unterstellt eine Art ,,gierige'' Suchstrategie, bei der wir entweder ein Feature hinzufügen oder entfernen.}
\end{figure}

\framebreak

\begin{blocki}{Greedy forward search}
  \item Let $S \subset \{1, \dots, p \}$ where $\{1, \dots p \}$ is an index set of all features.
  \item Start with the empty feature set $S = \emptyset$.
  \item For a given set $S$ generate all $S_j = S \cup \{j\}$ with $j \notin S$.
  \item Evaluate classifier on $S_j$ and use the best $S_j$.
  \item Iterate over this procedure.
  \item Terminate if:
    \begin{itemize}
      \item The performance measure no longer shows relevant improvement.
      \item A maximum number of features is used.
      \item A given performance value is reached.
    \end{itemize}
\end{blocki}

\framebreak

\textbf{Example for greedy forward search on iris data}
\begin{center}
\includegraphics[width = 0.6\textwidth]{figure_man/wrapperanim1.png}
\end{center}

\framebreak

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim2.png}
\end{center}

\framebreak

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim3.png}
\end{center}

\framebreak

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim4.png}
\end{center}

\framebreak

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim5.png}
\end{center}

\framebreak

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim6.png}
\end{center}

\framebreak

\begin{blocki}{Greedy backward search:}
  \item Start with the full index set of features $S = \{1, \ldots, p\}$.
  \item For a given set $S$ generate all

  $S_j = S \setminus\{j\}$ with $j \in S$.
  \item Evaluate classifier on $S_j$\
    and use the best $S_j$.
  \item Iterate over this procedure.
  \item Terminate if:
    \begin{itemize}
      \item The performance drops drastically.
      \item A given performance value gets undershot.
    \end{itemize}
  \end{blocki}

\framebreak

\begin{blocki}{Extensions:}
  \item Eliminate or add several features at once, to increase speed.
  \item Allow alternating forward and backward search.
  \item Randomly create candidate feature sets in each iteration.
  \item Continue search based on the set of features where an improvement is present.
  \item Use improvements of earlier iterations.
\end{blocki}

\framebreak

\begin{algorithm}[H]
\caption*{A simple 1+1 genetic algorithm}
\begin{algorithmic}[1]
  \State Start with a random set of features $S$ (bit vector $b$).
  \Repeat
  \State Flip a couple of bits in $b$ with probability $p$.
  \State Generate set $S'$ and bit vector $b'$, respectively.
  \State Judge the classifiers performance on $S'$.
  \State If $S'$ performs better than $S$, update $S \leftarrow S'$, otherwise $S \leftarrow S$.
  \Until One of the following conditions are met:
    \begin{itemize}
      \item A given performance value is reached.
      \item Budget is exhausted.
    \end{itemize}
\end{algorithmic}
\end{algorithm}

\framebreak

\begin{blocki}{Advantages:}
  \item Can be combined with every learner.
  \item Combinable with every performance measure.
  \item Optimizes the desired criterion directly.
\end{blocki}

\lz

\begin{blocki}{Disadvantages:}
  \item Evaluating the target function is expensive.
  \item Doesn't scale well if number of features gets big.
  \item Doesn't use much structure or available information from our model.
\end{blocki}

% \framebreak
%
% <<size="tiny", echo=TRUE>>=
% # specify the search strategy.
% # We want to use forward search:
% ctrl = makeFeatSelControlSequential(method = "sfs")
% ctrl
%
% # Selected features
% sfeats = selectFeatures(learner = "regr.lm", task = bh.task,
%   resampling = rdesc, control = ctrl, show.info = FALSE)
% sfeats
% @
%
% \framebreak
%
% <<size="tiny", echo=TRUE>>=
% # Visualize optimization path
% analyzeFeatSelResult(sfeats)
% @
%
% \framebreak
%
% <<size="tiny", echo=TRUE>>=
% # Fuse a base-learner with a search strategy (here: sfs)
% lrn = makeFeatSelWrapper("classif.rpart", resampling = rdesc,
%   control = ctrl, show.info = FALSE)
% res = resample(lrn, iris.task, resampling = rdesc,
%   show.info = FALSE, models = TRUE, extract = getFeatSelResult)
% res$extract[1:5]
% @
\end{vbframe}

\begin{vbframe}{Embedded Feature Selection}

\begin{itemize}
  \item Embedded techniques are methods that integrate feature selection directly into the learning process.
  \item They use an internal criteria of the applied learner, to have better control over the search for useful features.
  \item Embedded techniques usually have to be developed specifically for each learner.
  \item In the following, we will look at some examples.
  Some of them were already discussed in the previous lectures.
\end{itemize}

\end{vbframe}

\begin{vbframe}{SVM: Recursive Feature Elimination (RFE)}
\begin{itemize}
  \item RFE is a popular backward search technique for the linear SVM and the 2-class problem.
  \item Here we will always assume standardized features. (This should be the case for an SVM anyway!)
  \item Coefficient size $|\theta_j|$ tells us how much impact feature $j$ has on our classification
  since $f(x) = \theta^Tx + \theta_0$ is the decision function.
\end{itemize}

Idea: Sequentially drop the feature with the smallest $|\theta_j|$

\framebreak

\begin{blocki}{Recursive feature elimination}
  \item Standardize the data.
  \item Start with full set of features $S$.
  \item Fit a linear SVM, using the features in $S$, and estimate coefficients $\theta$.
  \item Remove feature(s) $j$ where $|\theta_j|$ is minimal from $S$.
  \item Iterate using the reduced $S$ for the SVM.
\end{blocki}

\framebreak

\begin{blocki}{Some notes:}
  \item Strictly speaking this procedure doesn't perform a selection, but rather constructs a ranking of the features.
  \item As for filters, we need an extra criterion for termination / selection.
  \item To improve speed one can drop $k$ features in each iteration.
  \item Extensions for other kernels or multi-class-tasks are not trivial.
\end{blocki}

\end{vbframe}

\begin{vbframe}{L1-Penalization / LASSO}

LASSO: least absolute shrinkage and selection operator
\begin{itemize}
  \item Linear methods that regularize the coefficients of the model with a $L_1$-penalty in the empirical risk
  $$ \risk(\beta) = \sum_{i=1}^n L(\theta^T \xi + \theta_0, \yi) +\lambda \sum_{j=1}^p |\theta_j| $$
  are very popular for high dimensional data.
  \item The penalty summand shrinks the coefficients towards 0 in the final model.
  \item Many (improved) variants: group LASSO, adaptive LASSO, ElasticNet, ...
  \item Has some very nice optimality results: e.g., compressed sensing
\end{itemize}
\end{vbframe}

\begin{vbframe}{Solving a feature selection problem (taken from Guyon (2003))}

\begin{enumerate}
  \item {\bf Do you have domain knowledge?}

    If yes, construct a better set of \enquote{ad hoc} features.
  \item {\bf Are your features commensurate?}

    If no, consider normalizing them.
  \item {\bf Do you suspect interdependence of features?}

  If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.
  \item {\bf Do you need to prune the input features (e.g. for cost, speed or data understanding reasons)}?

  If no, construct disjunctive features or weighted sums of features (e.g. by clustering or matrix factorization).
  \item {\bf Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)}?

  If yes, use a variable ranking method.
  Else, do it anyway to get baseline results.
  \item {\bf Do you need a predictor?}

  If no, stop.
  \item {\bf Do you suspect your data is \enquote{dirty} (has a few meaningless input patterns and/or noisy outputs or wrong class labels)?}

  If yes, detect the outlier examples using the top ranking features obtained in step 5 as representation; check and/or discard them.
  \item {\bf Do you know what to try first?}

  If no, use a linear predictor.
  Use a forward selection method, with the \enquote{probe} method as a stopping criterion or use the L0-norm embedded method.
  For comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features.
  Can you match or improve performance with a smaller subset?
  If yes, try a non-linear predictor with that subset.
  \item {\bf Do you have new ideas, time, computational resources, and enough examples?}

  If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods.
  Use linear and non-linear predictors.
  Select the best approach with model selection.
  \item {\bf Do you want a stable solution (to improve performance and/or understanding)?}

  If yes, sub-sample your data and redo your analysis for several bootstraps.
\end{enumerate}

\end{vbframe}

\begin{vbframe}{Open points / problems}

\begin{itemize}
  \item In general it is difficult to give suggestions when to use which feature selection method.
  \item Mostly it is reasonable to start with a simple fast method.
  If this results in unsatisfying results one can gradually move to more expensive methods.
  \item Not every introduced method can be generalized for multi-class-problems in an easy fashion.
  \item Combining the choice for an appropriate classifier and parameter tuning with feature selection is not simple.
\end{itemize}

\end{vbframe}

\endlecture
