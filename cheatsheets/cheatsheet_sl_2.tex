\documentclass{beamer}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{nicefrac}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-svm}


\title{SL :\,: BASICS} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ \invisible{x} % Package description in header
	% The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}
	

	
\begin{document}
\begin{frame}[fragile]{}
\vspace{-8ex}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First Column begin
%-------------------------------------------------------------------------------
% Data
%-------------------------------------------------------------------------------
\begin{myblock}{Information Theory (Continuous)}
%	
%	
	Differential entropy: Continuous random variable $X$ with density function $f(x)$ and support $\Xspace:$
%	
		$$ h(X) := h(f) := - \int_{\Xspace} f(x) \log(f(x)) dx $$
%
	Joint differential entropy: Continuous random vector $X$ with density function $f(x)$ and support $\Xspace:$
%	
		$$ h(X) = h(X_1, \ldots, X_n) = h(f) = - \int_{\Xspace} f(x) \log(f(x)) dx $$
%
	Properties:
%	
	\begin{enumerate}
		\setlength{\itemindent}{+.3in}
	\item $h(f)$ can be negative.
	\item $h(f)$ is additive for independent RVs.
	\item $h(f)$ is maximized by the multivariate normal, if we restrict 
	to all distributions with the same (co)variance, so
	$h(X) \leq \frac{1}{2} \ln(2 \pi e)^n |\Sigma|.$
	% We postpone the proof to a later chapter, as it is based on Kullback-Leibler divergence.
	% $H(X) \leq -g\frac{1}{g} \log_2(\frac{1}{g}) = log_2(g)$.
	\item Translation-invariant, $ h(X+a) = h(X)$. 
%	\item $h(aX) = h(X) + \log |a|$.
	\item $h(AX) = h(X) + \log |A|$ for random vectors and matrix A.
	\end{enumerate}
%
	\textbf{Conditional entropy} of $Y$ given $X$ (both continuous):
	%
	$$h(Y|X) = - \int f(x,y) \log f(x|y) dx dy.$$
	%
	\textbf{Mutual information:}
%	 
	\begin{equation*}\begin{aligned}
			I(X ; Y) &= \int f(x,y) \log \frac{f(x,y)}{f(x)f(y)} dx dy.
		\end{aligned}
	\end{equation*}
%
	\textbf{Cross-entropy} of two densities $p$ and $q$ on the same domain $\Xspace:$
	%
	$$ H_p(q) = \int q(x) \ln\left(\frac{1}{p(x)}\right) dx = - \int q(x) \ln\left(p(x)\right) dx $$
	%
	\textbf{Kullback-Leibler Divergence}:
	%Kullback-Leibler Divergence
	$$ D_{KL}(p \| q) = \E_p \left[\log \frac{p(X)}{q(X)}\right] = \int_{x \in \Xspace} p(x) \cdot \log \frac{p(x)}{q(x)} $$
	% 	
\end{myblock}
%
%-------------------------------------------------------------------------------
%   
%-------------------------------------------------------------------------------
\begin{myblock}{Hypothesis Space}
%
	\textbf{Underfitting:} Failure to obtain a sufficiently low training error. \\
	\textbf{Overfitting:} Large difference in training and test error.\\
	
	\textbf{VC dimension:} General measure of the complexity of a function space.
	%
	The \textbf{VC dimension} of a class of binary-valued functions $\Hspace = \{h: \Xspace \to \{0, 1\}\}$ is defined to be the largest number of points in $\Xspace$ (in some configuration) that can be shattered by members of $\Hspace$. \\
	%
	Notation: $VC_p(\Hspace)$, where $p$ denotes the dimension of $\Xspace$.\\
	% 
	\textbf{Shattering:} A set of points is said to be \textbf{shattered} by a class of functions if  a member of this class can perfectly separate them no matter how we assign binary labels to the points.
	%
\end{myblock}\vfill
% End First Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Second Column
\begin{myblock}{Regularization}  
%
	Regularized Empirical Risk:
	$$
	\riskrf = \riskef + \lambda \cdot J(f)  
	$$
	\begin{itemize}
		\setlength{\itemindent}{+.3in}
	%	
		\item $J(f)$ is the \textbf{complexity/roughness penalty} or \textbf{regularizer}.
		\item $\lambda > 0$ is the \textbf{complexity control} parameter. 
		\item For parameterized hypotheses: $\riskrt = \risket + \lambda \cdot J(\thetab)$. 
	\end{itemize}
	%
	Tackles the trade-off: \emph{maximizing} the fit (minimizing the train loss) vs.\ \emph{minimizing} the complexity of the model. \\
	
	%
	Regularization in the linear model ($\fx = \thetab^\top \xv$):
	%
	\begin{itemize}
		\setlength{\itemindent}{+.3in}
		%	
		\item Ridge regression: $J(\thetab) =  \|\thetab\|_2^2 = \thetab^\top \thetab.$
		\item Lasso regression: $J(\thetab) =  \|\thetab\|_1 = \sum_{j=1}^p |\theta_j|.$
		\item Elastic net regression: $J(\thetab) =  (\|\thetab\|_2^2,  \|\thetab\|_1)^\top$ and $\lambda=(\lambda_1,\lambda_2).$
		\item L0 regression: $J(\thetab) = \|\thetab\|_0 = \sum_{j=1}^p |\theta_j|^0.$
	%	
	\end{itemize}
	%
	\textbf{Early stopping:}
	%
	\begin{enumerate}
		\setlength{\itemindent}{+.3in}
		\item Split training data $\Dtrain$ into $\mathcal{D}_{\text{subtrain}}$ and $\mathcal{D}_{\text{val}}.$ 
		\item Train on $\mathcal{D}_{\text{subtrain}}$ and evaluate model using the validation set $\mathcal{D}_{\text{val}}$.
		\item Stop training when validation error stops decreasing.
		\item Use parameters of the previous step for the actual model.
	\end{enumerate}
%
\end{myblock} 
%
%
\begin{myblock}{Linear Support Vector Machines}
	%	
	Signed distance to the separating hyperplane:
	$$
	d \left(f, \xi \right) = \frac{\yi \fxi}{\|\thetab\|} = \yi \frac{\thetab^T \xi + \theta_0}{\|\thetab\|}
	$$ 
	Distance of $f$ to the whole dataset $\D:$ 
	$
	\gamma = \min\limits_i \Big\{ d \left(f, \xi \right) \Big\}
	$
	
	\textbf{Primal linear hard-margin SVM:}
	%
	  \begin{eqnarray*}
		& \min\limits_{\thetab, \theta_0} \quad & \frac{1}{2} \|\thetab\|^2 \\
		& \text{s.t.} & \,\,\yi  \left( \scp{\thetab}{\xi} + \theta_0 \right) \geq 1 \quad \forall\, i \in \nset
	\end{eqnarray*}
	%
	Support vectors: All instances $(\xi, \yi)$ with minimal margin
	$\yi  \fxi = 1$, fulfilling the inequality constraints with equality. 
	All have distance of $\gamma = 1 / \|\thetab\|$ from the separating hyperplane.
	
	\textbf{Dual linear hard-margin SVM:}
	%
	\begin{eqnarray*}
		& \max\limits_{\alphav \in \R^n} & \sum\nolimits_{i=1}^n \alpha_i - \frac{1}{2}\sum\nolimits_{i=1}^n\sum\nolimits_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}} \\
		& \text{s.t.} & \sum\nolimits_{i=1}^n \alpha_i \yi = 0, 
		 \quad \text{and} \quad \alpha_i \ge 0~\forall i \in \nset
	\end{eqnarray*}
	%
	Solution (if existing):
	%
	$$
	\thetah = \sum\nolimits_{i=1}^n \hat \alpha_i \yi \xi, \quad \theta_0 = \yi - \scp{\thetab}{\xi}.
	$$
	%
\end{myblock}


 
% End Second Column					
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Third Column#


%-------------------------------------------------------------------------------
% Regression Losses 
%------------------------------------------------------------------------------- 
\begin{myblock}{} \vspace{-4ex}
%	
	\textbf{Primal linear soft-margin SVM:} 	
	\begin{eqnarray*}
		& \min\limits_{\thetab, \thetab_0,\sli} & \frac{1}{2} \|\thetab\|^2 + C   \sum_{i=1}^n \sli \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \thetab_0 \right) \geq 1 - \sli \quad \forall\, i \in \nset,\\
		& \text{and} & \,\, \sli \geq 0 \quad \forall\, i \in \nset,\\
	\end{eqnarray*}
%
	where the constant $C > 0$ controls trade-off between the two conflicting
	objectives of maximizing the size of the margin and minimizing the
	frequency and size of margin violations\\
	
%
	\textbf{Dual linear soft-margin SVM:} 	
%	
	\begin{eqnarray*}
		& \max\limits_{\alphav \in \R^n} & \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}} \\
		& \text{s.t. } & 0 \le \alpha_i \le C, \forall\, i \in \nset \quad \text{and} \quad  \sum_{i=1}^n \alpha_i \yi = 0
	\end{eqnarray*}
%
Support Vectors: All instances $(\xi, \yi)$ which 
% 
\begin{itemize}
	\setlength{\itemindent}{+.3in}
	\item are located exactly on the
	margin and have $y\fx=1$.
	\item are margin violators, with $y\fx < 1$, and have an associated positive slack $\sli > 0$. 
	They are misclassified if $\sli \geq 1$.
\end{itemize}
%
Regularized empirical risk minimization representation:
%
$$ \risket = \frac{1}{2} \|\thetab\|^2 + C \sumin \Lxyi ;\; \Lyf = \max(1-yf, 0)$$
%

\end{myblock}

\begin{myblock}{Kernels}
%	
	\textbf{Mercer Kernel:} Continuous function
	$ k : \Xspace \times \Xspace \to \R $ fulfilling
%	
	\begin{itemize}
		\setlength{\itemindent}{+.3in}
	\item Symmetry: $k(\xv, \tilde \xv) = k(\tilde \xv, \xv)$ for all
	$\xv, \tilde \xv \in \Xspace$.
	\item Positive definiteness: For each finite subset $\left\{\xv^{(1)}, \dots, \xv^{(n)}\right\}$
	the \textbf{kernel Gram matrix} $\bm{K} \in \R^{n \times n}$ with entries
	$K_{ij} = k(\xi, \xv^{(j)})$ is positive semi-definite.
	\end{itemize}
%	
	Properties: For two Mercer kernels $k_1$ and~$k_2$:
%	
	\begin{itemize}
		\setlength{\itemindent}{+.3in}
		\item For $\lambda \geq 0$, $\lambda \cdot k_1$ is a kernel.
		\item $k_1 + k_2$ is a kernel.
		\item $k_1 \cdot k_2$ is a kernel (thus also $k_1^n$).
	\end{itemize}
	Examples:
	\begin{itemize}
		\setlength{\itemindent}{+.3in}
		\item Linear kernel: $k(\xv, \tilde \xv) = \xv^\top \tilde \xv$
		\item Homogeneous polynomial kernel:
		$ k(\xv, \xtil) = (\xv^T \xtil)^d, \text{ for } d \in \N$
		\item Nonhomogeneous polynomial k	ernel: $k(\xv, \tilde \xv) = (\xv^\top \tilde \xv + b)^d, \text{ for } b\geq 0, d \in \N$
		\item Radial Gaussian kernel (RBF):
		$k(\xv, \tilde \xv) = \exp(-\frac{\|\xv - \tilde \xv\|^2}{2\sigma^2})$ 
		or 
		$k(\xv, \tilde \xv) = \exp(-\gamma \|\xv - \tilde \xv\|^2), ~ \gamma>0$
	\end{itemize}
\end{myblock} 
%
%
%-------------------------------------------------------------------------------
% Classification 
%------------------------------------------------------------------------------- 

%\begin{myblock}{Classification}
% 				    We want to assign new observations to known categories according to criteria learned from a training set.  
%             \vspace*{1ex}
%             

%$y \in \Yspace = \gset : $ categorical output variable (label)\\ 

%\textbf{Classification} usually means to construct $g$ \textbf{discriminant functions}:
  
%$f_1(\xv), \ldots, \fgx$, so that we choose our class as \\ $h(\xv) = \argmax_{k \in \gset} \fkx$ \\

%\textbf{Linear Classifier:} functions $\fkx$ can be specified as linear functions\\

% \hspace*{1ex}\textbf{Note: }All linear classifiers can represent non-linear decision boundaries \hspace*{1ex}in our original input space if we include derived features. For example: \hspace*{1ex}higher order interactions, polynomials or other transformations of x in \hspace*{1ex}the model.

%\textbf{Binary classification: }If only 2 classes ($\Yspace = \setzo$ or  $\Yspace = \setmp$) exist, we can use a single discriminant function $\fx = f_{1}(\xv) - f_{2}(\xv)$.  \\


% \textbf{Generative approach }models $\pdfxyk$, usually by making some assumptions about the structure of these distributions and employs the Bayes theorem: 
% $\pikx = \postk \propto \pdfxyk \pik$. \\ %It allows the computation of \hspace*{1ex}$\pikx$. \\
% \textbf{Examples}: Linear discriminant analysis (LDA), Quadratic discriminant analysis (QDA), Naive Bayes\\
% 
% \textbf{Discriminant approach }tries to optimize the discriminant functions directly, usually via empirical risk minimization:\\ 
% $ \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.$\\
% \textbf{Examples}: Logistic/softmax regression, kNN


%\end{myblock}

%-------------------------------------------------------------------------------
% HRO - Components of Learning 
%-------------------------------------------------------------------------------          
%\begin{myblock}{Components of Learning}

%\textbf{Learning = Hypothesis space + Risk + Optimization} \\
%\phantom{\textbf{Learning}} \textbf{= }$ \Hspace + \risket + \argmin_{\thetab \in \Theta} 
%\risket$

% 
% \textbf{Learning &= Hypothesis space &+ Risk  &+ Optimization} \\
% &= $\Hspace &+ \risket &+ \argmin_{\thetab \in \Theta} \risket$
% 
% \textbf{Hypothesis space: } Defines (and restricts!) what kind of model $f$
% can be learned from the data.
% 
% Examples: linear functions, decision trees
% 
% \vspace*{0.5ex}
% 
% \textbf{Risk: } Quantifies how well a model performs on a given
% data set. This allows us to rank candidate models in order to choose the best one.
% 
% Examples: squared error, negative (log-)likelihood
% 
% \vspace*{0.5ex}
% 
% \textbf{Optimization: } Defines how to search for the best model, i.e., the model with the smallest {risk}, in the hypothesis space.
% 
% Examples: gradient descent, quadratic programming


%\end{myblock}
% End Third Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			  }
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
\end{columns}

\end{frame}
\end{document}
