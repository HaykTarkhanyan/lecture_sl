\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Advanced Regression Losses
}{
figure/plot_loss_overview.png
}{
\item Huber loss
\item Log-Cosh loss
\item Cauchy loss
\item $\eps$-Insensitive loss
\item Quantile loss
}

\begin{vbframe}{Advanced Loss Functions \citelink{WANG2020COMPRE}}

\begin{itemize}
\item Handle errors in custom fashion
\item Model other error distributions\\
(see section on max. likelihood)
\item Induce properties like robustness
\item Handle other predictive tasks
\end{itemize}

\vfill

\begin{center}
\includegraphics[width = 0.55\textwidth]{figure/plot_loss_overview.png}
\end{center}





\vspace{1cm}

% Examples:
% \begin{itemize}
% \item Quantile loss: Overestimating a clinical parameter might not be as bad as underestimating it.
% \item Log-barrier loss: Extremely under- or overestimating demand in production would put company profit at risk.
% \item $\eps$-insensitive loss: A certain amount of deviation in production does no harm, larger deviations do.
% \end{itemize}
% % \item Sometimes a custom loss must be designed specifically for the given application.
% % \item Some learning algorithms use specific loss functions, e.g., the hinge loss for SVMs.
% % \end{itemize}
\end{vbframe}

\begin{vbframe}{Huber Loss}

% Description
\vspace*{-0.5cm}

$$
\Lxy = \begin{cases}
  \frac{1}{2}(y - \fx)^2  & \text{ if } |y - \fx| \le \epsilon \\
  \epsilon |y - \fx|-\frac{1}{2}\epsilon^2 \quad & \text{ otherwise }
  \end{cases} \quad \epsilon > 0
$$

\begin{itemize}
\item Piece-wise combination of $L1$/$L2$ to have robustness/smoothness
\item Analytic properties: convex, differentiable (once)
\end{itemize}

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure/loss_huber_plot.png}
\end{center}

\begin{itemize}
\item No closed-form solution even for constant or linear model
\item Solution behaves like \textbf{trimmed mean}:\\
a (conditional) mean of two (conditional) quantiles  
\end{itemize}

\end{vbframe}

\begin{vbframe}{Log-cosh Loss \citelink{SALEH2022STATISTICAL}}

% Confirmed with Bernd: use def from https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0

$$
\Lxy = \log \left( \cosh(\left|y - \fx\right|) \right) \qquad \text{cosh}(x)=\frac{e^x+e^{-x}}{2}
$$

\vfill

\begin{itemize}
%\item Logarithm of the hyperbolic cosine of the residual.
\item Approx. $0.5 (\left|y - \fx\right|)^2$ for small residuals;\\
$\left|y - \fx\right| - \log 2$ for large residuals
\item Smoothed combo of $L1$ / $L2$ loss
\item Similar to Huber, but twice differentiable
\end{itemize}

\vfill

\begin{center}
\includegraphics[width = 0.5\textwidth]{figure/loss_logcosh.png}
\end{center}

\framebreak

%What is the idea behind the log-cosh loss?

\begin{columns}

\begin{column}{0.4\textwidth}
%\textbf{Left Column}

\vspace{1cm}

Essential idea:
\begin{enumerate}
    \item Derivative of $L1$ \\
    w.r.t. residual 
    %$\text{sign}(y-\fx)$ function
    \item Approx. $\sign$ with $\tanh$
    %$\text{sign}(y-\fx)$ using the cont. differentiable $\text{tanh}(y-\fx)$
    \item Integrate ``up again'' 
\end{enumerate}

\end{column}

\begin{column}{0.6\textwidth}


\begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure/logcosh-derivation.png}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetav$}
    \end{figure}

\end{column}

\end{columns}

\vfill

Same trick can be used to get differentiable pinball losses

\framebreak

\textbf{cosh($\theta,\sigma$) distribution}:\\

\begin{itemize}

\item Normalized reciprocal $\text{cosh}(x)$ is pdf: 
positive and $\int_{-\infty}^{\infty} \frac{1}{\pi \text{cosh}(x)} \text{d}x = 1$

\item Location-scale type ($\theta, \sigma$) 
resembling Gaussian with heavy tails

\item ERM using log-cosh 
is equivalent to MLE of $\text{cosh}(\theta,1)$ distribution

\end{itemize}

\begin{columns}

\begin{column}{0.5\textwidth}
%\textbf{Left Column}

{\normalsize 
\begin{itemize}\setlength{\itemsep}{0.32em}
    \item $p(x | \theta, \sigma)=\frac{1}{\pi \sigma \cosh \left(\frac{x-\theta}{\sigma}\right)}$
    \item $\E_{x \sim p}[x] = \theta$
    \item $\var_{x \sim p}[x]=\frac{1}{4} \pi^2 \sigma^2 $
    \item {\footnotesize $\thetah^{MLE}=\argmax_{\theta} \prodin \frac{1}{\pi \cosh \left(\yi-\theta\right)} =$}\\{\footnotesize $\thetah = \argmin_{\theta}\sumin \log(\text{cosh}(\yi-\theta))$}
\end{itemize}
}
\end{column}

\begin{column}{0.7\textwidth}

\begin{figure}
      \centering
        \scalebox{1}{\includegraphics{figure/cosh-gaussian-densities.png}}
    \end{figure}

\end{column}

\end{columns}


\end{vbframe}

\begin{vbframe}{Cauchy loss}

% Confirmed with Bernd: use def from https://www.user.tu-berlin.de/mtoussai/teaching/15-MachineLearning/15-MachineLearning-script.pdf, p. 24 

$$
\Lxy = \frac{c^2}{2} \log \left( 1 + \left( \frac{\left|y - \fx\right|}{c}\right)^2 \right), 
\quad c \in \R
$$

\begin{itemize}
\item Particularly robust toward outliers (controllable via $c$)
\item Analytic properties: differentiable, but not convex
\end{itemize}

\vfill

\begin{center}
\includegraphics[width = 0.6\textwidth]{figure/loss_cauchy.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Telephone data}

\begin{itemize}

\item Illustrate the effect of robust losses on telephone data set 
\item Nr. of calls (in 10mio units) in Belgium 1950-1973
\item Outliers due to a change in measurement without re-calibration 

\end{itemize}

\vfill

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/telephone-data.pdf}
\end{center}

\end{vbframe}


\begin{vbframe}{$\eps$-Insensitive loss}

\vspace*{-0.3cm}
$$
\Lxy =  \begin{cases}
  0  & \text{if } |y - \fx| \le \epsilon \\
  |y - \fx|-\epsilon & \text{otherwise }
  \end{cases}, \quad \epsilon \in \R_{+}
$$
\begin{itemize}
\item Modification of $L1$, errors below $\epsilon$ get no penalty
\item Used in SVM regression
\item Properties: convex, not differentiable for $ y - \fx \in \{-\epsilon, \epsilon\}$
\end{itemize}

\vfill

\begin{center}
% \includegraphics[width = 10cm, height = 4.7cm]{figure_man/2_6_loss_epsilon_plot1.png} \\
\includegraphics[width = 0.8\textwidth]{figure/loss_eps_insensitive.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Quantile Loss / Pinball Loss}

$$
\Lxy = \begin{cases} (1 - \alpha) (\fx - y) & \text{ if } y < \fx\\
\alpha (y - \fx) & \text{ if } y \ge \fx
\end{cases}, \quad \alpha \in (0, 1)
$$

\begin{itemize}
\item Extension of $L1$ loss (equal to $L1$ for $\alpha = 0.5$).
\item Penalizes either over- or under-estimation more
%\item $\alpha<0.5$ $(\alpha>0.5)$ penalty to over-estimation (under-estimation)
\item Risk minimizer is (conditional) 
    $\alpha$-quantile (median for $\alpha=0.5$)
\end{itemize}

\vfill

\begin{center}
% \includegraphics[width = 10cm, height = 4.7cm]{figure_man/2_3_loss_pinball_plot2.png}
\includegraphics[width = 0.8\textwidth]{figure/loss_quantile.png}
\end{center}

% \framebreak


\framebreak

\begin{itemize}

\item Simulate $n=200$ samples from heteroskedastic LM 
\item $y = 1+0.2x+\varepsilon$; 
$\quad \varepsilon \sim \mathcal{N}(0, 0.5+0.5x)$; $\quad x \sim \mathcal{U}[0,10]$
\item Fit LM with pinball losses to estimate $\alpha$-quantiles 

\end{itemize}

\vspace{-0.2cm}

\begin{center}
\includegraphics[width = 0.62\textwidth]{figure/quantile-regression.pdf}
\end{center}

\end{vbframe}




\endlecture

\end{document}
