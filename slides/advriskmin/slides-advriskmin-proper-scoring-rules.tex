\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
  	Proper Scoring Rules
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/bernoulli_prob.png
  }{
  \item Honest probabilistic forecasts
  \item Proper scoring rules
  \item log score    
  \item Brier score
}

\begin{vbframe}{Probabilistic Forecasts \citelink{GNEITING2007}}

Scoring rules $S(P,y)$ assess the quality of probabilistic forecasts by assigning a score based on the predictive distribution $P$ and the realized event $y$. The expected score w.r.t. the RV $y \sim Q$ is
$$ S(P,Q) = \E_{y \sim Q}[S(P,y)]$$



A scoring rule is \textbf{proper} if forecaster maximizes expected score for an observation drawn from $Q$ if the forecast is $Q$ rather than $P \neq Q$:

$$S(Q,Q) \geq S(P,Q) \,\, \text{for all} \, P,Q $$

\vspace{0.2cm}

$S$ is \textbf{strictly proper} when equality holds iff $P=Q$. (Strictly) proper scores ensure forecaster has an incentive to predict $Q$ and is encouraged to report true beliefs.

\vspace{0.2cm}

{\footnotesize \textbf{NB}: scores are typically positively oriented (maximization) while losses are negatively oriented (minimization). Scores could also be defined negatively oriented.}

\end{vbframe}

\begin{vbframe}{Binary classification scores}

For simplicity, we will only look at binary targets $y \sim \text{Bern}(p)$.\\ %Scoring rules are often related to loss functions $L$ by taking their negative value (maximization vs. minimization).
We want to find out if using a loss $L(y, \pi)$ (negative score) incentivizes honest forecasts $\pi=p$ for any $p \in [0,1]$.

\lz

For any loss $L$, its expectation w.r.t. $y$ is
$$\E_{y}[L(y, \pi)]=p \cdot L(1, \pi) + (1-p) \cdot L(0, \pi)$$

\vspace{0.2cm}

Let's first look at a negative example. Assuming the \textbf{L1 loss} $L(y,\pi)=|y-\pi|$, we obtain

$$\E_y[L(y, \pi)]=p |1-\pi| + (1-p) \pi = p+\pi(1-2p)$$

\vspace{0.2cm}

The expected loss is linear in $\pi$, hence we minimize it by setting $\pi = 1$ for $p>0.5$ and $\pi = 0$ for $p<0.5$. 

\vspace{0.2cm}

The score $S(\pi,y)=-L(y,\pi)$ is therefore not proper.

\end{vbframe}

\begin{vbframe}{Binary classification scores}

The \textbf{0/1 loss} $L(y,\pi)=\mathds{1}_{\{y \neq h_\pi\}}$ using the discrete classifier $h_{\pi}=\mathds{1}_{\{\pi>0.5\}}$ yields in expectation over $y$:

\begin{eqnarray*}
\E_y[L(y, \pi)] &=& p \cdot L(1,\pi) + (1-p) \cdot L(0,\pi)\\
&=& \left\{
\begin{array}{ll}
p & \text{if } h_{\pi} = 0 \\
1-p & \text{if } h_{\pi} = 1
\end{array}
\right.
\end{eqnarray*}

\begin{itemize}
    \item For $p>0.5$ we minimize the expected loss by choosing $h_{\pi}=1$, which holds true for any $\pi \in (0.5,1)$
    \item Likewise for $p\leq 0.5$, any $\pi \in (0, 0.5]$ minimizes the expected loss
\end{itemize}

\lz

The \textbf{0/1 score} (negative 0/1 loss) is therefore proper but not strictly proper since there is no unique maximum.

%$$\E_y[L(y, \pi)]=p L(1,\pi) + (1-p) L(0,\pi) = $$
\end{vbframe}

\begin{vbframe}{Binary classification scores}

To find strictly proper scores/losses, we can ask: Which functions have the property such that $\E_y[L(y,\pi)]$ is minimized at $\pi=p$? We have

$$\E_{y}[L(y,\pi)]=p \cdot L(1,\pi) + (1-p) \cdot L(0,\pi)$$

Let's further %restrict our search to scores 
assume that $L(1,\pi)$ and $L(0, \pi)$ can not be arbitrary, but are the same function evaluated at $\pi$ and $1-\pi$:
%$S(\pi,y)$ for which 
$L(1,\pi)=L(\pi)$ and $L(0,\pi)=L(1-\pi)$. Then

$$\E_{y}[L(y,\pi)]=p \cdot L(\pi) + (1-p) \cdot L(1-\pi)$$

\vspace{0.2cm}

Setting the derivative w.r.t. $\pi$ to $0$ and requiring $\pi=p$ at the optimum (\textbf{propriety}), we get the following first-order condition (F.O.C.):

\vspace{0.3cm}

$$p \cdot L'(p) \overset{!}{=} (1-p) \cdot L'(1-p)$$

\framebreak

\begin{itemize}\setlength\itemsep{1.9em}
    \item F.O.C.:\quad $p \cdot L'(p) \overset{!}{=} (1-p) \cdot L'(1-p)$
    \item One natural solution is $L'(p)=-1/p$, resulting in $-p/p=-(1-p)/(1-p)=-1$ and the antiderivative $L(p)=-\log(p)$. 
    \item This is the \textbf{log loss} $$L(y,\pi)=-(y \cdot \log(\pi) + (1-y) \cdot \log(1-\pi))$$
    \item The corresponding scoring rule (maximization) is the strictly proper \textbf{logarithmic scoring rule} $$S(\pi,y)=y \cdot \log(\pi) + (1-y) \cdot \log(1-\pi)$$
\end{itemize}

\framebreak

\begin{itemize} \setlength\itemsep{1.2em}
    \item F.O.C.:\quad $p \cdot L'(p) \overset{!}{=} (1-p) \cdot L'(1-p)$
    \item A second solution is $L'(p)=-2(1-p)$, resulting in $-2p(1-p)=-2(1-p)p$ and the antiderivative $L(p)=(1-p)^2=\frac{1}{2}((1-p)^2+(0-(1-p))^2)$
    \item This is also called the \textbf{Brier score} and is effectively the \textbf{MSE loss} for probabilities $$L(y,\pi)=\frac{1}{2}\sum_{i=1}^{2}(y_i-\pi_i)^2$$
     {\small (with $y_1=y, y_2=1-y$ and likewise $\pi_1=\pi, \pi_2=1-\pi$)}
    \item Using positive orientation (maximization), this gives rise to the \textbf{quadratic scoring rule}, which for two classes is $S(\pi,y)=-\frac{1}{2} \sum_{i=1}^{2}(y_i-\pi_i)^2$
\end{itemize}

\end{vbframe}



\endlecture

\end{document} 
