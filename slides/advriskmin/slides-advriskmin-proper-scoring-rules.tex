\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Proper Scoring Rules
}{
figure/bernoulli_prob.png
}{
\item Honest probabilistic forecasts
\item Log score    
\item Brier score
}

\begin{vbframe}{Probabilistic Predictions \citelink{GNEITING2007}}

\begin{itemize}
    \item 
\end{itemize}

Scoring rules $S(P,y)$ assess the quality of probabilistic predictions by assigning a score based on the predictive distribution $P$ and the realized event $y$. The expected score w.r.t. the RV $y \sim Q$ is
$$ S(P,Q) = \E_{y \sim Q}[S(P,y)]$$



A scoring rule is \textbf{proper} if predictor maximizes expected score for an observation drawn from $Q$ if the forecast is $Q$ rather than $P \neq Q$:

$$S(Q,Q) \geq S(P,Q) \,\, \text{for all} \, P,Q $$

\vspace{0.2cm}

$S$ is \textbf{strictly proper} when equality holds iff $P=Q$. (Strictly) proper scores ensure forecaster has an incentive to predict $Q$ and is encouraged to report true beliefs.

\vspace{0.2cm}

{\footnotesize \textbf{NB}: scores are typically positively oriented (maximization) while losses are negatively oriented (minimization). Scores could also be defined negatively oriented.}

\end{vbframe}

\begin{vbframe}{Binary classification scores}

For simplicity, we will only look at binary targets $y \sim \text{Bern}(p)$.\\ %Scoring rules are often related to loss functions $L$ by taking their negative value (maximization vs. minimization).
We want to find out if using a loss $\Lpixy$ (negative score) incentivizes honest forecasts $\pix=p$ for any $p \in [0,1]$.

\lz

For any loss $L$, its expectation w.r.t. $y$ is
$$\E_{y}[\Lpixy]=p \cdot L(1, \pix) + (1-p) \cdot L(0, \pix)$$

\vspace{0.2cm}

Let's first look at a negative example. Assuming the \textbf{L1 loss} $\Lpixy=|y-\pix|$, we obtain

$$\E_y[\Lpixy]=p |1-\pix| + (1-p) \pix = p+\pix(1-2p)$$

\vspace{0.2cm}

The expected loss is linear in $\pix$, hence we minimize it by setting $\pix = 1$ for $p>0.5$ and $\pix = 0$ for $p<0.5$. 

\vspace{0.2cm}

The score $S(\pix,y)=-\Lpixy$ is therefore not proper.

\end{vbframe}

\begin{vbframe}{Binary classification scores}

The \textbf{0/1 loss} $\Lpixy=\mathds{1}_{\{y \neq h_\pix\}}$ using the discrete classifier $h_{\pix}=\mathds{1}_{\{\pix>0.5\}}$ yields in expectation over $y$:

\begin{eqnarray*}
\E_y[\Lpixy] &=& p \cdot L(1,\pix) + (1-p) \cdot L(0,\pix)\\
&=& \left\{
\begin{array}{ll}
p & \text{if } h_{\pix} = 0 \\
1-p & \text{if } h_{\pix} = 1
\end{array}
\right.
\end{eqnarray*}

\begin{itemize}
    \item For $p>0.5$ we minimize the expected loss by choosing $h_{\pix}=1$, which holds true for any $\pix \in (0.5,1)$
    \item Likewise for $p\leq 0.5$, any $\pix \in (0, 0.5]$ minimizes the expected loss
\end{itemize}

\lz

The \textbf{0/1 score} (negative 0/1 loss) is therefore proper but not strictly proper since there is no unique maximum.

%$$\E_y[L(y, \pi)]=p L(1,\pi) + (1-p) L(0,\pi) = $$
\end{vbframe}

\begin{vbframe}{Binary classification scores}

To find strictly proper scores/losses, we can ask: Which functions have the property such that $\E_y[\Lpixy]$ is minimized at $\pix=p$? We have

$$\E_{y}[\Lpixy]=p \cdot L(1,\pix) + (1-p) \cdot L(0,\pix)$$

Let's further %restrict our search to scores 
assume that $L(1,\pix)$ and $L(0, \pix)$ can not be arbitrary, but are the same function evaluated at $\pix$ and $1-\pix$:
%$S(\pi,y)$ for which 
$L(1,\pix)=L(\pix)$ and $L(0,\pix)=L(1-\pix)$. Then

$$\E_{y}[\Lpixy]=p \cdot L(\pix) + (1-p) \cdot L(1-\pix)$$

\vspace{0.2cm}

Setting the derivative w.r.t. $\pix$ to $0$ and requiring $\pix=p$ at the optimum (\textbf{propriety}), we get the following first-order condition (F.O.C.):

\vspace{0.3cm}

$$p \cdot L'(p) \overset{!}{=} (1-p) \cdot L'(1-p)$$

\framebreak

\begin{itemize}\setlength\itemsep{1.9em}
    \item F.O.C.:\quad $p \cdot L'(p) \overset{!}{=} (1-p) \cdot L'(1-p)$
    \item One natural solution is $L'(p)=-1/p$, resulting in $-p/p=-(1-p)/(1-p)=-1$ and the antiderivative $L(p)=-\log(p)$. 
    \item This is the \textbf{log loss} $$\Lpixy=-(y \cdot \log(\pix) + (1-y) \cdot \log(1-\pix))$$
    \item The corresponding scoring rule (maximization) is the strictly proper \textbf{logarithmic scoring rule} $$S(\pix,y)=y \cdot \log(\pix) + (1-y) \cdot \log(1-\pix)$$
\end{itemize}

\framebreak

\begin{itemize} \setlength\itemsep{1.2em}
    \item F.O.C.:\quad $p \cdot L'(p) \overset{!}{=} (1-p) \cdot L'(1-p)$
    \item A second solution is $L'(p)=-2(1-p)$, resulting in $-2p(1-p)=-2(1-p)p$ and the antiderivative $L(p)=(1-p)^2=\frac{1}{2}((1-p)^2+(0-(1-p))^2)$
    \item This is also called the \textbf{Brier score} and is effectively the \textbf{L2 loss} for probabilities $$\Lpixy=\frac{1}{2}\sum_{i=1}^{2}(y_i-\pi_i(\xv))^2$$
     {\small (with $y_1=y, y_2=1-y$ and likewise $\pi_1(\xv)=\pix, \pi_2(\xv)=1-\pix$)}
    \item Using positive orientation (maximization), this gives rise to the \textbf{quadratic scoring rule}, which for two classes is $S(\pix,y)=-\frac{1}{2} \sum_{i=1}^{2}(y_i-\pi_i(\xv))^2$
\end{itemize}

\end{vbframe}



\endlecture

\end{document} 
