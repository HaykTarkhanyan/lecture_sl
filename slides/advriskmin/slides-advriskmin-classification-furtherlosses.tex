\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}
\input{../../latex-math/ml-trees} % For the comparison of Brier and Gini index

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{% Lecture title  
    Advanced Classification Losses
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/overview_classif.png
  }{
  \item (squared) Hinge loss
  \item $L2$ loss defined on scores
  \item Exponential loss
  \item AUC loss 
}

\begin{vbframe}{Hinge Loss}

\begin{itemize}
  \item 0-1-loss is intuitive but ill-suited to direct optimization
  \item \textbf{Hinge loss} is continuous and convex 
  upper bound on the 0-1-loss 
  $$\Lxy = \max \{ 0, 1 - y\fx \} \quad \text{for} \ y \in \setmp$$
  \item Only zero for margin $y\fx\geq 1$, 
  encourages confident predictions
  \item Often used in SVMs %(cf. chapters on SVMs)
  % \item A squared version exists for putting a sharper penalty on 
  % misclassifications:
  % $$\Lxy = \max \{ 0, (1 - y\fx)^2\}.$$
  \item Resembles a door hinge, hence the name:
\end{itemize}

\begin{center}
\includegraphics[width = 0.7\textwidth]{figure/hinge.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Squared Hinge Loss}

\begin{itemize}
  \item Can also define the \textbf{squared hinge loss}:
  $$\Lxy = \max \{ 0, (1 - y\fx)\}^2$$
  \item The $L2$ form punishes margins $y\fx \in (0, 1)$ less severely but puts 
  a high penalty on more confidently wrong predictions
  \item Cont. differentiable yet more outlier-sensitive than hinge loss
\end{itemize}

\begin{center}
\includegraphics[width = 0.7\textwidth]{figure/hinge_squared.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Squared Loss on Scores}

% Source: https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04a.loss-functions.pdf

\begin{itemize}
  \item Analogous to the Brier score defined on probabilities we can specify a 
  \textbf{squared loss on classification scores} (with labels $y \in \setmp$, using 
  that $y^2 = 1$):
  \begin{eqnarray*}
  \Lxy = &(y - \fx)^2 = y^2 - 2y\fx + \fx^2 =\\
  = &1 - 2y\fx + (y\fx)^2 = (1 - y\fx)^2
  \end{eqnarray*}
  \item Behaves like squared hinge loss for $y\fx < 1$, but is not clipped to $0$ for $y\fx>1$ (zero only for $y\fx = 1$ and increases again in $y\fx$ which is in general not desirable!)
\end{itemize}

\begin{center}
\includegraphics[width = 0.7\textwidth]{figure/squared_scores.png}
\end{center}

\end{vbframe}



\begin{vbframe}{Exponential Loss}


\begin{itemize}
\item Another smooth 
approx. of 0-1-loss is the \textbf{exponential loss}:
$$\Lxy = \exp(-y\fx)$$ 
\item Used in AdaBoost
\item Convex, differentiable (thus easier to optimize than 0-1-loss)
\item Loss increases exponentially for wrong predictions with high confidence; low-confidence correct predictions have positive loss
%\item No closed-form analytic solution to (empirical) risk minimization.
\end{itemize}


\begin{figure}
\includegraphics[width = 0.8\textwidth]{figure/exponential.png}
\end{figure}

\end{vbframe}

\begin{vbframe}{AUC-loss}

\begin{itemize}
\item Often AUC is used as an evaluation criterion for binary classifiers
\item Let $y \in \setmp$ with $\nn$ negative and $\np$ positive samples %$y_i, i = 1, \ldots, n_{-1} + n_1$.
\item The AUC can then be defined as
$$AUC = \frac{1}{\np} \frac{1}{\nn} \sum_{i: \yi = 1} \sum_{j: \yi[j] = -1} [f^{(i)} > f^{(j)}]$$
\item This is not differentiable w.r.t $f$ due to indicator $[f^{(i)} > f^{(j)}]$
\item The indicator function can be approximated by the distribution function of the triangular distribution on $[-1, 1]$ with mean $0$
\item Direct optimization of AUC is numerically difficult, rather use common loss and tune for AUC in practice

\end{itemize}

Comprehensive survey on advanced loss functions: \citelink{WANG2020COMPRE}

\end{vbframe}





\endlecture

\end{document}
