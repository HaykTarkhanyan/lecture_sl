\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-trees} 

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{L2/L1 Loss on Probabilities
  }{figure/brier.png
  }{
  \item Brier score / $L2$ loss on probabilities
  \item Derivation of risk minimizer
  \item Optimal constant model 
  \item $L1$ loss on probabilities
  \item Calibration
}


\begin{vbframe}{Brier Score}

\begin{footnotesize}

\begin{itemizeM}
\item Binary Brier score defined on probabilities $\pix \in [0, 1]$ and labels $y \in \{0, 1\}$ is $L2$ loss on probabilities $$\Lpixy = (\pix - y)^2$$
\item Despite convex in $\pix$ $$\Lxy=((1+\exp{(-\fx)})^{-1}-y)^2$$ as composite function not convex in $\fx$
\item Exception would be so-called linear prob. model with $\pix = \thetav^T \xv$,\\
but that is quite uncommon in ML 
\end{itemizeM}

% \vspace{-0.2cm}
% \begin{center}
% \includegraphics[width = 0.45\textwidth]{figure/brier.png}
% \includegraphics[width = 0.45\textwidth]{figure/brier_on_score.png}
% \end{center}

\lz 

\splitVCC{
\includegraphics[width = \textwidth]{figure/brier.png}
}{
\includegraphics[width = \textwidth]{figure/brier_on_score.png}
}

\end{footnotesize}



\end{vbframe}

\begin{vbframe}{Brier Score: Risk Minimizer}

\begin{itemizeL}
    \item Risk minimizer for (binary) Brier score is $$\piastxtil = \eta(\xtil) = \P(y=1~|~\xv = \xtil)$$
    \item Attains minimum if prediction equals \enquote{true} prob $\eta(\xv)$ of outcome
    \item Risk minimizer for multiclass Brier score is 
$$\pi^{\ast}_k(\xtil) = \eta_k(\xtil) =  \P(y = k ~|~ \xv = \xtil) $$
\end{itemizeL}

\framebreak
\textbf{Proof: } We only prove the binary case. We need to minimize 

$$
\E_x \left[L(1, \pix) \cdot \eta(\xv) + L(0, \pix) \cdot (1 - \eta(\xv)) \right]
$$

which we do pointwise for every $\xv$. We plug in the Brier score

\vspace*{-0.3cm}

\begin{eqnarray*}
	&& \argmin_c \quad L(1, c) \eta(\xv) + L(0, c) (1 - \eta(\xv)) \\ 
	&=&  \argmin_c \quad (c - 1)^2 \eta(\xv) + c^2 (1 - \eta(\xv))  \quad |{\,+\eta(\xv)^2-\eta(\xv)^2}\\
  &=&  \argmin_c \quad (c^2 -2c\eta(\xv) + \eta(\xv)^2)- \eta(\xv)^2 + \eta(\xv) \\
	&=&  \argmin_c \quad (c - \eta(\xv))^2
\end{eqnarray*}

The expression is minimized for $c = \eta(\xv)$

\end{vbframe}

\begin{vbframe}{Brier Score: Optimal constant Model}

\begin{itemizeL}
    \item Optimal constant probability model for labels $\Yspace = \setzo$ is 
    $$\thetah=\argmin_{\theta} \riske(\theta) = \argmin_{\theta} \sumin \left(\yi - \theta\right)^2 = \frac{1}{n} \sumin \yi$$
    \item Fraction of class-1 observations in the observed data\\ (directly follows from $L2$ proof for regression)
    \item Similarly, optimal constant for the multiclass Brier score is $$\thetah_k = \frac{1}{n}\sumin \I [\yi = k]$$
\end{itemizeL}

\end{vbframe}

\begin{vbframe}{Calibration and Brier Score}

\begin{itemizeL}
    \item As Brier score is proper scoring rule, it can be used for calibration
    \item Prediction $\pi(\mathbf{x})\in[0,1]$ called \textbf{calibrated} if 
$$\P\bigl(y=1 \mid \pi(\mathbf{x})=p\bigr)=p \quad \forall\, p\in[0,1]$$
    \item Means: if we predict $p$, then in $p \cdot 100\%$ of cases we observe $y=1$ (neither over- nor underconfident)
    \item Recall RM for Brier score $\pi^*(\mathbf{x}) = \eta(\mathbf{x}) = \P(y=1 \mid \mathbf{x})$. As \(\pi^*(\mathbf{x})=\eta(\mathbf{x})\), optimal predictor satisfies $$\P\bigl(y=1 \mid \pi^*(\mathbf{x})=p\bigr)=p$$
i.e., is perfectly calibrated
\end{itemizeL}

\end{vbframe}

\begin{vbframe}{L1 loss on probabilities}
\begin{itemizeM}
    \item Binary L1 loss on probabilities $\pix \in [0, 1]$ and labels $y \in \setzo$: $$\Lpixy = |\pix - y|$$
    \item As L1 loss not a proper scoring rule (see part on this), should not necessarily expect good calibration
    \item Despite convex in $\pix$ $$\Lxy=|(1+\exp{(-\fx)})^{-1}-y|$$ as composite function not convex in $\fx$ 
\end{itemizeM}

\lz

% \begin{center}
% \includegraphics[width = 0.45\textwidth]{figure/l1_loss.png}
% \includegraphics[width = 0.45\textwidth]{figure/l1_loss_on_score.png}
% \end{center}
\splitVCC{
\includegraphics[width = \textwidth]{figure/l1_loss.png}
}{
\includegraphics[width = \textwidth]{figure/l1_loss_on_score.png}
}

\end{vbframe}





\endlecture

\end{document}
