\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\newcommand{\titlefigure}{figure/bias_variance_decomposition-linear_model_bias.png}
\newcommand{\learninggoals}{
  \item Understand how to decompose the generalization error of an inducer into 
  \begin{itemize}
    \item \footnotesize Bias of the inducer
    \item \footnotesize Variance of the inducer
    \item \footnotesize Noise in the data
  \end{itemize} 
  }

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Bias-Variance Decomposition}
\lecture{Introduction to Machine Learning}




\begin{vbframe} {Bias-Variance decomposition}

Let us take a closer look at the generalization error of a learning algorithm $\ind_{L, O}$.
This is the expected error an induced model, on trainings sets of size $n$, when this is applied to a fresh, random test observation.
  $$\GEind = \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right)  $$
We therefore need to take the expectation over all training sets of size $n$, as well as the independent test observation.

\lz 

We assume that the data is generated by 
$$
y = \ftrue(\xv) + \epsilon\,,
$$
with normally distributed error $\epsilon \sim \mathcal{N}(0, \sigma^2)$ independent of $\xv$.  

\framebreak 

By plugging in the $L2$ loss $L(y, f(\xv)) = (y - f(\xv))^2$ we get

\begin{footnotesize}
\begin{eqnarray*}
\GEind &=& \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right)\right) = \E_{\D_n, xy}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right) \\
&=& \E_{xy}\underbrace{\left[\E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2~|~ \xv, y\right)\right]}_{(*)} 
\end{eqnarray*}
\end{footnotesize}

Let us consider the error $(*)$ conditioned on one fixed test observation $(\xv, y)$ first. (We omit the $~|~\xv, y$ for better readability for now.)

\begin{footnotesize}
\begin{eqnarray*}
(*) &=& \E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right)\\
&=& \underbrace{\E_{\D_n}\left(y^2\right)}_{= y^2} + \underbrace{\E_{\D_n}\left(\fh_{\D_n}(\xv)^2\right)}_{(1)}  - 2\underbrace{\E_{\D_n}\left(y\fh_{\D_n}(\xv)\right)}_{(2)} 
% &=& y^2 + \underbrace{\E_{xy}^2\left(y\right)}_{(1)} + \underbrace{\E_{\D_n, xy}\left(\fh_{\D_n}(\xv)^2\right)}_{(2)}  - 2\underbrace{\E_{\D_n, xy}\left(y\fh_{\D_n}(\xv)\right)}_{(3)} \\
\end{eqnarray*}
\end{footnotesize}

by using the linearity of the expectation.  %and $\var(y) = \E(y^2) - \E^2(y)$. 

\framebreak
\begin{footnotesize}
$$
\E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right) = 
y^2 + \underbrace{\E_{\D_n}\left(\fh_{\D_n}(\xv)^2\right)}_{(1)}  - 2\underbrace{\E_{\D_n}\left(y\fh_{\D_n}(\xv)\right)}_{(2)} =
$$
\end{footnotesize}


\begin{footnotesize}
By using that $\E(z^2) =\var(z) + \E^2(z)$, we see that
% \begin{eqnarray*}
$$
= y^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + \E^2_{\D_n}\left(\fh_{\D_n}(\xv)\right) - 2y \E_{\D_n}\left(\fh_{\D_n}(\xv))\right) 
$$
Plug in the definition of $y$
$$
 = \ftrue(\xv)^2 +2\epsilon\ftrue(\xv) + \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + \E^2_{\D_n}\left(\fh_{\D_n}(\xv)\right) - 2 (\ftrue(\xv)+\epsilon)\E_{\D_n}\left(\fh_{\D_n}(\xv))\right) 
$$
Reorder terms and use the binomial formula
$$
= \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + 
  \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2
 +2 \epsilon \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)
$$
% \end{eqnarray*}
\end{footnotesize}


\framebreak 

\begin{footnotesize}
$$
(*) = \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + 
  \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2
 +2 \epsilon \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)
$$
\end{footnotesize}

Let us come back to the generalization error by taking the expectation over all fresh test observations $(\xv, y) \sim \Pxy$: 

\begin{footnotesize}
\begin{eqnarray*}
\GEind 
% &=& \var_{xy}(y) + \E_{xy}^2\left(\ftrue(\xv)\right) + \E_{xy}\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]  \\ &+& \E_{xy}\left[\E^2_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]  - 2\E_{xy}\left[\E_{\D_n}\left(\ftrue(\xv)~|~\xv, y\right) \E_{\D_n}\left(\fh_{\D_n}(\xv)~|~\xv, y\right)\right] \\
% &=& \underbrace{\E_{xy}(y^2 - \ftrue(\xv)^2~|~\xv, y)}_{= \sigma^2} + \E_{xy}\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right] \\ &+& \E_{xy}\left[\E^2_{\D_n}\left(\ftrue(\xv)-\fh_{\D_n}(\xv)~|~\xv, y\right)\right] \\
  &=& \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of inducer at } (\xv, y)} \\ &+& \E_{xy}\underbrace{\left[\left(\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of inducer at } (\xv, y)} + \underbrace{0}_{\text{As $\epsilon$ is zero-mean and independent}}
\end{eqnarray*}
\end{footnotesize}


\framebreak 

\begin{footnotesize}
$\GEind =$  
$$
 \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of inducer at } (\xv, y)} + \E_{xy}\underbrace{\left[\left(\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of inducer at } (\xv, y)}  
$$
\end{footnotesize}
\begin{enumerate}
  \item The first term expresses the variance of the data. 
    This is \textbf{noise} present in the data.
    Also called Bayes, intrinsic or unavoidable error.
    No matter what we do, we will never get below this error.
  \item The second term expresses how the predictions fluctuate on test-points on average, if we vary the training data. Expresses also the learner's tendency to learn random things irrespective of the real signal (overfitting).
  \item The third term says how much we are "off" on average at test locations (underfitting).
    Models with high capacity have low \textbf{bias} and models with low capacity have high \textbf{bias}.
\end{enumerate}


% So for the squared error loss, the generalized prediction error can be decomposed into

% \begin{itemize}
% \item \textbf{Noise}: Intrinsic error, independent from the learner, cannot be avoided
% \item \textbf{Variance}: Learner's tendency to learn random things irrespective of the real signal (overfitting)
% \item \textbf{Bias}: Learner's tendency to \textbf{consistently} misclassify certain instances (underfitting)
% \end{itemize}




% \framebreak

% For the $k$-NN learning algorithm, which outputs models of the shape $$\fh_{\D}(x) = \frac{1}{k}\sum_{i: \xi \in N_k(x)} \yi,$$
% the generalization error becomes 

% \begin{eqnarray*}
% \GE(\mathcal{I}_{L, O}) &=& \sigma^2 + \var\left(\fh_{\D}(\xv)\right) + \text{Bias}\left(\fh_{\D}(\xv)\right)^2 \\
% &=&\sigma^2 + \frac{\sigma^2}{k} + \E_x \left(f(\xv) - \frac{1}{k}\sum_{\xi \in N_k(\xv)}\fxi\right)^2 \\
% \end{eqnarray*}

% where we assumed for simplicity that training inputs $\xv^{(i)}$ are fixed and the randomness arises only from $y$.



\framebreak

Let us consider the following example. We will generate a dataset using the following model : 
$$y = x + \frac{x^2}{2} + \epsilon  \ , \ \ \ \ \epsilon \sim 
N (0, 1)$$

The data is then split in a training set and a test set.

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-train_test.png}
\end{center}

\framebreak

We will train several (low capacity) linear models sampling with replacement from the training dataset. This is commonly known as \textbf{bootstrapping}.

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_1.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_2.png}
\end{center}

\framebreak

By creating several models, we obtain the average model over different samples of the training dataset.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model.png}
\end{center}

\framebreak

We can now evaluate the squared bias, by computing the average squared difference between the average model and the true model, on the location of the test points.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model_bias.png}
\end{center}

\framebreak

We may also calculate the average variance of the predictions of the models we trained, at the test points location.

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-linear_model_variance.png}
\end{center}

The generalization error is then: 
$$\GEind = 1 + 1.628 + 0.135 = 2.763 $$


\framebreak

We will repeat the same procedure, but using a high-degree polynomial that has more capacity.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-complex_model.png}
\end{center}


\framebreak

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-complex_model_bias.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-complex_model_variance.png}
\end{center}


The generalization error is then: 
$$\GEind = 1 + 0.139 + 1.963 = 3.102 $$


\framebreak

What happens if we use a model with the same complexity as the true model? 

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-correct_model.png}
\end{center}

\framebreak

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-correct_model_bias.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-correct_model_variance.png}
\end{center}


The generalization error is then: 
$$\GEind = 1 + 0.008 + 0.082 = 1.091 $$

\end{vbframe}

\begin{vbframe}{Capacity and overfitting}

\begin{itemize}
  \item 
    The performance of a learner depends on its ability to 
    \begin{itemize}
      \item \textbf{fit} the training data well
      \item \textbf{generalize} to new data
    \end{itemize}  
  \item Failure of the first point is called \textbf{underfitting}
  \item Failure of the second item is called \textbf{overfitting}
\end{itemize}  


In our examples we could see that:

\begin{itemize}
  \item The linear model failed to fit the training data well and thus underfitted (high-bias).
  \item The high-degree polynomial model failed to generalize to new data and thus overfitted (high-variance).
  \item The best Generalization error is obtained when the model has the correct complexity.
  \item Even if the model is correct, there is a lower boundary for the error: The Variance of the data.
\end{itemize}

\framebreak

\begin{figure}
  \centering
  \includegraphics[width = 0.6\textwidth]{figure_man/lcurve_1.png}
  \tiny{\\ Credit: Ian Goodfellow}
\end{figure}


\begin{itemize}
  \item The tendency of a model to over/under fit is a function of its capacity, determined by the type of hypotheses it can learn.
  \item The generalization error is minimized when it has the right capacity.
\end{itemize}
\end{vbframe}


\endlecture
\end{document}


