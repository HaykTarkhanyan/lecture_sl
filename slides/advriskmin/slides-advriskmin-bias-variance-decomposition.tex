\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{% Lecture title  
    Bias-variance Decomposition
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/bias_variance_decomposition-linear_model_bias.png
  }{
  \item Understand how to decompose the generalization error of a learner into 
  \begin{itemize}
    \item \footnotesize bias of the learner
    \item \footnotesize variance of the learner
    \item \footnotesize inherent noise in the data
  \end{itemize} 
}

\begin{vbframe} {Bias-variance decomposition}

Generalization error of learner  $\ind$: 
Expected error of model $\fh_{\D_n}$, on training sets of size $n$, evaluated on a fresh, random test sample.
  $$GE_n\left(\ind\right) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right)  $$
Expectation is taken over all training sets \textbf{and} independent test sample.\\
\lz
For L2 loss, there is an additive decomposition of $GE_n\left(\ind\right)$ into three components.$^{\ast}$ For this, we assume that the data is generated by 
$$
y = \ftrue(\xv) + \epsilon\,,
$$
with zero-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$ independent of $\xv$.
\vspace{0.1cm}

{\scriptsize ${\ast}$ Similar decomps also exist for other losses expressible as Bregman divergences (e.g. cross-entropy). One exception is the $0/1$ loss \citelink{BROWN2024BIAS}}

\framebreak 


\begin{footnotesize}
$GE_n\left(\ind\right) =$  
$$
 \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of learner at } (\xv, y)} + \E_{xy}\underbrace{\left[\left(\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of learner at } (\xv, y)}  
$$
\end{footnotesize}
\begin{enumerate}
  \item The first term expresses the variance of the data. 
    This is pure \textbf{noise} in the data.
    Also called Bayes, intrinsic or irreducible error.
    No matter what we do, we will never get below this error.
  \item The second term expresses, on average, how much $\fh_{\D_n}(\xv)$ fluctuates around test points if we vary the training data. Also expresses learner's tendency to learn random things irrespective of real signal (overfitting)
  \item The third term says how much we are ``off'' on average at test locations (underfitting).
    Models with high capacity typically have low \textbf{bias} and \textit{vice versa}
\end{enumerate}




\framebreak

\textbf{Illustration}: Let us consider the following example. We generate a dataset using the following model : 
$$y = x + \frac{x^2}{2} + \epsilon  \ , \ \ \ \ \epsilon \sim 
N (0, 1)$$

The data is then split into a training set and a test set.

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-train_test.png}
\end{center}

\framebreak

To obtain estimates for the bias and variance, we train several models by sampling with replacement from the training data. This is commonly known as \textbf{bootstrapping}. \\
\vspace{0.3cm}
First, we train several (low capacity) linear models (polynomial of degree $d=1$).

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_1.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_2.png}
\end{center}

\framebreak

By creating several models, we obtain the average model over different samples of the training dataset.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model.png}
\end{center}

\framebreak

We can now estimate the (squared) bias, by computing the average squared difference between the average model and the true model, at the test point locations.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model_bias.png}
\end{center}

\framebreak

We compute the average variance of the predictions of the models we trained at the test point locations.
\vspace{-0.35cm}
\begin{center}
  \includegraphics[width = 0.35\textwidth]{figure/bias_variance_decomposition-linear_model_variance.png}
\end{center}
\vspace{-0.8cm}

$$GE_n\left(\ind\right) \approx 1 + 1.628 + 0.135 = 2.763 $$


\begin{itemize}
  \item The biggest component of the generalization error is the bias.
%  \item Computing the MSE in the usual way for each model, via L2 loss, and then averaging over models gives rise to nearly the same value, as expected
\end{itemize}

\framebreak

\begin{center}
  \includegraphics[width = 0.35\textwidth]{figure/bias_variance_decomposition-linear_model_variance.png}
\end{center}

\begin{footnotesize}

\begin{itemize}
  \item We can now check whether this alternative computation of the GE is correct
  \item So, we simply compute the MSE in the standard fashion for each model
  \item So for each model we compute the L2 loss at each data point, then average
  \item Then we average these MSEs over all models
  \item Result = 2.72, would be closer if we average over more models and test points 
\end{itemize}

\end{footnotesize}


\framebreak


We will repeat the same procedure, but use a high-degree polynomial ($d=7$) with more capacity.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-complex_model.png}
\end{center}


\framebreak

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-complex_model_bias.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-complex_model_variance.png}
\end{center}

$$GE_n\left(\ind\right) \approx 1 + 0.139 + 1.963 = 3.102 $$

\begin{itemize}
  \item The generalization error is higher than before
  \item Even though the bias is lower, the variance of the learner is higher. 
\end{itemize}



\framebreak

What happens if we use a model with the same complexity as the true model (quadratic polynomial)? 

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-correct_model.png}
\end{center}

\framebreak

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-correct_model_bias.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-correct_model_variance.png}
\end{center}

$$GE_n\left(\ind\right) \approx 1 + 0.008 + 0.082 = 1.091 $$

\begin{itemize}
  \item The generalization error is the lowest at this complexity.
  \item The variance of the data acts as a lower bound.
\end{itemize}
 

\end{vbframe}


\begin{vbframe}{Capacity and overfitting}

  \begin{figure}
    \centering
    \includegraphics[width = 0.6\textwidth]{figure_man/lcurve_1.png}
    \tiny{\\ Credit: Ian Goodfellow}
  \end{figure}
  

\begin{itemize}
  \item 
    The performance of a learner depends on its ability to 
    \begin{enumerate}
      \item \textbf{fit} the training data well
      \item \textbf{generalize} to new data
    \end{enumerate}  
  \item Failure of the first point is called \textbf{underfitting}
  \item Failure of the second point is called \textbf{overfitting}
\end{itemize}  


\framebreak

\begin{figure}
  \centering
  \includegraphics[width = 0.6\textwidth]{figure_man/lcurve_1.png}
  \tiny{\\ Credit: Ian Goodfellow}
\end{figure}


\begin{itemize}
  \item The tendency of a model to underfit/overfit is a function of its capacity, determined by the type of hypotheses it can learn
  \item Usually, low bias means high capacity, which in turn means a higher chance of overfitting
  \item Low-bias models usually have 
  also higher variance
  \item For such models, regularization (we discuss later) is essential
  \item Even for correctly specified models, the generalization error is lower-bounded by the irreducible noise $\sigma^2$
\end{itemize}
\end{vbframe}


\begin{vbframe}{Approximation and Estimation \citelink{BROWN2024BIAS} }
The bias-variance decomp is often confused or equated with the related (but different) decomp of \textbf{excess risk} into \textbf{estimation} and \textbf{approximation} error.
\vspace{-0.3cm}

\begin{eqnarray*}
    \underbrace{\risk(\hat f_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}})}_{\text{excess risk}} &=& \underbrace{\risk(\hat f_{\Hspace}) - \risk(\fbayes_{\Hspace})}_{\text{estimation error}} + \underbrace{\risk(\fbayes_{\Hspace}) -  \risk(\fbayes_{\Hspace_{all}})}_{\text{approx. error}} 
\end{eqnarray*}

Both are commonly described using the same figure and analogies
\vspace{-0.1cm}
\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{figure_man/biasvar-vs-estapprox-tradeoff.png}
    \tiny{\\ Credit: \cite{BROWN2024BIAS}}
  \end{figure}

{\footnotesize \textbf{NB}: Bias-var decomp. only holds for certain losses, above decomposition is universal.}

\end{vbframe}

%\framebreak

\begin{vbframe}{Approx./Estimation Error \citelink{BROWN2024BIAS}}
    
\begin{itemize}
    \item The approx. error is a structural property of $\Hspace$.
    \item The estimation error is random due to dependence on data in $\fh$
    \item Estimation error arises because we choose $f \in \Hspace$ with limited training data using $\riske$ instead of $\risk$
\end{itemize}

Knowing $\fh_{\Hspace} \in \arg\inf_{f \in \Hspace} \riske(f)$ assumes we found a global minimizer of $\riske$, which is often impossible (e.g. DNNs). 
\vspace{0.2cm}

In practice, optimizing $\riske$ gives us our ``best guess'' $\tilde{f}_{\Hspace} \in \Hspace$ of $\fh_{\Hspace}$. We can now decompose its excess risk finer as

\begin{eqnarray*}
    \underbrace{\risk(\tilde{f}_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}})}_{\text{excess risk}} &=& \underbrace{\risk(\tilde{f}_{\Hspace}) - \risk(\fh_{\Hspace})}_{\text{optim. error}} + \underbrace{\risk(\hat{f}_{\Hspace}) - \risk(\fbayes_{\Hspace})}_{\text{estimation error}} + \underbrace{\risk(\fbayes_{\Hspace}) -  \risk(\fbayes_{\Hspace_{all}})}_{\text{approx. error}} 
\end{eqnarray*}

Note that the optimization error can be negative, even though $\riske(\tilde{f}_{\Hspace}) \geq \riske(\fh_{\Hspace})$ always holds.

\framebreak

We can further decompose estimation error more finely by defining the \textit{centroid} model or ``systematic'' model part.\\
\vspace{0.15cm}

Given $\fh_{\Hspace} \in  \arg\min_{f \in \Hspace} \riske(f)$ the centroid model under squared loss is the mean prediction at each $x$ over all $\D_n$, $f^{\circ}_{\Hspace} := \E_{\D_n \sim \Pxy^n}[\fh_{\Hspace}] $.
\vspace{0.15cm}

With $f^{\circ}_{\Hspace}$ we can decompose the expected estimation error as

\begin{eqnarray*}
 \underbrace{\E_{\D_n \sim \Pxy^n}\left[\risk(\hat{f}_{\Hspace}) - \risk(f ^{\ast}_{\Hspace})\right]}_{\text{expected estimation error}} &=& \underbrace{\E_{\D_n \sim \Pxy^n}\left[\risk(\hat{f}_{\Hspace}) - \risk(f^{\circ}_{\Hspace}) \right]}_{\text{estimation variance}} + \underbrace{\risk(f^{\circ}_{\Hspace})-\risk(f^{\ast}_{\Hspace})}_{\text{estimation bias}}   
\end{eqnarray*}

\begin{itemize}

\item estimation bias measures distance of centroid model to risk minimizer over $\Hspace$
\item estimation variance measures spread of ERM around centroid model induced by randomness due to $\D_n$
\end{itemize}

\framebreak

{\small We can now connect the derived quantities back to bias and variance and see how they differ. As we see, bias is not only approx. error and variance is not estimation error.}
\vspace{-0.2cm}

$$ \text{bias} = \text{approximation error} + \text{estimation bias} $$
$$\text{variance} = \text{optimization error} + \text{estimation variance}$$
% variance = estimation variance + optimization error
\vspace{-0.4cm}
\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figure_man/expected-risk-decomp.png}
    \tiny{\\ Credit: \cite{BROWN2024BIAS}}
  \end{figure}

{\scriptsize \textbf{NB}: For special case of LM and squared loss (OLS), we have zero optim. error and estimation bias. Hence both decompositions agree there.}


\end{vbframe}

\endlecture
\end{document}


