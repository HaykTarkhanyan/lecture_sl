\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Pseudo-Residuals
}{
figure/pseudo_residual_1.png
}{
\item Concept of pseudo-residuals 
\item PRs for common losses
}

\begin{vbframe}{Pseudo-Residuals}

\begin{itemize}
\item In regression, residuals are defined as 
$\rx := y - \fx$
\item Generalize concept to \textbf{pseudo-residuals}: 
%as the negative first derivatives of loss functions w.r.t. $\fx$
$$
\tilde{r}(\xv) := - \frac{\partial \Lxy}{\partial \fx}
$$
\item If we wiggle $\fx$, how much does $L$ change?
\item Can be used for score-based classifiers and other models
\item Note that $\tilde{r}(\xv)$ depends on $y$, $\fx$ and $L$

\vfill

% \includegraphics[width=0.7\textwidth]{figure_man/loss.png}
\includegraphics[width=0.7\textwidth]{figure/plot_quad_pseudores.png}

\end{itemize}

\end{vbframe}


\begin{frame}[t]{Best point-wise update}

\begin{overlayarea}{\textwidth}{\textheight}

\begin{itemize}
\item Assume we have (partially) fitted a model $\fx$ to data $\D$ 
\item Assume we could update $\fx$ point-wise as we like 
\item Under squared loss, for a fixed $\xv \in \Xspace$, the best point-wise update is the direction of the residual $\rx = y - \fx$
$$
\fx \leftarrow \fx + \rx
$$
\only<2>{
\item  Point-wise error at this specific $\xv$ becomes $0$
}
\only<3>{
\item In gradient boosting, we don't do point-wise updates but ``smoothly distort'' $f$ so we generalize
}
\end{itemize}

\begin{center}
\only<1>{\includegraphics[width=0.4\textwidth]{figure/pseudo_residual_1.png}}
\only<2>{\includegraphics[width=0.4\textwidth]{figure/pseudo_residual_2.png}}
\only<3>{\includegraphics[width=0.4\textwidth]{figure/pseudo_residual_3.png}}
\end{center}

\end{overlayarea} 

\end{frame}

\begin{vbframe}{Approximate Best point-wise update}

\begin{itemize}

\item When applying GD to compute a point-wise update of $\fx$, 
we go a step of negative gradient of loss

$$
	\fx \leftarrow \fx - \frac{\partial \Lxy}{\partial \fx} 
$$
\item This would be for one observation 

\item This is effectively the PR which is the direction of the pseudo-residual

\begin{eqnarray*}
	\fx &\leftarrow& \fx + \tilde{r}(\xv)\\ 
\end{eqnarray*}

\item Iteratively stepping towards the direction of the pseudo-residuals is the underlying idea of gradient boosting, which is a learning algorithm that will be covered in a later chapter

\end{itemize}

\end{vbframe}


\begin{vbframe}{GD in ML and Pseudo-Residuals}

\begin{itemize}
	\item In GD, we move in the direction of the negative gradient by updating the parameters: 
	$$
	 \thetav^{[t + 1]} = \thetav^{[t]} - \alpha^{[t]} \cdot \nabla_{\thetav} \left.\risket\right|_{\thetav = \thetav^{[t]}}	
	$$
	% with step size $\alpha^{[t]}$. 
	\item This can be seen as approximating the unexplained information (measured by the loss) through a model update
	% \framebreak 
	\item Using the chain rule:
        % mwe see that the pseudo-residuals are input to the update direction
	\begin{eqnarray*}
	\nabla_{\thetav} \risket &=&\sumin \left.\frac{\partial L\left(\yi, \fx\right)}{\partial \fx} \right|_{f = \fxit} 
        % _{= - \tilde r^{(i)}}
	\cdot \nabla_{\thetav} \fxit \\ 
	&=& - \sumin \tilde r^{(i)} \cdot \nabla_{\thetav} \fxit.
	\end{eqnarray*}
	\item Update is loss-optimal directional change of model output 
        and a loss-independent derivative of $\fx$
        \item This is a flexible, nearly loss-independent variant of GD
        % The unexplained information -- the negative gradient -- can be thought of as residuals, which is therefore also called pseudo-residuals. 
\end{itemize}	

% For risk minimization, the update rule for the parameter $\thetav$ is 
% \begin{footnotesize}
% \begin{eqnarray*}
% \thetav^{[t+1]} &\leftarrow & \thetav^{[t]} - \alpha^{[t]}  \sumin \nabla_{\thetav} \left. \Lxyit \right|_{\thetav = \thetav^{[t]}} \\
% \thetav^{[t+1]} &\leftarrow & \thetav^{[t]} + \alpha^{[t]} \sumin \tilde r^{(i)} \cdot \left. \nabla_{\thetav} \fxit \right|_{\thetav = \thetav^{[t]}} 
% \end{eqnarray*}
% \end{footnotesize}
% $\alpha^{[t]} \in [0,1]$ is called \enquote{learning rate} in this context.
\end{vbframe}

\begin{vbframe}{Pseudo-residuals for common losses}

\lz
\lz

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|l|}
\hline
\textbf{Loss} & \textbf{Domain of $y$} & \textbf{Pseudo residual $\tilde r$} \\ \hline
Squared loss & $y \in \mathbb{R}$ & $y - f(\xv)$ \\ \hline
Bernoulli loss & $y \in \{0,1\}$ & $y - s(f(\xv))=y-\pi(\xv)$ \\ \hline
Multinomial loss & $y \in \{1,\dots,g\}$ & $\mathds{1}_{\{y=k\}} - \pi_k(\xv)$ \\ \hline
Exponential loss & $y \in \{-1,1\}$ & $y\,\exp(-yf(\xv))$ \\ \hline
\end{tabular}
\end{table}

\lz

NB: $\pi(\xv)=s(f(\xv))=\frac{\exp(f(\xv))}{1+\exp(f(\xv))}$ is the (sigmoidal) logistic function,
and $\pi_k(\xv)$ its multi-class generalization, the softmax.

\end{vbframe}



\endlecture

\end{document}
