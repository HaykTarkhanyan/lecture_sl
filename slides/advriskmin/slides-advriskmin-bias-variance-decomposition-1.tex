\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Bias-Variance 2: \\
Bias-Variance Decomposition
}{
figure/bias_variance_decomposition-linear_model_bias.png
}{
\item Decompose GE of learner into 
\begin{itemize}
\item \footnotesize bias of learner
\item \footnotesize variance of learner
\item \footnotesize inherent noise of data
\end{itemize} 
\item Simulation study demo
}

\begin{vbframe} {Bias-variance decomposition}

\begin{itemize} 
\item Generalization error of learner  $\ind$: 
Expected error of model $\ind(\D_n) = \fh_{\D_n}$, on train sets of size $n$, evaled on fresh test sample
$$
GE_n\left(\ind\right) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right)  
$$

\item $\E$ taken over all train sets \textbf{and} independent test sample

\item For L2 loss, can additively decompose  $GE_n\left(\ind\right)$ into 3 components

\item Assume data is generated by 
$$
y = \ftrue(\xv) + \epsilon
$$
with 0-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$; independent of $\xv$

\item Similar decomps exist for other losses expressable as Bregman divergences (e.g. log-loss). One exception is $0/1$ \citelink{BROWN2024BIAS}

\end{itemize} 

\framebreak 


$GE_n\left(\ind\right) =$  
$$
 \underbrace{\sigma^2}_{\text{Var. of $y$}} + \E_{x}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv\right)\right]}_{\text{Variance of learner at } \xv} + \E_{x}\underbrace{\left[\left(\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2~|~\xv\right)\right]}_{\text{Squared bias of learner at } \xv}  
$$

\begin{enumerate}
  \item First: variance of data (in $y$), ``pure''
     \textbf{noise}; aka Bayes, intrinsic or irreducible error; 
    whatever we we do, will never be better
  \item Second: how much \textbf{$\fh_{\D_n}(\xv)$ fluctuates} at test $\xv$ if we vary training data, averaged over feature space; = learner's tendency to learn random things irrespective of real signal (overfitting)
  
  \item Third: how ``off'' are we on average at test locations (underfitting); uses ``average model integrated out over all $\D_n$; \\
  models with high capacity have low \textbf{bias} and vice versa
\end{enumerate}


\end{vbframe} 

\begin{vbframe} {Simulation Example}

\begin{itemize}
\item True model:
$$y = x + \frac{x^2}{2} + \epsilon  \qquad \epsilon \sim 
N (0, 1)$$
\item Split in train and test sets 
\begin{center}
  \includegraphics[width = 0.42\textwidth]{figure/bias_variance_decomposition-train_test.png}
\end{center}

\end{itemize}

\end{vbframe} 


\begin{vbframe} {Simulation Example}


\begin{itemize}
\item Let's estimate bias and variance via bootstrapping

\item (Could have also used Monte Carlo integration of the above quantities,
BS slightly easier to visually explain)

\item First, train several (low capacity) LMs
\item These are the $\fh_{\D_n}(\xv)$, seen as a RV, based on the random
data $\D_n}$


\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_1.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_2.png}
\end{center}
\end{itemize}


\end{vbframe} 

\begin{vbframe} {Average Model}

\begin{itemize}
\item Average model over different training datasets
\item This is $\E_{\D_n}[\fh_{\D_n}(\xv)]$ in the decomp
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model.png}
\end{center}

\end{vbframe} 


\begin{vbframe} {Squared Bias Computation / Estimation}

\begin{itemize}
\item Compute sq. diff. between avg. and true model at each test $x$
\item Then average over all test points
\item This is 
$ 
\E_x[ ( \ftrue(\xv)-\E_{\D_n} (\fh_{\D_n}(\xv) ))^2~|~\xv ) ]
$
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model_bias.png}
\end{center}

\end{vbframe} 


\begin{vbframe} {Variance Computation}

\begin{itemize}
\item Compute variance of model predictions at each test $x$
\item Then average over all test points
\item This is 
$
\E_{x} [\var_{\D_n} (\fh_{\D_n}(\xv) ~|~\xv ) ]
$

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-linear_model_variance.png}
\end{center}

\item Here, we know data variance $\sigma^2=1$;\\
could also estimate it from residuals

\end{itemize}


\end{vbframe} 


\begin{vbframe} {Decomp Result and Comparison with MSE}

\begin{footnotesize}
\begin{itemize}
\item Decomp result; here bias is largest:
$$GE_n\left(\ind\right) \approx 1 + 1.628 + 0.135 = 2.763 $$
\begin{center}
  \includegraphics[width = 0.35\textwidth]{figure/bias_variance_decomposition-linear_model_mse.png}
\end{center}
\item Regular MSE: For each model, compute MSE on test set
\item Then we average these MSEs over all models
\item Result = 2.72; checks out; \\
better if we avg. over more models and test points 
\item In general: Error quite high as we underfitted
\end{itemize}

\end{footnotesize}

\end{vbframe} 


\begin{vbframe} {Higher complexity learner}

\begin{itemize}
\item Same procedure, but using a high-degree polynomial ($d=7$)

\begin{center}
  \includegraphics[width = 0.55\textwidth]{figure/bias_variance_decomposition-complex_model.png}
\end{center}


\framebreak

\begin{center}
  \includegraphics[width = 0.3\textwidth]{figure/bias_variance_decomposition-complex_model_bias.png}
  \includegraphics[width = 0.3\textwidth]{figure/bias_variance_decomposition-complex_model_variance.png}
  \includegraphics[width = 0.3\textwidth]{figure/bias_variance_decomposition-complex_model_mse.png}
\end{center}


$$GE_n\left(\ind\right) \approx 1 + 0.139 + 1.963 \approx 3.103 $$

\vfill


\item GE higher than before, although hypo space now contains $\ftrue$
\item Bias is lower, and variance higher 
\item Higher capacity learner overfits (here), to our detriment\\
   We also do not regularize, that would be better
   
\item NB: There is an ``edge effect'' on LHS, Runge effect,\\
  leads to higher bias as ``artifact'' here (ignore this)


\framebreak

\item What happens if we use a model with the same complexity as the true model (quadratic polynomial)? 
\end{itemize}

\begin{center}
  \includegraphics[width = 0.55\textwidth]{figure/bias_variance_decomposition-correct_model.png}
\end{center}

\framebreak

\begin{center}
  \includegraphics[width = 0.3\textwidth]{figure/bias_variance_decomposition-correct_model_bias.png}
  \includegraphics[width = 0.3\textwidth]{figure/bias_variance_decomposition-correct_model_variance.png}
  \includegraphics[width = 0.3\textwidth]{figure/bias_variance_decomposition-correct_model_mse.png}
\end{center}

$$GE_n\left(\ind\right) \approx 1 + 0.008 + 0.082 = 1.09 $$

\begin{itemize}
  \item Naturally: better result
  \item Low bias, low variance
  \item Bias should not be that much lower than
  high degree polynomial; but see comment there
  \item In any case, variance of the data is lower bound
\end{itemize}
 

\end{vbframe}


\endlecture
\end{document}


