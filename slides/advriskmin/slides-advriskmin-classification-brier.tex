\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-trees} % For the comparison of Brier and Gini index

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    Brier Score
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/brier.png
  }{
  \item Know the Brier score 
  \item Derive the risk minimizer
  \item Derive the optimal constant model 
  % \item Understand the connection between Brier score and Gini splitting 
}

% \begin{vbframe}{Classification Losses: (Naive) L2-Loss}


% $$
% \Lxy = (1 - y\fx)^2,
% $$


% \begin{itemize}
%   \item L2-loss defined on scores
%   \item Predictions with high confidence $|f(x)|$ are penalized  regardless of whether the signs of $y$ and $f(x)$ match.
%   \item Squared loss on the loss functions is thus not the best choice.

%   <<loss-squareclass-plot, echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
%  x = seq(-2, 5, by = 0.01)
%  plot(x, (1-x)^2, type = "l", xlab = expression(y %.% f(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
%  box()
% @

%   \item The theoretical risk becomes
%     \begin{eqnarray*}
%     \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 (P(1 | x)) + (1+\fx)^2 (1-P(1 | x))] \\
%     &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx P(1 | x)].
%     \end{eqnarray*}
%   \item By differentiating w.r.t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

%     \begin{eqnarray*}
%     f(\xv) &=& 2\cdot P(1 | \xv) - 1.
%     \end{eqnarray*}
%     \item The empiricla optimzer is then
%     $$
%     \fh(\xv) = \frac{2}{n}\cdot \sumin \I[y^{(i)} = 1] - 1.
%     $$


% \end{itemize}

% \end{vbframe}

\begin{vbframe}{Brier Score}

The binary Brier score is defined on probabilities $\pi \in [0, 1]$ and 0-1-encoded labels $y \in \{0, 1\}$ and is the $L2$ loss on probabilities.
\begin{equation*}
\Lpiy = (\pi - y)^2
\end{equation*}
As the Brier score is a proper scoring rule (cf. section on proper scoring rules), it can be used for calibration. %Note that it is not convex on probabilities anymore.
Despite convex in $\pi$, $$L(y,\pi(f))=((1+\exp{(-f)})^{-1}-y)^2$$ as a composite function is not convex in $f$ anymore (log. sigmoid for $\pi$).
\vspace{-0.2cm}
\begin{center}
\includegraphics[width = 0.6\textwidth]{figure/brier.png}
\end{center}


\end{vbframe}

\begin{vbframe}{Brier Score: Risk Minimizer}

The risk minimizer for the (binary) Brier score is 

\begin{eqnarray*}
\pixbayes = \eta(\xv) := \P(y=1~|~\xv = \xv),
\end{eqnarray*}

which means that the Brier score attains its minimum if the prediction equals the \enquote{true} probability $\eta(\xv)$ of the outcome. 

\lz 

The risk minimizer for the multiclass Brier score is 
$$\pi^\ast(\xv) = \P(y = k ~|~ \xv = \xv). $$
 

\textbf{Proof: } We only show the proof for the binary case. We need to minimize 

$$
\E_x \left[L(1, \pix) \cdot \eta(\xv) + L(0, \pix) \cdot (1 - \eta(\xv)) \right],
$$

which we do point-wise for every $\xv$. We plug in the Brier score

\vspace*{-0.3cm}

\begin{eqnarray*}
	&& \argmin_c \quad L(1, c) \eta(\xv) + L(0, c) (1 - \eta(\xv)) \\ 
	&=&  \argmin_c \quad (c - 1)^2 \eta(\xv) + c^2 (1 - \eta(\xv))  \quad |{+\eta(\xv)^2-\eta(\xv)^2}\\
  &=&  \argmin_c \quad (c^2 -2c\eta(\xv) + \eta(\xv)^2)- \eta(\xv)^2 + \eta(\xv) \\
	&=&  \argmin_c \quad (c - \eta(\xv))^2.
\end{eqnarray*}

The expression is minimal if $c = \eta(\xv) = \P(y = 1~|~\xv = \xv)$.

\end{vbframe}

\begin{vbframe}{Brier Score: Optimal constant Model}

The optimal constant probability model $\pix = \theta$ w.r.t. the Brier score for labels from $\Yspace = \setzo$ is:

\vspace*{-0.2cm}

\begin{equation*}
  \argmin_{\theta} \riske(\theta) = \argmin_{\theta} \sumin \left(\yi - \theta\right)^2 = \frac{1}{n} \sumin \yi
  %\Leftrightarrow \frac{\partial \riske(\theta)}{\partial \theta} &=& - 2 \cdot \sumin (\yi - \theta) = 0 \\
  %\hat \theta &=& \frac{1}{n} \sumin \yi.   
\end{equation*}

This is the fraction of class-1 observations in the observed data.\\
(This directly follows from our $L2$ proof for regression).

\vspace*{0.2cm}

Similarly, for the multiclass brier score the optimal constant is $$\thetah_k = \frac{1}{n}\sumin [y = k]$$. 

\end{vbframe}

\begin{vbframe}{Calibration and the Brier Score}
A predictor \(\pi(\mathbf{x})\in[0,1]\) is \emph{calibrated} if 
\[
\P\bigl(y=1 \mid \pi(\mathbf{x})=p\bigr)=p \quad \forall\, p\in[0,1].
\]
Intuitively, this means if we predict $p$, then in $100p\%$ of cases we observe $y=1$ (neither over- or underconfident). Recall the risk minimizer for the Brier score is
\[
\pi^*(\mathbf{x}) = \eta(\mathbf{x}) = \P(y=1 \mid \mathbf{x}).
\]
Since \(\pi^*(\mathbf{x})=\eta(\mathbf{x})\) exactly, it follows that the optimal predictor satisfies
\[
\P\bigl(y=1 \mid \pi^*(\mathbf{x})=p\bigr)=p,
\]
i.e., is perfectly calibrated.
\end{vbframe}







\endlecture

\end{document}