\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}
\input{../../latex-math/ml-trees} % For the comparison of Brier and Gini index

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{Bernoulli Loss
  }{figure/bernoulli_prob.png
  }{
  \item Bernoulli (log, logistic, binomial) loss
  \item Risk minimizer
  \item Optimal constant
  \item Complete separation problem
}

\begin{vbframe}{On probabilities}
 
\begin{itemizeL}
\item  Likelihood of Bernoulli RV:
$$
\LLt = \prodin \pixit^{\yi} \left(1-\pixit\right)^{1-\yi} \,\,, \, y \in \setzo
$$
\item Transform into NLL:
$$- \loglt = \sumin - \yi \log\left(\pixit\right) - \left(1-\yi\right)\log\left(1-\pixit\right)$$
\item Bernoulli loss: loss on single sample
$$
\Lpixy = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right) \qquad y \in \setzo 
$$ 
\end{itemizeL}

\end{vbframe}


\begin{vbframe}{On probabilities}
 
\begin{itemizeS}
\item Bernoulli loss
$$
\Lpixy = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right) \qquad y \in \setzo 
$$ 
\item Confidently wrong predictions are harshly penalized

\begin{center}
\includegraphics[width=0.35\textwidth]{figure/bernoulli_prob.png}  \end{center}

\item A.k.a. Binomial, log, or cross-entropy loss
\item Can also write for $y \in \setmp $
$$
\Lpixy  = - \frac{1 + y}{2} \log\left(\pix\right) - \frac{1 - y}{2} \log\left(1 - \pix\right) \qquad y \in \setmp 
$$
\end{itemizeS}

\end{vbframe}



\begin{vbframe}{On decision scores}

\begin{itemize}
    \item Transform probs into scores (log-odds): $\fx = \log\left(\frac{\pix}{1 - \pix}\right)$
    \item Then $\pix = \left(1 + \exp(-\fx)\right)^{-1}$
    \item Yields equivalent loss formulation
$$\Lxy = - y \cdot \fx + \log(1 + \exp(\fx)) \quad \text{for } y \in \setzo$$
    \item For these and other simple derivations, see deep dive
\end{itemize}

\vfill

\begin{center}
% \includegraphics[width = 10cm ]{figure_man/bernoulli-loss.png} \\
\begin{figure}
  \subfloat{\includegraphics[width=0.45\textwidth]{figure/logistic.png}}
  \includegraphics[width=0.45\textwidth]{figure/bernoulli_logloss.png}\\
\end{figure}
\end{center}

\end{vbframe}

\begin{vbframe}{Loss in terms of margin}

\begin{itemize}
\item For $y \in \setmp$, loss becomes: 
$$
  \Lxy = \log(1+\exp(-y \cdot \fx)) 
$$
%  \item For $y=-1$ plug $y'=0$ in old loss: $L(0,\fx)=\log(1+\exp(\fx)$
%  \item For $y=y'=1$: 
%  \begin{eqnarray*}
%   L(1,\fx)=-\fx+\log(1+\exp(\fx))&=&\log(1+\exp(\fx))-\log(\exp(\fx))\\
%   &=&\log(1+\exp(-\fx)
% \end{eqnarray*}
 
%  \item Convert between 2 encodings using $y=2y'-1$ for $y' \in \setzo$
\item All loss variants convex, differentiable
\end{itemize}

 \vfill

\begin{center}
\includegraphics[width = 8cm]{figure/bernoulli_margin.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Risk Minimizer on Probs}

\begin{itemize}
    \item For probs and $y \in \setzo$, the risk minimizer is
$$
  \piastxtil = \eta(\xtil) = \P(y = 1 ~|~ \xv = \xtil) 
$$
\end{itemize}

%% BB: I am keeping this proof here for didactive purposes

\textbf{Proof:} We have seen before
$$
  \riskf = \E_x \left[L(1, \pix) \cdot \eta(\xv) + L(0, \pix) \cdot (1 - \eta(\xv)) \right]
$$

For fixed $\xv$, minimize inner part pointwise, use $c\in(0,1)$ for best value:

\begin{footnotesize}
\begin{eqnarray*}
  \frac{d }{d c} \left(- \log c  \cdot \eta(\xv)- \log (1 - c) \cdot (1 - \eta(\xv))\right) &=& 0 \\
  - \frac{\eta(\xv)}{c} + \frac{1 - \eta(\xv)}{1 - c} &=& 0 \\
  \frac{- \eta(\xv) + \eta(\xv) c + c - \eta(\xv) c}{c (1 - c)} &=& 0 \\
  c &=& \eta(\xv)
\end{eqnarray*}
\end{footnotesize}

\end{vbframe}

\begin{vbframe}{Risk Minimizer on Scores}


\begin{itemizeS}
\item For $y \in \{-1, 1\}$ and scores $\fx$: RM is pointwise log-odds
$$\fxbayes =  \log (\frac{\eta(\xv) }{1-\eta(\xv) })$$
\item Undefined for $\eta(\xv) \in \setzo$
\item Monotonously increasing in $\eta(\xv)$, with  $\fbayes(\xv) = 0$ if $\eta(\xv)=0.5$

\end{itemizeS}

\vfill

\begin{center}
\includegraphics[width=0.5\textwidth]{figure/logistic_inverse.png}
\end{center}

% \textbf{Proof: } As before we minimize 
% \begin{eqnarray*}
%   \riskf &=& \E_x \left[L(1, \fx) \cdot \eta(\xv) + L(-1, \fx) \cdot (1 - \eta(\xv)) \right] \\
%   &=& \E_x \left[ \log(1 + \exp(- \fx)) \eta(\xv)+ \log(1 + \exp(\fx)) (1 - \eta(\xv)) \right] 
% \end{eqnarray*}

% For fixed $\xv$ we get pointwise optimal value $c$ by setting derivative to $0$: 

% \begin{footnotesize}
%   \begin{eqnarray*}
%   \frac{d }{d c} \log(1 + \exp(-c)) \eta(\xv)+ \log(1 + \exp(c)) (1 - \eta(\xv)) &=& 0 \\
%   - \frac{\exp(-c)}{1 + \exp(-c)} \eta(\xv) + \frac{\exp(c)}{1 + \exp(c)} (1 - \eta(\xv)) &=& 0 \\ 
%   % - \frac{\exp(-c)}{1 + \exp(-c)} \eta(\xv) + \frac{1}{1 + \exp(- c)} (1 - \eta(\xv)) &=& 0\\ 
%   % &=& -  \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{1}{1 + \exp(-c)} - \frac{1}{1 + \exp(-c)} p \\
%   - \frac{\exp(-c) \eta(\xv) - 1 + \eta(\xv)}{1 + \exp(-c)} &=& 0 \\
%   - \eta(\xv) + \frac{1}{1 + \exp(-c)} &=& 0\\
%   % \eta(\xv) &=& \frac{1}{1 + \exp(-c)} \\
%    c &=& \log\left(\frac{\eta(\xv)}{1 - \eta(\xv)}\right)
%   \end{eqnarray*}
% \end{footnotesize}

\end{vbframe}



\begin{vbframe}{Empirical optimal constant models}


\begin{itemizeL}
\item Optimal constant probability model for labels $\Yspace = \setzo$ is 
$$\thetah = \argmin_{\theta} \risket = \frac{1}{n} \sumin \yi$$
\item Fraction of class-1 observations in observed data
%\item Derived by setting derivative of risk to $0$ and solving for $\theta$
\item Optimal constant score model:
$$\thetah = \argmin_{\theta} \risket = \log \frac{\np}{\nn} = \log \frac{\np / n}{\nn /n}$$ 
$\nn$ and $\np$ are nr.  of neg. and pos. observations
\item Again shows connection to log-odds 
\end{itemizeL}

\end{vbframe}

%% BB: we dont need summary slides
% \begin{vbframe}{Naming Conventions}

% \begin{itemize}
    
% \item We have seen several closely related loss functions: 
% \begin{alignat*}{3} \Lxy    &= \log(1+\exp(-y\fx)) &&\quad \text{for } y \in \setmp \\ \Lxy    &= - y \cdot \fx + \log(1 + \exp(\fx)) &&\quad \text{for } y \in \setzo \\ \Lpixy  &= - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right) &&\quad \text{for } y \in \setzo \\ 
% \Lpixy  &= - \frac{1 + y}{2} \log\left(\pix\right) - \frac{1 - y}{2} \log\left(1 - \pix\right) &&\quad \text{for } y \in \setmp \end{alignat*}

% \item Names that are used interchangeably: Bernoulli-, Binomial-, logistic-, log-, or cross-entropy loss
% \end{itemize}

% \end{vbframe}


\begin{vbframe}{Optimization Properties: Convergence}

\begin{footnotesize}
%The choice of the loss function may also impact convergence behavior.

\begin{itemize}

\item In case of \textbf{complete separation}, optimization might 
fail

\begin{minipage}{0.6\textwidth}
\item Loss strictly decreasing in margin
$y \cdot \fx$: 
$$\Lxy = \log \left( 1 + \exp \left( - y  \fx \right)\right)$$
\item $f$ linear in $\thetav$, e.g.,
\textbf{log. regr.} with $\fxt = \thx$

\end{minipage}
\begin{minipage}{0.30\textwidth}
  \includegraphics[width=\textwidth]{figure/bernoulli.png}
\end{minipage}%

\item Assume data separable, so we can find $\thetav$:
$$ \yi \fxit = \yi \thetav^T \xi > 0 ~~ \forall \xi$$

\item Can now construct a strictly better $\thetav$

$$    
\riske(2 \cdot \thetav) = \sumin L ( 2 \yi \thetav^T \xi) < \risket
$$

\item As ||$\thetav$|| increases, sum strictly decreases, as argument of L is strictly larger

\item Loss is bounded from below, but no global optimium, cannot converge
\end{itemize} 
\end{footnotesize}


\framebreak

\begin{itemizeL}
\item
Geometrically, this translates to an ever steeper slope of the 
logistic/softmax function, i.e., increasingly sharp discrimination:

\vspace{0.3cm}
\begin{minipage}[b]{0.5\textwidth}
\includegraphics[width=0.8\textwidth]{figure/softmax_1}
\end{minipage}%
\begin{minipage}[b]{0.5\textwidth}
\includegraphics[width=0.8\textwidth]{figure/softmax_2}
\end{minipage}%
\item In practice, data are rarely linearly separable and misclassified 
examples act as counterweights to increasing parameter values
\item Can also use \textbf{regularization} for  robust solutions
\end{itemizeL}

\end{vbframe}


\endlecture

\end{document}