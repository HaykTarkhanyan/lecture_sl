\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{
  	Risk Minimization Basics
  }{
  figure_man/optimization_steps.jpeg
  }{
  \item Risk minimization and ERM recap
  \item Bayes optimal model, Bayes risk
  \item Bayes regret, estimation and approximation error    
  \item Optimal constant model
  \item Consistency
}

\begin{vbframe}{Empirical Risk Minimization}
To learn a model, we usually do ERM:
\vspace{-0.3cm}
$$\riskef = \sumin \Lxyi$$
\vspace{-0.2cm}
{\small
\begin{itemize}\setlength\itemsep{0.5pt} 
    \item observations $\left(\xi, \yi\right) \in  \Xspace \times \Yspace$
%    so from feature and target space
    \item model $f_{\Hspace}:\Xspace \rightarrow \R^g$, 
    %$\fx$ is a model 
    from hypothesis space $\Hspace$;
    maps a feature vector to output score;
    often we omit $\Hspace$ in index 
    \item loss $L:\Yspace\times\R^g\rightarrow\R$ ,
    %$\Lxy$ 
    measures error between label and prediction
    \item data generating process (DGP) $\Pxy$, we assume %$(\xv,y) \sim \Pxy$ and 
    $\left(\xi, \yi\right)  \overset{\text{i.i.d.}}{\sim} \Pxy$    
    %\item $f_{\Hspace}^{\ast}$ is the minimizer of the theoretical risk $\riskf$ over $\Hspace$ and $\hat{f}_{\Hspace}$ the minimizer of $\riskef$
\end{itemize}
}
Minimizing theoretical risk, so expected loss over DGP, is major goal:

$$ \risk \left(f\right) := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy $$
\end{vbframe}

\begin{vbframe}{Two short examples}
\textbf{Regression with linear model:}\\
\begin{itemize}
    \item Model: $f(\xv) = \thetav^\top \xv + \theta_0$
    \item Squared loss:  
    $\Lxy = \left(y-\fx\right)^2$
    \item Hypothesis space: $$\Hspace_{\text{lin}} = \left\{ \xv \mapsto \thetav^\top \xv + \theta_0 : \thetav \in \R^d, \theta_0 \in \R \right\}$$
\end{itemize}

\vspace{0.3cm}

\textbf{Binary classification with shallow MLP:}\\
\begin{itemize}
    \item Model: $f(\xv) = \pi(\xv)= \sigma(\bm{w}_2^{\top} \text{ReLU}(\bm{W}_1 \xv + \bm{b}_1) + b_2)$
    \item Bernoulli / Log / Cross-Entropy loss: $\Lpixy = -(y\log(\pix)+(1-y)\log(1-\pix))$\\ 
    \item Hypothesis space: {\small $$\Hspace_{\text{MLP}} = \left\{ \xv \mapsto \sigma(\bm{w}_2^{\top} \text{ReLU}(\bm{W}_1 \xv + \bm{b}_1) + b_2): \mathbf{W}_1 \in \R^{h \times d}, \mathbf{b}_1 \in \R^h, \mathbf{w}_2 \in \R^h, b_2 \in \R \right\}$$}
\end{itemize}
  
\end{vbframe}

\begin{vbframe}{Hypothesis Spaces and Parametrization}
We often write $\riskf$, but finding an optimal $f$ is operationalized as finding optimal $\thetav \in \Theta$ among a family of parametrized curves:

$$\Hspace = \{f_{\thetav}: f_{\thetav}\,\text{from functional family parametrized by}\,\thetav\}$$

\vfill


\begin{itemizeL}
    \item Optimizing numeric vectors is more convenient than functions
\item For some model classes, some parameters encode the same function (non-injective mapping, non-identifiability). \\
We don't care here, now. 
\end{itemizeL}

\end{vbframe}


\begin{vbframe}{Optimal loss values -- M-Estimators}

\begin{itemize}
\item Assume some RV $z \sim Q, Z \in \Yspace$ as target
\item $z$ not the same as $y$, as we want to fiddle with its distribution
\item We now consider $\argmin_c \E_{z \sim Q}[L(z, c)]$\\
What is the constant that approximates $z$ with minimal loss?

\end{itemize}

\lz

3 cases for Q
\begin{itemize}
\item $Q = P_y$, distribution of labels y, marginal of $\Pxy$
\item $Q = P_n$, the empirical product distribution for data $y^{(1)}, \ldots, y^{(n)}$
\item $Q = P_{y | x = \tilde{x}}$, conditional label distribution at point $x = \tilde{x}$
\end{itemize}

\lz

%Solving $\argmin_c \E_{z \sim Q}[L(z, c)]$ for any $Q$ provides 
%multiple results.


\end{vbframe}


\begin{vbframe}{Optimal unconditional values}

\begin{itemizeL}

\item Associating such a 
$$c = \argmin_c \E_{z \sim Q}[L(z, c)]$$
with a distribution, is also called a statistical functional

\item This specific loss-minimizing value is called an M-estimator

\item If we look at the empirical counterpart, with the empirical distribution, this is the so-called ``plug-in'' estimator

$$\argminlim_{c \in \R} \sumin L(\yi,c)$$

\end{itemizeL}

\end{vbframe}


\begin{vbframe}{Optimal Constant Model}

\begin{itemize}
\item Goal: loss optimal, constant baseline predictor
\item ``constant'': featureless ML model, always predicts same value
\item ``baseline'': more complex model has to be better 
%Can use it as baseline in experiments, if we don't beat this
%with more complex model, that model is useless
\item Also useful as optimal intercept


$$f_{c}^{\ast} = \argmin_{c \in \R} \E_{xy} \left[L(y,c)\right] = \argmin_{c \in \R} \E_{y} \left[L(y,c)\right]$$

\item Estimation via ERM: $\hat{f}_c = \argminlim_{c \in \R} \sumin L(\yi,c)$

\end{itemize}

\begin{center}
	\includegraphics[width = 0.32\textwidth]{figure/l1_vs_l2.png}
\end{center}
\end{vbframe}






\begin{vbframe}{Risk Minimizer}

\begin{itemizeM}

\item Assume, hypothesis space $\Hspace=\Hspace_{all}$ is unrestricted;\\
contains any measurable $f: \Xspace \to \R^g$

\item We know $\Pxy$

\item $f$ with minimal risk across $\Hspace_{all}$
is called\\
\textbf{risk minimizer}, \textbf{population minimizer} or \textbf{Bayes optimal model}

\begin{eqnarray*}
	\fbayes_{\Hspace_{all}} &=& \argmin_{f \in \Hspace_{all}} \risk\left(f\right) = \argmin_{f \in \Hspace_{all}}\Exy\left[\Lxy\right]\\ &=&  \argmin_{f \in \Hspace_{all}}\int \Lxy \text{d}\Pxy
\end{eqnarray*}

\item The resulting risk is called \textbf{Bayes risk}:  $\riskbayes_{} = \risk(\fbayes_{\Hspace_{all}})$

\item \textbf{Risk minimizer within} $\Hspace \subset \Hspace_{all}$ is
$\fbayes_{\Hspace} = \argmin_{f \in \Hspace} \riskf$

\end{itemizeM}

\end{vbframe}



% \begin{frame}[t]{optimal point-wise predictions}  

% To derive the risk minimizer we usually make use of the following trick: 

% \begin{itemize}
% 	\item We can choose $\fx$ as we want (unrestricted hypothesis space, no assumed functional form)
% 	\item Consequently, for a fixed value $\xv \in \Xspace$ we can select \textbf{any} value $c$ we want to predict 
%         % (we are not restricted by any functional form, e.g., a linear function)
% 	% \item Instead of looking for the optimal $f$ in function space (which is impossible), we compute the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
% 	\item So we construct the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
% \end{itemize}

% \lz
% \begin{center}
% \includegraphics[width=1\textwidth]{figure/optimal_pointwise.png}
% \end{center}


% \end{frame}

\begin{frame}[t]{optimal point-wise predictions}  

\begin{itemize}

\item To derive the RM, by law of total expectation 
$$    \riskf = \E_{xy} \left[\Lxy\right] 
    = \E_x \left[\E_{y|x}\left[\Lxy~|~\xv\right]\right]$$

	\item We can choose $\fx$ as we want from $\Hspace_{all}$ %(unrestricted hypothesis space, no assumed functional form)
	\item Hence, for fixed feature vector $\xtil$ we can select \textbf{any} value $c$ to predict. So we construct the \textbf{point-wise optimizer} 
        % (we are not restricted by any functional form, e.g., a linear function)
	% \item Instead of looking for the optimal $f$ in function space (which is impossible), we compute the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
 $$\fbayes(\xtil) = \argmin_c \E_{y|x}\left[L(y, c)~|~ \xv = \xtil \right] $$
%        \item Under L2 loss, best point-wise prediction is simply cond. mean

%$f^{\ast}(\xtil) = \E_{y|x} \left[y ~|~ \xv = \xtil \right]$.
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{figure/optimal_pointwise.png}
\end{center}


\end{frame}

\begin{vbframe}{Theoretical and Empirical Risk}  
 

\begin{itemize}
\item Bayes risk minimizer is mainly a theoretical tool
\item In practice, need to restrict $\Hspace$ for efficient search %so we efficiently search over it. 
\item We don't normally know $\Pxy$. Instead, use ERM. 
$$
{\hat f}_{\Hspace} = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi
$$
\item Due to \textbf{law of large numbers}, empirical risk for fixed model converges to true risk, so consistent estimator
$$
\riskeb(f) = \frac{1}{n} \sumin \Lxyi \overset{n \to \infty}{\longrightarrow} \riskf 
$$
\item Still, that does not imply that the selected ERM minimizer converges to $f^{\ast}$, due to overfitting or lack of uniform convergence. 
\item Would need more assumptions / math. machinery for this, \\
will not pursue this here. 

\end{itemize}




\end{vbframe}



\begin{vbframe}{Estimation and Approximation Error} 

\begin{itemizeL}
    \item Goal: Train model $\hat f_{\Hspace}$ with risk $\risk\left(\hat f_{\Hspace}\right)$ close to Bayes risk $\riskbayes$ \\
    \item Minimize \textbf{Bayes regret} or \textbf{excess risk}
$$
	\risk\left(\hat f_{\Hspace}\right) - \riskbayes
$$ 

\item Decompose: 

\begin{eqnarray*}
	\risk\left(\hat f_{\Hspace}\right) - \riskbayes &=& \underbrace{\left[\risk\left(\hat f_{\Hspace}\right) - \inf_{f \in \Hspace} \risk(f)\right]}_{\text{estimation error}} + \underbrace{\left[\inf_{f \in \Hspace} \risk(f) - \riskbayes\right]}_{\text{approximation error}} \\
    &=& \left[\risk(\hat{f}_{\Hspace}) - \risk(\fbayes_{\Hspace})\right] + \left[\risk(\fbayes_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}}) \right] 
\end{eqnarray*}

\end{itemizeL}

\framebreak 

% https://docs.google.com/presentation/d/1vXuX8P6p7TFlXnpVcInQM6PR3qHk9LqC2Z75_I4u4Lo/edit#slide=id.p

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/risk_minimization_diagram.png}
\end{center}



$$
	\risk\left(\hat f_{\Hspace}\right) - \riskbayes = \underbrace{\left[\risk\left(\hat f_{\Hspace}\right) - \inf_{f \in \Hspace} \risk(f)\right]}_{\text{estimation error}} + \underbrace{\left[\inf_{f \in \Hspace} \risk(f) - \riskbayes\right]}_{\text{approximation error}}  
$$

\vfill

\begin{itemize}
\item \textbf{Estimation error}:
We fit $\hat f_{\Hspace}$ via ERM on finite data, \\
so we don't find best $f \in \Hspace$
\item \textbf{Approximation error}: 
$\Hspace$ will often not contain Bayes optimal $\fbayes$ 
\end{itemize}

\end{vbframe}


\begin{vbframe}{(Universally) Consistent Learners \citelink{STNE1977CONS}}
% https://mlweb.loria.fr/book/en/consistency.html

\textbf{Consistency} is an asymptotic property of a learning algorithm, which ensures the algorithm returns \textbf{the correct model} when given \textbf{unlimited data}.

\lz 

Let $\ind:\mathbb{D} \to \Hspace$ be a learning algorithm that takes a training set $\Dtrain \sim \Pxy$ of size $n_\text{train}$ and estimates a model $\hat f: \Xspace \to \R^g$. 

\lz 

The learning method $\ind$ is said to be \textbf{consistent} w.r.t. a certain distribution $\Pxy$ if the risk of the estimated model $\hat f$ converges in probability ( \enquote{$\overset{p}{\longrightarrow}$}) to the Bayes risk $\riskbayes$ when $n_\text{train}$ goes to $\infty$: 

$$
	\risk\left(\ind\left(\Dtrain\right)\right) \overset{p}{\longrightarrow} \riskbayes \quad \text{for } n_\text{train} \to \infty
$$

\vfill




\framebreak 

Consistency is defined w.r.t. a particular distribution $\Pxy$.
But since we usually don't know $\Pxy$, consistency
does not offer much help to choose an algorithm for a specific task. 

\lz 

More interesting is the stronger concept of \textbf{universal consistency}: An algorithm is universally consistent if it is consistent for \textbf{any} distribution. 

\lz 

In Stone's famous consistency theorem (1977), the universal consistency of a weighted average estimator such as KNN was proven. Many other ML models have since then been proven to be universally consistent (SVMs, ANNs, etc.).

%\lz

%\textbf{Note}: universal consistency is a desirable property - however, it does not tell us anything about convergence rates ...

\end{vbframe}



% \begin{frame}[t]{Risk Minimizer And Optimal Constant}

% Later, we will derive risk minimizers for various losses. 

% \begin{table}[] 
% \footnotesize
% \renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
%   \begin{tabular}{c|lll}
%   Name & Risk Minimizer & Optimal Constant\\ \hline
%   L2 & $\fxbayes = \E_{y|x} \left[y ~|~ \xv \right]$ & $\fxh = \frac{1}{n} \sumin \yi$ \\
%   L1 & $\fxbayes = \text{med}_{y|x} \left[y ~|~ \xv \right]$ & $\fxh = \text{med}(\yi)$\\
%   0-1 & $\hxbayes = \argmax_{l \in \Yspace} \P(y = l~|~ \xv)$  & $\hxh = \text{mode} \left\{\yi\right\}$ \\
%   Brier & $\pixbayes = \P(y = 1~|~\xv)$ & $\pixh = \frac{1}{n} \sumin \yi$\\
%   Bernoulli (on probs) & $\pixbayes = \P(y = 1~|~\xv)$ & $\pixh = \frac{1}{n} \sumin \yi$\\
%   Bernoulli (on scores) & $\fxbayes = \log\left(\frac{\P(y = 1 ~|~\xv)}{1 - \P(y = 1 ~|~\xv)}\right)$ & $\fxh = \log \frac{n_{+1}}{n_{-1}}$  
%   \end{tabular}
% \end{table}

% \only<1>{
% We see: For regression, the RMs model the conditional expectation and median of the underlying distribution. This makes intuitive sense, depending on your concept of how to best estimate central location / how robust this location should be.
% }
% \only<2>{
%     For the 0-1 loss, the risk minimizer constructs the \textbf{optimal Bayes decision rule}: We predict the class with maximal posterior probability.
% }
% \only<3>{
%     For Brier and Bernoulli, we predict the posterior probabilities (of the true DGP!). Losses that have this desirable property are called \textbf{proper scoring (rules)}.
% }

% \end{frame}





\endlecture

\end{document} 
