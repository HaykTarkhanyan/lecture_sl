\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{% Lecture title  
    Maximum Likelihood Estimation vs.
    Empirical Risk Minimization
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/residuals_plot_L2_title.png
  }{
  \item Connection between maximum likelihood and risk minimization
  \item Correspondence between Gaussian errors and L2 loss, Laplace errors and L1 loss, and Bernoulli targets and Bernoulli/log loss
  %\item Correspondence between Laplace errors and L1 loss
%\item Correspondence between Bernoulli targets and the Bernoulli / log loss 
}

\begin{vbframe}{Maximum Likelihood}

Regression from a maximum likelihood perspective. Assume: 

$$
	y~|~ \xv \sim p(y~|~\xv, \thetav)
$$

\vspace{0.5cm}

Common case: 
true underlying relationship $\ftrue$ with additive noise (surface plus noise model): 

\vspace{0.5cm}

\begin{minipage}{0.5\textwidth}
$$
y = \ftrue(\xv) + \eps
$$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/ftrue.pdf}
\end{minipage}

where $\ftrue$ has params $\thetav$ and $\eps$ a RV that follows some distribution $\P_\eps$, with $\E[\eps] = 0$. Also, assume $\epsilon \perp \!\!\! \perp \xv$.


\framebreak 

From a statistics / maximum-likelihood perspective, we assume (or we pretend) we know the underlying distribution family $p(y~|~\xv, \thetav)$:

\begin{itemize}
\item Given i.i.d data $
\D = \Dset
$
 from $\Pxy$ 

the maximum-likelihood principle is to maximize the \textbf{likelihood}

\begin{minipage}{0.5\textwidth}
$$ \LLt = \prod_{i=1}^n \pdfyigxit $$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/log_reg_ml.pdf}
\end{minipage}

or equivalently minimize the \textbf{negative log-likelihood (NLL)}

\begin{minipage}{0.5\textwidth}
$$ -\loglt = -\sumin \lpdfyigxit. $$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/log_reg_erm.pdf}
\end{minipage}
\end{itemize}


\framebreak 

From a machine learning perspective we assume our hypothesis space corresponds to the space of the (parameterized) $\ftrue$. 

\begin{itemize}
\item Simply define neg. log-likelihood as \textbf{loss function} 
$$ \Lxyt := - \lpdfygxt $$
\item Then, maximum-likelihood 
 = ERM
$$- \loglt = \risket = \sumin \Lxyit$$

\item NB: When we are only interested in the minimizer, we can ignore multiplicative or additive constants.
\item We use $\propto$ as \enquote{proportional up to positive multiplicative and additive constants}

\end{itemize}

\end{vbframe}


\begin{vbframe}{Gaussian Errors - L2-Loss} 

Assume $y = \ftrue(\xv) + \eps$ with additive Gaussian errors, i.e. $\epsi \sim \mathcal{N}(0, \sigma^2)$. Then $y~|~\xv \sim N\left(\ftrue(\xv), \sigma^2\right)$. The likelihood is then 

{\small
\begin{eqnarray*}
\LL(\thetav) &=& \prodin \pdf\left(\yi ~\bigg|~ \fxit, \sigma^2\right) \\ &\propto& \prodin \exp\left(-\frac{1}{2\sigma^2} \left(\yi - \fxit\right)^2\right)
\end{eqnarray*}
}

%\framebreak 

Minimizing Gaussian NLL is ERM with $L2$-loss:
{\small
\begin{eqnarray*}
- \loglt &=& - \log\left(\LL(\thetav)\right) \\
&\propto& - \log\big(\prodin \exp\big(-\frac{1}{2\sigma^2} \big(\yi - \fxit\big)^2\big)\big) \\
&\propto& \sumin \left(\yi - \fxit\right)^2
\end{eqnarray*}
}

\framebreak 

\begin{footnotesize}
\begin{itemize}
	\item We simulate data $y ~|~x \sim \mathcal{N}\left(\ftrue(x), 1\right)$ with $\ftrue = 0.2 \cdot x$ 
\item Let's plot residuals as histogram, after fitting our model with $L2$-loss (blue)
\item Q-Q-plot compares empirical residuals vs. theoretical quantiles of Gaussian 
\end{itemize}
\end{footnotesize}

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/residuals_plot_L2.pdf}
\end{center}

\begin{itemize}
    \item The residuals are approximately normally distributed 
\end{itemize}

\end{vbframe}

\begin{vbframe}{Laplace Errors - L1-Loss}

Let's consider Laplacian errors $\eps$ now, with density: 

\begin{minipage}{0.5\textwidth}
$$
 \frac{1}{2\sigma} \exp\left(-\frac{|\eps|}{\sigma}\right)\,, \sigma > 0.
$$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/laplace_plot.pdf}
\end{minipage}
Then
$$
y = \ftrue(\xv) + \eps 
$$

also follows Laplace distribution with mean $\fxit$ and scale  $\sigma$. 

\framebreak 

The likelihood is then 

\begin{eqnarray*}
\LL(\thetav) &=& \prod_{i=1}^n \pdf\left(\yi ~\bigg|~ \fxit, \sigma\right) \\ &\propto& \exp\left(-\frac{1}{\sigma}\sumin \left|\yi - \fxit\right|\right)\,
\end{eqnarray*}
The negative log-likelihood is
$$
- \loglt \propto \sumin \left|\yi - \fxit\right|
$$

MLE for Laplacian errors = ERM with L1-loss. 

\begin{itemize}
\item Some losses correspond to more complex or less known error densities, like the Huber loss \citelink{MEYER2021ALTERNATIVE}

\item Huber density is (unsurprisingly) a hybrid of Gaussian and Laplace

\end{itemize}

\framebreak 

\begin{footnotesize}
\begin{itemize}
	\item We simulate data $y ~|~x \sim \text{Laplacian}\left(\ftrue(x), 1\right)$ with $\ftrue = 0.2 \cdot x$
\item We can plot the empirical error distribution, i.e. the distribution of the residuals after fitting a regression model w.r.t. $L1$-loss
\item With the help of a Q-Q-plot we can compare the empirical residuals vs. the theoretical quantiles of a Laplacian distribution  
\end{itemize}
\end{footnotesize}
\includegraphics{figure/residuals_plot_L1.pdf}



\end{vbframe}



\begin{vbframe}{Maximum Likelihood in Classification}

Let us assume the outputs $y$ to be Bernoulli-distributed, i.e. $y ~|~ \xv \sim \text{Bern}(\pi_\text{true}(\xv))$. 
The negative log likelihood is
\begin{eqnarray*}
- \loglt &=& -\sumin \lpdfyigxit \\ 
&=& - \sumin \log \big[\pi(\xi)^{y^{(i)}} \cdot (1 - \pi(\xi))^{(1 - y^{(i)})} \big]\\
&=& \sumin -\yi \log[\pi(\xi)] - (1-\yi) \log [1 - \pi(\xi)]. 
\end{eqnarray*}


%\framebreak 

This gives rise to the following loss function 

$$
  L(y, \pix) = -y\log(\pix)-(1-y)\log(1-\pix), \quad y \in \{0, 1\}
$$

which we introduced as \textbf{Bernoulli} loss 



\end{vbframe}




\begin{vbframe}{Distributions and losses}
For \textbf{every} error distribution $\P_\eps$ we can derive an equivalent loss function, which leads to the same point estimator for the parameter vector $\thetav$ as maximum-likelihood. Formally, $\thetah \in \argmax_{\thetav} \LL(\thetav) \Leftrightarrow \thetah \in \argmin_{\thetav}-\log(\LL(\thetav))$.  %\implies \thetah \in \argmin_{\thetav}-\log(\LL(\thetav))$
    %\item $\thetah \in \argmax_{\thetav} \log\left(\LL(\thetav)\right) \implies $
    
\vspace{0.2cm}    
\textbf{But}: Other way does not always work: We cannot derive a pdf/error distrib. for every loss -- the Hinge loss is one prominent example (some prob. interpretation is still possible \citelink{SOLLICH1999NINTH})
%\framebreak

\vspace{0.2cm}
\textbf{When} does the reverse direction hold?

If we can write loss as $L(y,\fx)=L_{\mathbb{P}}(y-\fx)=L_{\mathbb{P}}(r)$ for $r \in \mathbb{R}$, then minimizing $L_{\mathbb{P}}(y-\fx)$ is equiv. to maximizing a conditional log-likelihood $\log(p(y-f(\xv|\thetav))$ if
\begin{enumerate}
 \setlength{\itemsep}{1.0em}
    \item $\log(p(r))$ is affine trafo of $L_{\mathbb{P}}$ (undoing the $\propto$):
% minus due to sign chance when we go from loss to loglik
    $\log(p(r)) = a - bL_{\mathbb{P}}(r),\,\,a \in \mathbb{R}, b>0$
    \item  $p$ is a pdf (non-negative and integrates to one)
\end{enumerate}

\end{vbframe}


\endlecture
\end{document}
