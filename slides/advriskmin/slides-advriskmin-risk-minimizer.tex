\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
  	Risk Minimizers
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/optimization_steps.jpeg
  }{
  \item Bayes optimal model (also: risk minimizer, population minimizer)
  \item Bayes risk
  \item Bayes regret, estimation and approximation error    
  \item Optimal constant model
  \item Consistent learners
}

\begin{vbframe}{Empirical Risk Minimization}
Very often, in ML, we minimize the empirical risk
\vspace{-0.3cm}
$$\riskef = \sumin \Lxyi$$
\vspace{-0.2cm}
{\small
\begin{itemize}\setlength\itemsep{0.5pt} 
    \item each observation $\left(\xi, \yi\right) \in  \Xspace \times \Yspace$,
    so from feature and target space
    \item $f_{\Hspace}:\Xspace \rightarrow \mathbb{R}^g$, 
    f is a model from hypothesis space $\Hspace$;
    maps a feature vector to output score;
    sometimes or often we omit $\Hspace$ in the index 
    \item $L:\left(\Yspace\times\mathbb{R}^g\right)\rightarrow\mathbb{R}$ is loss;\\
    $\Lyf$ measures distance between label and prediction
    \item We assume that $(\xv,y) \sim \Pxy$ and $\left(\xi, \yi\right)  \overset{\text{i.i.d.}}{\sim} \Pxy$ \\
    $\Pxy$ is the distribution of the data generating process (DGP)
    %\item $f_{\Hspace}^{\ast}$ is the minimizer of the theoretical risk $\riskf$ over $\Hspace$ and $\hat{f}_{\Hspace}$ the minimizer of $\riskef$
\end{itemize}
}
Let's define (and minimize) loss in expectation, the theoretical risk:

$$ \risk \left(f\right) := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy $$
\end{vbframe}

\begin{vbframe}{Two short examples}
\textbf{Regression with linear model:}\\
\begin{itemize}
    \item Model: $f(\xv) = \thetab^\top \xv + \theta_0$
    \item Squared loss:  
    $\Lyf = \left(y-f\right)^2$
    \item Hypothesis space: $$\Hspace_{\text{lin}} = \left\{ \xv \mapsto \thetab^\top \xv + \theta_0 : \thetab \in \mathbb{R}^d, \theta_0 \in \mathbb{R} \right\}$$
\end{itemize}

\vspace{0.3cm}

\textbf{Binary classification with shallow MLP:}\\
\begin{itemize}
    \item Model: $f(\xv) = \pi(\xv)= \sigma(\bm{w}_2^{\top} \text{ReLU}(\bm{W}_1 \xv + \bm{b}_1) + b_2)$
    \item Binary cross-entropy loss: $\Lpiv = -(y\log(\pi)+(1-y)\log(1-\pi))$\\ 
    \item Hypothesis space: {\small $$\Hspace_{\text{MLP}} = \left\{ \xv \mapsto \sigma(\bm{w}_2^{\top} \text{ReLU}(\bm{W}_1 \xv + \bm{b}_1) + b_2): \mathbf{W}_1 \in \mathbb{R}^{h \times d}, \mathbf{b}_1 \in \mathbb{R}^h, \mathbf{w}_2 \in \mathbb{R}^h, b_2 \in \mathbb{R} \right\}$$}
\end{itemize}
  
\end{vbframe}

\begin{vbframe}{Optimal constants for a loss}

\begin{itemize}
\item Let's assume some RV $z \in \Yspace$ for a label
\item z not RV $y$, because we want to fiddle with its distribution
\item Assume z has distribution Q, so $z \sim Q$
\item We can now consider $\argmin_c \E_{z \sim Q}[L(z, c)]$\\
so the score-constant which loss-minimally approximates z
\end{itemize}

\lz

We will consider 3 cases for Q
\begin{itemize}
\item $Q = P_y$, simply our labels and their marginal distribution in $\Pxy$
\item $Q = P_{y | x = x}$, conditional label distribution at point $x = x$
\item $Q = P_n$, the empirical product distribution for data $y_1, \ldots, y_n$
\end{itemize}

\lz

If we can solve $\argmin_c \E_{z \sim Q}[L(z, c)]$ for any $Q$,
we will get multiple useful results!


\end{vbframe}


\begin{vbframe}{Optimal Constant Model}

\begin{itemize}
{\footnotesize
\item We would like a loss optimal, constant baseline predictor
\item A "featureless" ML model, which always predicts the same value
\item Can use it as baseline in experiments, if we don't beat this
with more complex model, that model is useless
\item Will also be useful as component in algorithms and derivations
}
\end{itemize}

%By restricting $\Hspace$ to constant functions $f_c$ independent of $\xv$, the constant model minimizing the theoretical risk 


$$f_{c}^{\ast} = \argmin_{c \in \mathbb{R}} \E_{xy} \left[L(y,c)\right] = \argmin_{c \in \mathbb{R}} \E_{y} \left[L(y,c)\right]$$

{\footnotesize and $\fx = \theta = c$ that optimizes the empirical risk $\risket$ is denoted as as $\hat{f}_c = \argmin_{c \in \mathbb{R}} \risket$.}

\vspace*{-0.2cm}

\begin{center}
	\includegraphics[width = 0.38\textwidth]{figure/l1_vs_l2.png}
\end{center}
\end{vbframe}


\begin{vbframe}{Optimal Constant Model}

\begin{itemize}
\item Let's start with the simplest case, L2 loss
\item And we want to find and optimal constant model for 

\end{itemize}

$$ \argmin \E [L(z, c)] = $$
$$ \argmin \E [(z - c)^2] = $$
$$ \argmin \E [z^2] - 2cE[z] + c^2 = $$
$$ E[z] $$

\begin{itemize}
\item Using $Q = P_y$, this means that, given we know the label distribution,
the best constant is $c = E[y]$.
\item If we only have data $y_1, \ldots y_n$
$\argmin \E_{z \sim P_n} [(z - c)^2] = \E_{z \sim P_n}[z] = \frac{1}{n} \sumin \yi = \bar{y}$


\item And we want to find and optimal constant model for 

\end{itemize}


\end{vbframe}




\begin{vbframe}{Risk Minimizer}

%Our goal is to minimize the risk

%$$ \risk_L\left(f\right) := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy. $$

%for a certain hypothesis $f \in \Hspace$ and a loss $\Lyf$. 

%{\tiny NB: As $\risk_L$ depends on loss $L$, we sometimes 
%make this explicit with a subscript if needed and omit it in other cases.}

\lz 

Let us assume we are in an \enquote{ideal world}: 

\begin{itemize}
	\item The hypothesis space $\Hspace=\Hspace_{all}$ is unrestricted. We can choose any measurable $f: \Xspace \to \R^g$. 
	\item We also assume an ideal optimizer; the risk minimization can always be 
        solved perfectly and efficiently.
	\item We know $\Pxy$. 
\end{itemize}

How should $f$ be chosen? 


\framebreak 

The $f$ with minimal risk across all (measurable) functions 
is called the \textbf{risk minimizer}, \textbf{population minimizer} or \textbf{Bayes optimal model}. 

\begin{eqnarray*}
	\fbayes_{\Hspace_{all}} &=& \argmin_{f \in \Hspace_{all}} \risk\left(f\right) = \argmin_{f: \Xspace \to \R^g}\Exy\left[\Lxy\right]\\ &=&  \argmin_{f: \Xspace \to \R^g}\int \Lxy \text{d}\Pxy. 
\end{eqnarray*}

The resulting risk is called \textbf{Bayes risk}:  $\riskbayes_{} = \risk(\fbayes)$

\lz

Note that if we leave out the hypothesis space in the subscript it becomes clear from the context!\\

Similarly, we define the risk minimizer over some $\Hspace \subset \Hspace_{all}$ as

\begin{eqnarray*}
	\fbayes_{\Hspace} &=& \argmin_{f \in \Hspace} \risk\left(f\right)
\end{eqnarray*}


% Note that we search over an unrestricted hypothesis space (that is over all possible functions $f: \Xspace \to \R^g$)!



\end{vbframe}



% \begin{frame}[t]{optimal point-wise predictions}  

% To derive the risk minimizer we usually make use of the following trick: 

% \begin{itemize}
% 	\item We can choose $\fx$ as we want (unrestricted hypothesis space, no assumed functional form)
% 	\item Consequently, for a fixed value $\xv \in \Xspace$ we can select \textbf{any} value $c$ we want to predict 
%         % (we are not restricted by any functional form, e.g., a linear function)
% 	% \item Instead of looking for the optimal $f$ in function space (which is impossible), we compute the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
% 	\item So we construct the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
% \end{itemize}

% \lz
% \begin{center}
% \includegraphics[width=1\textwidth]{figure/optimal_pointwise.png}
% \end{center}


% \end{frame}

\begin{frame}[t]{optimal point-wise predictions}  

To derive the risk minimizer, observe that by law of total expectation 
$$    \risk(f) = \E_{xy} \left[\Lxy\right] 
    = \E_x \left[\E_{y|x}\left[\Lxy~|~\xv = \xv\right]\right].$$

\begin{itemize}
	\item We can choose $\fx$ as we want (unrestricted hypothesis space, no assumed functional form)
	\item Hence, for a fixed value $\xv \in \Xspace$ we can select \textbf{any} value $c$ we want to predict. So we construct the \textbf{point-wise optimizer} 
        % (we are not restricted by any functional form, e.g., a linear function)
	% \item Instead of looking for the optimal $f$ in function space (which is impossible), we compute the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
 $$\fxbayes = \mbox{argmin}_c \E_{y|x}\left[L(y, c)~|~ \xv = \xv \right] \quad \forall \xv \in \Xspace.$$
\end{itemize}

\begin{center}
\includegraphics[width=1\textwidth]{figure/optimal_pointwise.png}
\end{center}


\end{frame}

\begin{vbframe}{Theoretical and Empirical Risk}  
 

The risk minimizer is mainly a theoretical tool: 

\begin{itemize}
	\item In practice we need to restrict the hypothesis space $\Hspace$ such that we can efficiently search over it. 
	\item In practice we (usually) do not know $\Pxy$. Instead of $\riskf$, we are optimizing the empirical risk

	\begin{eqnarray*}
		{\hat f}_{\Hspace} = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi
	\end{eqnarray*}

\end{itemize}

Note that according to the \textbf{law of large numbers} (LLN), the empirical risk converges to the true risk (but beware of overfitting!): 

\begin{eqnarray*}
	\bar\risk_\text{emp}(f) = \frac{1}{n} \sumin \Lxyi \overset{n \to \infty}{\longrightarrow} \riskf. 
\end{eqnarray*}

\end{vbframe}



\begin{vbframe}{Estimation and Approximation Error} 

\textbf{Goal of learning: } Train a model $\hat f_{\Hspace}$ for which the true risk $\risk\left(\hat f_{\Hspace}\right)$ is close to the Bayes risk $\riskbayes$. In other words, we want the \textbf{Bayes regret} or \textbf{excess risk}


$$
	\risk\left(\hat f_{\Hspace}\right) - \riskbayes
$$ 

to be as low as possible. 

\lz 

The Bayes regret can be decomposed as follows: 

\begin{eqnarray*}
	\risk\left(\hat f_{\Hspace}\right) - \riskbayes &=& \underbrace{\left[\risk\left(\hat f_{\Hspace}\right) - \inf_{f \in \Hspace} \risk(f)\right]}_{\text{estimation error}} + \underbrace{\left[\inf_{f \in \Hspace} \risk(f) - \riskbayes\right]}_{\text{approximation error}} \\
    &=& \left[\risk(\hat{f}_{\Hspace}) - \risk(\fbayes_{\Hspace})\right] + \left[\risk(\fbayes_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}}) \right] 
\end{eqnarray*}

\framebreak 

% https://docs.google.com/presentation/d/1vXuX8P6p7TFlXnpVcInQM6PR3qHk9LqC2Z75_I4u4Lo/edit#slide=id.p

\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/risk_minimization_diagram.png}
\end{center}

\begin{itemize}
	\item $\risk\left(\hat f\right) - \inf_{f \in \Hspace} \risk(f)$ is the \textbf{estimation error}. We fit $\hat f$ via empirical risk minimization and (usually) use approximate optimization, so we usually do not find the optimal $f \in \Hspace$.
	\item $\inf_{f \in \Hspace} \risk(f) - \riskbayes$ is the \textbf{approximation error}. We need to restrict to a hypothesis space $\Hspace$ which might not even contain the Bayes optimal model $\fbayes$. 
\end{itemize}

\end{vbframe}


\begin{vbframe}{(Universally) Consistent Learners}
% https://mlweb.loria.fr/book/en/consistency.html

\textbf{Consistency} is an asymptotic property of a learning algorithm, which ensures the algorithm returns \textbf{the correct model} when given \textbf{unlimited data}.

\lz 

Let $\ind:\mathbb{D} \to \Hspace$ be a learning algorithm that takes a training set $\Dtrain \sim \Pxy$ of size $n_\text{train}$ and estimates a model $\hat f: \Xspace \to \R^g$. 

\lz 

The learning method $\ind$ is said to be \textbf{consistent} w.r.t. a certain distribution $\Pxy$ if the risk of the estimated model $\hat f$ converges in probability ( \enquote{$\overset{p}{\longrightarrow}$}) to the Bayes risk $\riskbayes$ when $n_\text{train}$ goes to $\infty$: 

$$
	\risk\left(\ind\left(\Dtrain\right)\right) \overset{p}{\longrightarrow} \riskbayes \quad \text{for } n_\text{train} \to \infty.
$$

\vfill




\framebreak 

Consistency is defined w.r.t. a particular distribution $\Pxy$.
But since we usually do not know $\Pxy$, consistency
does not offer much help to choose an algorithm for a particular task. 

\lz 

More interesting is the stronger concept of \textbf{universal consistency}: An algorithm is universally consistent if it is consistent for \textbf{any} distribution. 

\lz 

In Stone's famous consistency theorem from 1977, the universal consistency of a weighted average estimator as KNN was proven. Many other ML models have since then been proven to be universally consistent (SVMs, ANNs, etc.).

\lz

\textbf{Note} that universal consistency is obviously a desirable property - however, (universal) consistency does not tell us anything about convergence rates ...

\end{vbframe}



% \begin{frame}[t]{Risk Minimizer And Optimal Constant}

% Later, we will derive risk minimizers for various losses. 

% \begin{table}[] 
% \footnotesize
% \renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
%   \begin{tabular}{c|lll}
%   Name & Risk Minimizer & Optimal Constant\\ \hline
%   L2 & $\fxbayes = \E_{y|x} \left[y ~|~ \xv \right]$ & $\fxh = \frac{1}{n} \sumin \yi$ \\
%   L1 & $\fxbayes = \text{med}_{y|x} \left[y ~|~ \xv \right]$ & $\fxh = \text{med}(\yi)$\\
%   0-1 & $\hxbayes = \argmax_{l \in \Yspace} \P(y = l~|~ \xv)$  & $\hxh = \text{mode} \left\{\yi\right\}$ \\
%   Brier & $\pixbayes = \P(y = 1~|~\xv)$ & $\pixh = \frac{1}{n} \sumin \yi$\\
%   Bernoulli (on probs) & $\pixbayes = \P(y = 1~|~\xv)$ & $\pixh = \frac{1}{n} \sumin \yi$\\
%   Bernoulli (on scores) & $\fxbayes = \log\left(\frac{\P(y = 1 ~|~\xv)}{1 - \P(y = 1 ~|~\xv)}\right)$ & $\fxh = \log \frac{n_{+1}}{n_{-1}}$  
%   \end{tabular}
% \end{table}

% \only<1>{
% We see: For regression, the RMs model the conditional expectation and median of the underlying distribution. This makes intuitive sense, depending on your concept of how to best estimate central location / how robust this location should be.
% }
% \only<2>{
%     For the 0-1 loss, the risk minimizer constructs the \textbf{optimal Bayes decision rule}: We predict the class with maximal posterior probability.
% }
% \only<3>{
%     For Brier and Bernoulli, we predict the posterior probabilities (of the true DGP!). Losses that have this desirable property are called \textbf{proper scoring (rules)}.
% }

% \end{frame}





\endlecture

\end{document} 
