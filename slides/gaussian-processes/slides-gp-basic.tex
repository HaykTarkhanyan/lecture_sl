\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Gaussian Processes
}{
Stochastic Processes and Distributions on Functions
}{
figure_man/discrete/marginalization-more.png
}{
\item GPs model distributions over functions 
\item The marginalization property makes this distribution easily tractable
\item GPs are fully specified by mean and covariance function 
\item GPs are indexed families
}

\begin{framei}[sep=L]{Weight-Space View}
\item Until now: hypothesis space $\Hspace$ of parameterized functions $\fxt$ % (in particular, the space of linear functions)
\item ERM: find risk-minimal parameters (weights) $\thetav$
\item Bayesian paradigm: distribution over $\thetav$ $\Rightarrow$ update prior to posterior belief after observing data acc to Bayes' rule
$$
p(\thetav | \Xmat, \yv) 
= \frac{\text{likelihood} \cdot \text{prior}}{\text{marginal likelihood}} 
= \frac{p(\yv | \Xmat, \thetav) \cdot q(\thetav)}{p(\yv|\Xmat)}
$$
\end{framei}

\begin{framei}[sep=L]{Function-Space View}
\item New POV: rather than finding $\thetav$ to parameterize $\fxt$ in parameter space,  search space of admissible functions directly
\item Sticking to Bayesian inference, specify prior distribution \textbf{over functions} and update according to observed data points
\end{framei}

\begin{framei}{drawing from function priors}
\item Imagine we could draw functions from some prior distribution

\vfill

\imageC[.8]{figure/gp_sample/1_1.pdf}
\end{framei}

\foreach \i in{1,2,3} {
\begin{framei}{drawing from function priors}
\addtocounter{page}{0}
\item After observing some data points, we restrict sampling to functions consistent with the data
\vfill
\imageC[.8]{figure/gp_sample/2_\i.pdf}
\end{framei}
}

\begin{framei}{drawing from function priors}
\item Variety of admissible functions shrinks with observing more data
\vfill
\imageC[.8]{figure/gp_sample/2_4.pdf}
\item Intuitively: distributions over functions have ``mean'' \& ``variance''
\end{framei}

\begin{frame}{Weight-space vs. Function-space View}
\splitVTT{
\textbf{Weight-Space View}
\spacer
\begin{itemizeL}
\item Parameterize functions \\ (e.g., $\fxt = \thx$)
\item Define distributions on $\thetav$
\item Inference in param space $\Theta$
\end{itemizeL}
}{
\textbf{Function-Space View}
\spacer
\begin{itemizeL}
\item Work on functions directly \phantom{(e.g., $\fxt = \thx$)}
\item Define distributions on $f$
\item Inference in fun space $\Hspace$
\end{itemizeL}
}
\end{frame}

% --------------------------------------------------

% \section{Distributions on Functions}

\begin{framei}[sep=L]{discrete functions}
\item Let $\Xspace = \xdat$ be a finite set
\item Let $\Hspace = \{h ~|~ h: \Xspace \rightarrow \R\}$
\item Any $h \in \Hspace$ has finite domain with $n < \infty$ elements $\Rightarrow$ neat representation with $n$-dim vector 
$$\bm{h} = [h(\xi[1]), \dots, h(\xi[n] ) ]$$
\end{framei}

\foreach \i [count=\idx from 1] in {2, 5, 10} {
\begin{framei}[sep=L]{example: random discrete functions}
\item Case $\idx$: finite input space with cardinality $|\Xspace| = \i$
\item Example functions living in this space
\vfill
\splitVThree{
\imageC{figure/discrete/example_\i_1.pdf}
}{
\imageC{figure/discrete/example_\i_2.pdf}
}{
\imageC{figure/discrete/example_\i_3.pdf}
}
\end{framei}
}

\begin{framei}{Distributions on Discrete Functions}
\item Specify (discrete) probability function on functions $h \in \Hspace$ $\Rightarrow$ natural way: vector representation
$$\bm{h} = [h(\xi[1]), \dots, h(\xi[n] ) ]$$
\item $\bm{h}$ as $n$-dim RV, e.g.,
$$\bm{h} \sim \gaussmk$$
\item For now: set $\mv = \zero$, assume $\Kmat$ to be given
\end{framei}

\foreach \i [count=\idx from 1] in {2, 5, 10} {
\foreach \j in {1, 2, 3} {
\begin{framei}{example: random discrete functions}
\item Case $\idx$ (ctd): $h: \Xspace \to \Yspace$ defined on $\i$ points
\item Sample representatives by sampling from a $\i$-dim Gaussian
\ifnum \i=2
$$\bm{h} = [h(\xi[1]), h(\xi[\i])] \sim \normal(\zero, \Kmat)$$
\else 
$$\bm{h} = [h(\xi[1]), \dots, h(\xi[\i])] \sim \normal(\zero, \Kmat)$$
\fi
\vfill
\splitV{
\imageC[.9]{figure/discrete/example_norm_\i_\j_a.pdf}
}{
\imageC[.9]{figure/discrete/example_norm_\i_\j_b.pdf}
}
\end{framei}
}
}

\begin{framei}{Role of the Covariance Function}
\item Note: covariance controls ``shape'' of function drawn
\item Extreme case a) almost perfectly correlated 
\footnotesize
$$\Kmat = \begin{pmatrix} 1 & 0.99 & \dots & 0.99 \\
0.99 & 1 & \dots & 0.99 \\
0.99 & 0.99 & \ddots & 0.99 \\
0.99 & \dots & 0.99 & 1 \end{pmatrix}$$
\normalsize
\item Extreme case b) uncorrelated $\Rightarrow$ $\Kmat = \id$
\vfill
\splitVCompact{.35}{.35}{
\imageC{figure/discrete/example_extreme_50_1.pdf}
}{
\imageC{figure/discrete/example_extreme_50_2.pdf}
}
\end{framei}

\begin{framei}[sep=L]{spatial correlation}
\item ``Meaningful'' functions (on numeric spaces) often characterized by a spatial property:
% $\xi, \xi[j]$ close in $\Xspace$ $\Rightarrow$ $f(\xi), f(\xi[j])$ should be close in $\Yspace$
$$\xi, \xi[j] \text{ close in } \Xspace \Rightarrow f(\xi), f(\xi[j]) \text{ close in } \Yspace$$
\item In other words: if points are close in $\Xspace$, their function values should be correlated
\item Enforce this by choosing covariance function s.t.
$$\Kmat_{ij} \text{ high } \Leftrightarrow \xi[i], \xi[j] \text{ close in } \Xspace$$
\end{framei} 

\begin{framei}[sep=L]{spatial correlation}
\item Compute covariance matrix by a function based on distance between $\xi$ and $\xi[j]$
\item E.g., $\Kmat_{ij} = k(\xi, \xi[j]) = \exp \left(- \tfrac{1}{2} \| \xi - \xi[j] \|^2 \right)$
\vfill
\splitVCompact{.35}{.35}{
\imageC{figure/discrete/example_extreme_50_3.pdf}
}{
\imageC{figure/discrete/example_extreme_50_4.pdf}
}
\item More on covariance function, or \textbf{kernel}, $k(\cdot, \cdot)$ later on
\end{framei} 

\begin{framei}{From Discrete to Continuous Functions}
\item Recall: we defined distributions on functions by the (Gaussian) random vector of corresponding function values 
$$\bm{h} = [h(\xi[1]), \dots, h(\xi[n])] \sim \gaussmk$$
\item We can do this for ever larger $n$ (as ``granular'' as we want)
\vfill
\imageC{figure/discrete/example_limit.pdf}
\end{framei}

\begin{framei}[sep = L]{From Discrete to Continuous Functions}
\item No matter how large $n$ is: still functions over discrete domains
\item Can we simply extend our distribution def to \textbf{continuous}-domain functions by taking $n \rightarrow \infty$?
\item Unclear how to obtain ``infinitely'' long (Gaussian) random vectors
\item Observation: random vectors $\bm{h}$ are collections of RVs enumerated by index set $\nset$ $\Rightarrow$ \textbf{indexed family} 
\item Can we use more general, infinite index sets? % to create such collections? $\Rightarrow$ \textbf{stochastic processes}
\end{framei}

\begin{framei}[sep=L]{definition: indexed family}
\item Mathematical ``rule'' to map indices to some state space %$\mathcal{S}$
$$s: T \rightarrow \mathcal{S}, \quad t \mapsto s_t = s(t) $$
\item Gives rise to collection $\{s_t: t \in T\}$
\item Can assume many forms depending on $T$ and $\mathcal{S}$
\item $T$: in particular, finite vs infinite
% \vfill
% \begin{tabular}{|l|l|l|}
% \hline
% \textbf{Cardinality of $T$} & \textbf{$\mathcal{S}$: (real) numbers} & \textbf{$\mathcal{S}$: RVs} \\
% \hline
% Finite & list & random vector \\
% \hline
% Countably $\infty$ & sequence & discrete-time \textbf{stochastic process} \\
% \hline
% Uncountably $\infty$ & sequence & continuous-time \textbf{stochastic process} \\
% \hline
% \end{tabular}
% \vfill
\item $\mathcal{S}$: numbers, RVs, sets, \dots
\end{framei}

\begin{frame}{example: real-valued indexed families}
\splitV{
\begin{itemizeL}
\item $\mathcal{S} = \R$
\item $t \mapsto s_t$
\item Finite index set, e.g., $T = \nset$ $\Rightarrow$ list
\item Countably $\infty$ index set, e.g., $T = \N$ $\Rightarrow$ sequence 
\end{itemizeL}
}{
\imageR{figure_man/indexed_family/indexed_family_1.png}
\imageR{figure_man/indexed_family/indexed_family_2.png}
}
\vfill
\begin{itemize}
\item Uncountably $\infty$ index set, e.g., $T = \R$ $\Rightarrow$ \\example: $\{\sin(t): t \in \R\}$
\end{itemize}
\end{frame}

\begin{framei}[sep=L]{definition: stochastic process}
\item Special kind of indexed family: RVs with specific covariance structure
$$\{Y_t: t \in T\}$$ 
\item Some distributional assumption for $Y_t$
\item Often temporal interpretation of $T$ $\Rightarrow$ e.g., stock prices
\item Intuition: probability distributions describe random vectors, SP describe random functions
\end{framei}

\begin{frame}{example: stochastic processes}
\splitV{
\begin{itemizeL}
\item $\mathcal{S}$: space of RVs
\item $t \mapsto Y_t$
\item Finite index set, e.g., $T = \nset$ \\$\Rightarrow$ random vector
\item Countably $\infty$ index set, e.g., $T = \N$ $\Rightarrow$ discrete-time SP
\end{itemizeL}
}{
\imageR{figure_man/indexed_family/indexed_family_4.png}
\imageR{figure_man/indexed_family/indexed_family_3.png}
}
\vfill
\begin{itemize}
\item Uncountably $\infty$ index set, e.g., $T = \R$ $\Rightarrow$ continuous-time SP
\end{itemize}
\end{frame}

\begin{framei}[sep=L]{intuition: gaussian processes}
\item \textbf{Gaussian process}: special case of SP 
\item Index set: $\Xspace$, cov structure: Gaussian 
\item Corollary: any finite random vector drawn from a GP is multivariate Gaussian $\Rightarrow$ \textbf{marginalization property}
\vfill
\splitV{
\imageC{figure_man/indexed_family/indexed_family_5.png}
}{
\imageC[.8]{figure_man/indexed_family/indexed_family_6.png}
}
\vfill
\item Precisely what we were looking for: process to generate arbitrarily long Gaussian random vectors!
\end{framei}

\foreach \i in {5, 10, 50}{
\begin{framei}[sep=L]{marginalization property}
\item Distinguishing feature of GPs: for any finite set of inputs,
    $$
      \fv = \fvec \sim \gaussmk
    $$ 
\item $\mv$ and $\Kmat$ are calculated by a mean and cov function, resp
\vfill
\splitV{
\imageC[.9]{figure/discrete/example_marginalization_\i.pdf}
}{
\ifnum \i=5
\imageC[1]{figure_man/discrete/marginalization-5.png}
\else
\imageC[1]{figure_man/discrete/marginalization-more.png}
\fi
}
\end{framei}
}

\begin{framei}[sep=L]{definition: gaussian process}
\item Formally, 
$$\fx \sim \gp$$
iff for any finite set of inputs $\xdat$, 
$$
\fv = \fvec \sim \gaussmk
$$
where
$$
\mv := (m(\xi))_{i \in \nset}, \quad
\Kmat := \left(k(\xi, \xi[j]) \right)_{i,j \in \nset}
$$
with \textbf{mean function} $m(\cdot)$ and \textbf{covariance function} $k(\cdot, \cdot)$
\end{framei}

\begin{framei}[sep=L]{existence theorem}
% \item Idea: for any particular finite-dim distributions, the corresponding GP exists
\item For \textbf{any} 
\begin{itemize}
\item state space $\Xspace$,
\item mean function $m: \Xspace \rightarrow \R$,
\item covariance function $k: \Xspace \times \Xspace \rightarrow \R$, 
\end{itemize}
there \textbf{exists} a Gaussian process $\mathcal{GP}(m(\cdot), k(\cdot, \cdot))$ s.t.
\begin{eqnarray*}
\E(\fx) &=& m(\xv) \\
\cov(\fx, f(\xv^\prime)) &=& \kxxp
\end{eqnarray*}
$\forall \xv, \xv^\prime \in \Xspace$
\item Version of Kolmogorov consistency theorem  \\
$\Rightarrow$ proof: \citelink{GRIMMETT2001PROC} (Thm. 8.6.3)
\item Intuition: for any mean \& cov structure we prescribe for random vectors $\fv$, there is a corresponding GP to generate them
\end{framei}

\begin{framei}[sep=L]{definition: gaussian process}
\item GPs are thus completely specified by their mean \& cov function
\begin{eqnarray*}
m(\xv) &=& \E[f(\xv)] \\
k(\xv, \xv^\prime) &=& \cov(\fx, f(\xv^\prime)) = \E \left[\left( f(\xv) - \E[f(\xv)] \right) \left( f(\xv^\prime) - \E[f(\xv^\prime)] \right) \right]
\end{eqnarray*}
\item For now, we consider zero-mean GPs with $m(\xv) \equiv \zero$ 

$\Rightarrow$ common, not necessarily drastic assumption 
\item Properties of GPs mainly governed by $k(\cdot, \cdot)$
\item By virtue of existence thm: sampling from GPs gives us random functions with our properties of choice
\end{framei}

\begin{framei}[sep=L]{sampling from gaussian processes}
\item Example: $\fx \sim \mathcal{GP}(\zero, \kxxp)$ with cov function
$$ \kxxp = \exp\left(-\tfrac{1}{2}\|\xv - \xv^\prime\|^2\right)$$
\item To visualize sample functions, 
\begin{itemize}
\item choose high number $n$ of (equidistant) points $\xdat$
  \item compute $\Kmat = \left(k(\xi, \xi[j]) \right)_{i,j \in \nset}$ from all pairs $\xi, \xi[j]$ 
  \item draw $\bm{f} \sim \mathcal{N}(\bm{0}, \bm{K})$ 
\end{itemize}
\item 10 randomly drawn functions (note 0 mean)
\vfill
\imageC[.9]{figure/gp_sample/different_samples.pdf}
\end{framei}

\endlecture
\end{document}
