\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/l1_l2_hat.png}
\newcommand{\learninggoals}{
  \item Know the geometry of ridge vs. lasso regularization 
  \item Understand the effects of the methods on model coefficients
  \item Understand that lasso creates sparse solutions
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}
 
\lecturechapter{Regularization: Lasso vs. Ridge}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Lasso vs. ridge Geometry}
$$ 
  \min_{\thetab} \sumin \left(\yi - \fxit\right)^2 \qquad \text{ s.t. } \|\thetab\|_p^p  \leq t 
$$ 
  \vspace{-0.5cm}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{figure_man/l1_l2_hat.png}}
  \end{figure}

  \begin{itemize}
    \item \small{In both cases, the solution which minimizes $\riskrt$ is always a point on the boundary of the feasible region (for sufficiently large $\lambda$).
    \item As expected, $\hat{\thetab}_{\text{lasso}}$ and $\hat{\thetab}_{\text{ridge}}$ have smaller parameter norms than $\thetah$.}
    \item For lasso, the solution likely touches vertices of the constraint region. This induces sparsity and is a form of variable selection.
    \item In the $p>n$ case, lasso selects at most $n$ features \citebutton{Zou and Hastie, 2005}{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x}.
    
  \end{itemize}
  
\end{vbframe}

\begin{vbframe}{Coefficient Paths and 0-Shrinkage}

\textbf{Example 1: Motor Trend Car Roads Test (mtcars)} \\

%We cannot overfit here with an unregularized linear model as the task is so low-dimensional. But 
We see how only lasso shrinks to exactly 0.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/l1_l2_regupaths_mse.pdf}\\
\end{figure}
\vspace{-0.3cm}
Coef paths and cross-val. MSE for $\lambda$ values for ridge and lasso.

\framebreak
\textbf{Example 2: High-dimensional simulated data} \\
We simulate a continuous, correlated dataset with 50 features, 100 observations $\xv^{(1)},\dots, \xv^{(100)} \overset{\text{i.i.d.}}{\sim} \normal \left(\mathbf{0}, \Sigma \right)$ and 
$$ y = 10 \cdot (x_1 + x_2) + 5 \cdot (x_3 + x_4) + 1 \cdot \sum_{j = 5}^{14} x_j + \epsilon $$
where $\epsilon \sim \normal \left(0, 1\right)$ and $ \forall k, l \in \{1, ..., 50\}$: 
$$Cov(x_k, x_l) = 
  \begin{cases}
   0.7^{|k-l|} & \text{for } k \neq l \\
   1 & else 
   \end{cases}. 
$$ 
Note that 36 of the 50 features are noise variables. \\
\framebreak 
Coefficient histograms for different $\lambda$ values for ridge and lasso for simulated data along with the cross-validated MSE.

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/shrinkage_2.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Regularization and Feature Scaling}

  \begin{itemize}
    \item Typically we omit $\theta_0$ in the penalty term $J(\thetab)$ so that the ``infinitely'' regularized model is the constant model (but this can be implementation-dependent).
    \item Note that unregularized LM has inductive bias of \textbf{rescaling equivariance}, i.e., if you scale some features, we can simply "anti-scale" the coefs and the risk does not change.
    \item Penalty methods typically not equivariant under rescaling of the inputs, so one usually standardizes the features beforehand.  
    \item While regularized LMs exhibit low-complexity inductive bias, they lose equivariance property: if you down-scale features, coefficients have to become larger to counteract. Then they are penalized stronger in $J(\thetab)$, making some features less attractive without relevant changes in data.
      
    % \item While ridge regression usually leads to smaller estimated coefficients, but still dense $\thetab$ vectors,
    %   the lasso will usually create a sparse $\thetab$ vector and can therefore be used for variable selection.
    %\item SVMs combine (usually) hinge loss with L2-regularization. But also for SMVs this concept is generalized to different losses and different penalties.
  \end{itemize}

\framebreak

%\textbf{Example:}\\

\footnotesize{
\begin{itemize}
    \item Let the DGP be $y = \sum_{j=1}^{5} \theta_j x_{j} +\varepsilon$ for $\thetab=(1,2,3,4,5)^\top$, $\varepsilon \sim \mathcal{N}(0,1)$ %and $n=100$
    \item Suppose $x_5$ was measured in $m$ but we change the unit to $cm$ ($\Tilde{x}_5=100 \cdot x_5$):
\end{itemize}
\vspace{-0.4cm}
\begin{table}[h]
\centering
\begin{tabular}{|c|c c c c c c|}
\hline
\textbf{Method} & \( \hat{\theta}_1 \) & \( \hat{\theta}_2 \) & \( \hat{\theta}_3 \) & \( \hat{\theta}_4 \) & \( \hat{\theta}_5 \) & MSE \\ \hline
OLS             & 0.9835872 & 2.147303 & 3.005854 & 3.917807 & \textbf{5.20491245} & 0.8124301 \\ %\hline
OLS Rescaled    & 0.9835872 & 2.147303 & 3.005854 & 3.917807 & \textbf{0.05204912} & 0.8124301 \\ \hline
\end{tabular}
%\caption{Equivariant OLS estimates under rescaling of $x_5$}
\end{table}
\vspace{-0.1cm}
\begin{itemize}
    \item Estimate $\hat{\theta}_5$ gets scaled by $1/100$ while other estimates and MSE are invariant
    \item Running ridge regression with $\lambda=10$ on same data shows that rescaling of of $x_5$ does not result in inverse rescaling of $\hat{\theta}_5$ (everything changes!)
    \item This is because $\hat{\theta}_5$ now lives on small scale while $L2$ constraint stays the same. Hence remaining estimates can ``afford'' larger magnitudes.
\end{itemize}
\vspace{-0.4cm}
\begin{table}[h]
\centering
\begin{tabular}{|c|c c c c c c|}
\hline
\textbf{Method} & \( \hat{\theta}_1 \) & \( \hat{\theta}_2 \) & \( \hat{\theta}_3 \) & \( \hat{\theta}_4 \) & \( \hat{\theta}_5 \) & MSE \\ \hline
ridge           & 0.7093407 & 1.873643 & 2.661345 & 3.557891 & \textbf{4.63642392} & 1.3664731 \\ %\hline
ridge Rescaled  & 0.8021802 & 1.942568 & 2.675207 & 3.569190 & \textbf{0.05134698} & 1.0796400 \\ \hline
\end{tabular}
%\caption{ridge estimates for $\lambda=10$ under rescaling of $x_5$}
\end{table}
}

\begin{itemize}
    \item This also implies that for very correlated features in lasso through a unit change we could arbitrarily force a feature out of the model
\end{itemize}

\framebreak

\begin{comment}
\textbf{Example:}
\begin{itemize}
\item Let the true data generating process be
$$ y = x_1 + \epsilon, \quad \epsilon \sim \normal \left(0, 1\right).$$
\item Let there be 5 features $x_1, \ldots, x_5 \sim \normal (0,1)$.
\item Using the lasso (package \texttt{glmnet}), we get
\footnotesize
\vspace{0.2cm}

\begin{table}[]
\begin{tabular}{lllll}
(Intercept) & x1    & x2    & x3    & x4    \\
-0.056      & 0.489 & 0.000 & 0.000 & 0.000 
\end{tabular}
\end{table}


\normalsize
\item But if we rescale any of the noise features, say $x_2 = 10000 \cdot x_2$ and don't use standardization, we get
\footnotesize
\vspace{0.2cm}

\begin{table}[]
\begin{tabular}{lllll}
(Intercept) & x1       & x2        & x3       & x4       \\
-0.106830   & 0.000000 & -0.000013 & 0.000000 & 0.000000         
\end{tabular}
\end{table}

\normalsize

\item This is due to the fact, that the coefficient of $x_2$ will live on a very small scale as the covariate itself is large. The feature will thus get less penalized by the $L1$-norm and is favored by lasso.
\end{itemize}
\end{comment}

\end{vbframe}

\begin{vbframe}{Correlated Features}

% \textbf{Example:} lasso vs ridge for two highly correlated variables ($X_4, X_5$). 

\includegraphics[width=0.9\textwidth]{figure/regu_example_multicollinearity.png}

Consider $n=100$ simulated observations using $y = 0.2X_1 + 0.2X_2 + 0.2X_3 + 0.2X_4 + 0.2X_5 + \epsilon$.\\
$X_1$-$X_4$ are independent, but $X_4$ and $X_5$ are strongly correlated.

\vspace{0.1cm}

We see that lasso shrinks the coefficient for $X_5$ to zero early on, while ridge assigns similar coefficients to $X_4, X_5$ for larger $\lambda$.

\end{vbframe}


\begin{vbframe}{Synopsis}

\begin{itemize}
\item Neither one can be classified as overall better
\item lasso can set some coefficients to zero, thus performing variable selection, while ridge regression usually leads to smaller estimated coefficients, but still dense parameter vectors $\thetab$.
\item Lasso is likely better if true underlying structure is sparse, so if only few features influence $y$. Ridge works well if there are many (weakly) influential features.
\item Lasso has difficulties handling correlated predictors. For high correlation ridge dominates lasso in performance.
\item For lasso one of the correlated predictors will have a larger coefficient, while the rest are (nearly) zeroed. The respective feature is, however, selected randomly. 
\item For ridge the coefficients of correlated features are similar.
\item For references, see \citebutton{Tibshirani, 1996}{https://www.jstor.org/stable/2346178} and \citebutton{Zou and Hastie, 2005}{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x}
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
