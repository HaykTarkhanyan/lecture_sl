\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/lasso_ridge_enet_2d.png}
\newcommand{\learninggoals}{
  \item Know the elastic net as compromise between ridge and lasso regression
  \item Know regularized logistic regression
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Regularization: Elastic Net and GLMs}
\lecture{Introduction to Machine Learning}



% \section{Elastic Net}

\begin{vbframe} {Elastic Net \citebutton{Zou and Hastie, 2005}{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x}}


Elastic Net \citebutton{Zou and Hastie, 2005}{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x} combines the $L1$ and $L2$ penalties:
\small{
\begin{align*}
\mathcal{R}_{\text{elnet}}(\thetab) &=  \sumin (\yi - \thetab^\top \xi)^2 + \lambda_1 \|\thetab\|_1 + \lambda_2 \|\thetab\|_2^2 \\
&= \sumin (\yi - \thetab^\top \xi)^2 + \lambda \left( (1-\alpha) \|\thetab\|_1 + \alpha \|\thetab\|_2^2\right),\, \alpha=\frac{\lambda_2}{\lambda_1+\lambda_2}, \lambda=\lambda_1+\lambda_2
\end{align*}}
\begin{figure}
\includegraphics[width=0.55\textwidth]{figure/lasso_ridge_enet_2d.png}\\
\end{figure}
\vspace{-0.2cm}
\begin{itemize}
\item Correlated features tend to be either selected or zeroed out together.
\item Selection of more than $n$ features possible for $p>n$.
\end{itemize}


\framebreak
\footnotesize
Simulating 50 data sets with 100 observations each for two coefficient settings: \\
\vspace{-0.3cm}
$$\yv =\Xmat \boldsymbol{\theta}+ \epsilon, \quad \epsilon \sim N(0,1)$$
\vspace{-0.3cm}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
{\footnotesize \textbf{Ridge} performs better for correlated features}: \\ 
$\boldsymbol{\theta}=(\underbrace{2,\ldots,2}_{5},\underbrace{0,\ldots,0}_{5})$\\
$ \operatorname{corr}(\Xmat_{i},\Xmat_{j})=0.8^{|i-j|}$ for all $i$ and $j$
  \end{center}
\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
{\footnotesize \textbf{Lasso} performs better for sparse truth/no correlation:} \\
$\boldsymbol{\theta}=(2, 2, 2,\underbrace{0,\ldots,0}_{7})$ \\
$\operatorname{corr}(\Xmat_{i},\Xmat_{j})= 0$ for all $i \neq j$, otherwise 1
\end{center}
\end{column}
\end{columns}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/enet_lasso_ridge_mse.png}\\
\end{figure}
{\normalsize $\implies$ Elastic Net handles both cases well}
\framebreak

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/enet_tradeoff.png}\\
\end{figure}


\footnotesize
LHS: ridge cannot perform variable selection compared to lasso/E-Net. \\
Lasso more frequently ignores relevant features than E-Net (longer tails in violin plot).\\
RHS: ridge estimates of noise features hover around $0$ while lasso/E-Net produce $0$s.
%Since Elastic Net offers a compromise between Ridge and lasso, it is suitable for both data situations.

\end{vbframe}


% \section{Regularized Logistic Regression}

\begin{vbframe}{Regularized Logistic Regression}

Regularizers can be added very flexibly to basically any model which is based on ERM.

\lz 

Hence, we can construct, e.g., $L1$- or $L2$-penalized logistic regression to enable coefficient shrinkage and variable selection in this model class. 

% \lz 
% We can add a regularizer to the risk of logistic regression

\begin{align*}
\riskrt &= \risket + \lambda \cdot J(\thetab) \\
%&= \sumin \mathsf{log} \left[1 + \exp \left(-\yi f\left(\left.\xi~\right|~ \thetab\right)\right)\right] + \lambda \cdot J(\thetab) \\
&= \sumin \mathsf{log}\left[1 + \mathsf{exp}\left(-2\yi f\left(\left.\xi~\right|~ \thetab\right)\right)\right] + \lambda \cdot J(\thetab)
\end{align*}

% The other parts of the logistic regression remain exactly the same,
% except for the fitting algorithm to find \(\hat{\thetab}_{\text{reg}}\) (no closed-form solution, numerical optimization methods are necessary).

\end{vbframe}

\begin{vbframe}{Regularized Logistic Regression}


We fit a logistic regression model using polynomial features for \(x_1\)
and \(x_2\) with maximum degree of \(7\). We add an $L2$ penalty. We
see for

\begin{itemize}

\item
  \(\lambda = 0\): The unregularized model seems to overfit.
\item
  \(\lambda = 0.0001\): Regularization helps to learn the underlying
  mechanism.
\item
  \(\lambda = 1\): The real data-generating process is captured very well.
\end{itemize}

\scriptsize

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/reg_logreg.png}\\
\end{figure}


\normalsize 

\end{vbframe}


\endlecture
\end{document}
