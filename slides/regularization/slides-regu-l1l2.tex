\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/solution-path-ridge-only.png}
\newcommand{\learninggoals}{
  \item Know the regularized linear model
  \item Know Ridge regression ($L2$ penalty)
  \item Know Lasso regression ($L1$ penalty)
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Lasso and Ridge Regression}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Regularization in the linear model}

  \begin{itemize} \setlength{\itemsep}{1.3em}
  \item Linear models can also overfit if we operate in a high-dimensional space with not that many observations.    
  \item The OLS estimator requires a full-rank design matrix.
  \item For highly correlated features, OLS becomes highly sensitive to random errors in the observed response, producing a large variance in the fit. 
  \item We now add a complexity penalty to the loss:
  $$
  \riskrt = \sumin \left(\yi - \thetab^\top \xi \right)^2 + \lambda \cdot J(\thetab). 
  $$ 
  \end{itemize}

\end{vbframe}

\begin{vbframe}{Example: Ridge Regression}
Assume the data generating process $y=3x_{1} -2x_{2} +\epsilon $, where $\displaystyle \epsilon \sim N( 0,1)$. The true minimizer is given by $\theta ^{*} =( 3,-2)^{T}$.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/lin_reg_l2.png}
\end{figure}

With increasing regularization, $\theta_{\textit{reg}}$ is pulled back to the origin.

\end{vbframe}


% \section{Ridge Regression}

\begin{vbframe}{Ridge Regression}
Intuitive measure of model complexity is deviation from 0-origin, as 0-model contains no effects. %Models close to this either have few active features or only weak effects. 
So we measure $J(\thetab)$ through a vector norm, shrinking coefs closer 0 (\textbf{shrinkage methods}).\\
\vspace{0.2cm}
\textbf{Ridge regression} uses a simple $L2$ penalty:
\begin{eqnarray*}  
\thetah_{\text{Ridge}} &=& \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \lambda \|\thetab\|_2^2 \\
&=& \argmin_{\thetab} \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right) + \lambda \thetab^\top \thetab
\end{eqnarray*}

Optimization is possible (as in the normal LM) in analytical form:
$$\thetah_{\text{Ridge}} = ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv$$

Name comes from the fact that we add positive entries along the diagonal "ridge" $\Xmat^T \Xmat$

\framebreak 

We understand the geometry of these 2 mixed components in our regularized risk objective much better, if we formulate the optimization as a constrained problem (see this as Lagrange multipliers in reverse).

\vspace{-0.5cm}

\begin{eqnarray*}
\min_{\thetab} && \sumin \left(\yi - \fxit\right)^2 \\
  \text{s.t. } && \|\thetab\|_2^2  \leq t \\
\end{eqnarray*}

\vspace{-1.0cm}

\begin{figure}
\includegraphics[width=0.3\textwidth]{figure_man/ridge_hat.png}
\end{figure}

\begin{footnotesize} 
NB: Relationship between $\lambda$ and $t$ will be explained later.
\end{footnotesize}

\framebreak
  
\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure_man/solution-path-ridge-only.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\begin{itemize}
  \item We still optimize the $\risket$, but cannot leave a ball around the origin.
  \item $\risket$ grows monotonically if we move away from $\thetah$ (elliptic contours).
  \item Inside constraints perspective: From origin, jump from contour line to contour line (better) until you become infeasible, stop before.
\item Outside constraints perspective: From $\thetah$, jump from contour line to contour line (worse) until you become feasible, stop then.
  \item So our new optimum will lie on the boundary of that ball.
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}


\end{vbframe}



\begin{vbframe}{Example: Polynomial Ridge Regression}

Consider $y=f(x)+\epsilon$ where the true (unknown) function is \(f(x) = 5 + 2x +10x^2 - 2x^3\) (in red).

\lz

We now fit the data using a \(d\)th-order polynomial
\[ f(x) = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]
Using model complexity $d = 10$ overfits:

\begin{center}
\includegraphics[width = 10cm ]{figure/poly_ridge_1.png} \\
\end{center}

\framebreak

With an $L2$ penalty we can now select $d$ "too large" but regularize our model by shrinking its coefficients. Otherwise we have to optimize over the discrete $d$.

\vfill

\begin{center}
\includegraphics[width = 11cm ]{figure/poly_ridge_2.png} \\
\end{center}


\begin{center}
\tiny
\begin{tabular}{ c| c c c c c c c c c c c c}
 $\lambda$ & $\theta_0$ & $\theta_1$ & $\theta_2$ & $\theta_3$ & $\theta_4$ & $\theta_5$ & $\theta_6$ & $\theta_7$ & $\theta_8$ & $\theta_9$ & $\theta_{10}$ \\ 
 \hline
 0.00 & 12.00 & -16.00 & 4.80 & 23.00 & -5.40 & -9.30 & 4.20 & 0.53 & -0.63 & 0.13 & -0.01 \\  
 10.00 & 5.20 &1.30 & 3.70 & 0.69 & 1.90 & -2.00 & 0.47 & 0.20 & -0.14 & 0.03 & -0.00 \\ 
 100.00 & 1.70 & 0.46 & 1.80 & 0.25 & 1.80 & -0.94 & 0.34 & -0.01 & -0.06 & 0.02 & -0.00
\end{tabular}
\end{center}


\end{vbframe}

% \section{Lasso Regression}

\begin{vbframe}{Lasso Regression}

Another shrinkage method is the so-called \textbf{Lasso regression} ({\scriptsize{least absolute shrinkage and selection operator}}), which uses an $L1$ penalty on $\thetab$:
\vspace{-0.2cm}
\begin{eqnarray*}
\thetah_{\text{Lasso}}= \argmin_{\thetab} \underbrace{\sumin \left(\yi - \thetab^T \xi\right)^2}_{\left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right)} + \lambda \|\thetab\|_1
\end{eqnarray*}
Optimization is much harder now. $\riskrt$ is still convex, but in general there is no analytical solution and it is non-differentiable.\\
\vspace{0.2cm}

For special case of orthonormal design $\Xmat^{\top}\Xmat=\id$ we can get closed-form solution in terms of $\thetah_{\text{OLS}}=(\Xmat^{\top}\Xmat)^{-1}\Xmat^{\top}\yv=\Xmat^{\top}\yv$:
$$\thetah_{\text{Lasso}}=\text{sign}(\thetah_{\text{OLS}})(\vert \thetah_{\text{OLS}} \vert - \lambda)_{+}\quad(\text{sparsity})$$

Comparing this to $\thetah_{\text{Ridge}}$ we see qualitatively different behavior as $\lambda \uparrow$:
$$\thetah_{\text{Ridge}}=\frac{\thetah_{\text{OLS}}}{1+\lambda}\quad (\text{no sparsity, uniform downscaling})$$


%\textbf{NB}: lasso=least absolute shrinkage and selection operator.

\framebreak 

We can also rewrite this as a constrained optimization problem. The penalty results in the constrained region to look like a diamond shape.
\vspace{-0.2cm}
\begin{eqnarray*}
\min_{\thetab} \sumin \left(\yi - \fxit\right)^2\,
\text{subject to: } \|\thetab\|_1 \leq t
\end{eqnarray*}
The kinks in $L1$ enforce sparse solutions because ``the loss contours first hit the sharp corners of the constraint'' at coordinate axes where (some) entries are zero. 
\vspace{-0.1cm}
\begin{figure}%\includegraphics[width=0.3\textwidth]{figure_man/lasso_hat.png}\\
\includegraphics[width=0.95\textwidth]{figure/lasso_contours_cases.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Comparing Solution paths for $L1$/$L2$}
\begin{itemize}
    \item Ridge regression results in a smooth solution path with non-sparse parameters
    \item Lasso regression induces sparsity, but only for large enough $\lambda$
\end{itemize}
 \lz
\begin{figure}
\includegraphics[width=0.9\textwidth]{figure_man/solution-path-ridge-lasso.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Effect of $L1$/$L2$ on Loss Surface}
Regularized empirical risk $\riskr(\theta_1,\theta_2)$ using squared loss for $\lambda \uparrow$. $L1$ penalty makes non-smooth kinks at coordinate axes more pronounced, while $L2$ penalty warps $\riskr$ toward a ``basin'' (elliptic paraboloid). 
 
\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/reg_surfaces_l1_l2.png}\\
\end{figure}

\end{vbframe}


\endlecture
\end{document}

