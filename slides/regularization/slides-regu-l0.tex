\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/lasso_ridge_hat.png}
\newcommand{\learninggoals}{
  \item Know Lq (quasi-)norm regularization
  \item Understand that L0 regularization simply counts the number of non-zero parameters
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{L0 Regularization}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{LQ norm Regularization} 

Besides $L1$ and $L2$ norm we could use any $Lq$ norm for regularization.

\begin{figure}
  \scalebox{0.7}{\includegraphics{figure_man/lasso_ridge_hat.png}}\\
%\includegraphics[height=2.3cm]{figure_man/contour.pdf}
\caption{\textit{Top:} Ridge and Lasso loss contours and feasible regions.
  \textit{Bottom:} Different feasible region shapes for $Lq$ norms $\sum_j |\theta_j|^q$.}
\end{figure}
  
\end{vbframe}


\begin{vbframe} {L0 regularization}

  \begin{itemize}
    \item Consider the $L0$-regularized risk of a model $\fxt$
  $$
  \riskrt = \risket + \lambda \|\thetab\|_0 := \risket + \lambda \sum_j |\theta_j|^0.
  $$
      \item Unlike the $L1$ and $L2$ norms, the $L0$ "norm" simply counts the number of non-zero parameters in the model.
    \begin{figure}
      \centering
        \scalebox{0.8}{\includegraphics{figure_man/l0_norm.png}}
        \tiny{\\ Credit: Christos Louizos}
        \caption{\footnotesize $Lp$ norm penalties for a parameter $\thetab$ according to different values of $p$.}
    \end{figure}
    \item For any parameter $\thetab$, the $L0$ penalty is zero for $\thetab = 0$ (defining $0^0 := 0$) and is constant for any $\thetab \neq 0$, no matter how large or small it is.
    \item $L0$ regularization induces sparsity in the parameter vector more aggressively than $L1$ regularization, but does not shrink concrete parameter values as L1 and L2 does.
    \item Model selection criteria such as Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are special cases of $L0$ regularization (corresponding to specific values of $\lambda$).
    \item The $L0$-regularized risk is neither continuous, differentiable or convex. 
    \item It is computationally hard to optimize (NP-hard) and likely intractable. 
      For smaller $n$ and $p$ we might be able to solve this nowadays directly, for larger scenarios efficient approximations of the $L0$ are still topic of current research.
  \end{itemize}
\end{vbframe}

\endlecture
\end{document}
