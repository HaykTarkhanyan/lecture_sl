\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/bias-variance-ridge.png}
\newcommand{\learninggoals}{
  \item Understand why overfitting happens
  \item Know how overfitting can be avoided
  \item Know regularized empirical risk minimization 
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Introduction to Regularization}
\lecture{Introduction to Machine Learning}

%\section{Motivation for Regularization}

\begin{vbframe}{What is Regularization?}

Regularization comprises all methods that add preferences for specific solutions (\textbf{inductive bias}) to a model, usually in the context of ``low complexity'' priors (shrinkage and sparsity). By controlling complexity we can reduce overfitting and achieve an optimal bias-variance tradeoff.
\vspace{0.1cm}
\begin{itemize}
\setlength{\itemsep}{1.0em}
    \item \textbf{Explicit regularization} methods define an explicit measure of model complexity and add this as penalty to empirical risk (e.g., $L1/L2$)
    \item \textbf{Implicit regularization} includes removing outliers, early stopping, data augmentation, parameter sharing, dropout or ensembling
    \item \textbf{Structured regularization} methods incorporate structural prior knowledge over groups of parameters or subnetworks (e.g., the group lasso \citebutton{Yuan and Lin, 2005}{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x})
\end{itemize}

\end{vbframe}

\begin{vbframe}{Regularization for Invariance}
Prior knowledge can also be of the form that predictions should remain invariant under certain input transformations.\\
In image classification, label ``cat'' should hold regardless of position or size of relevant object (translation/scale invariance)
\begin{enumerate}\setlength\itemsep{1.02em}
    \item \textbf{Pre-processing}: By computing invariant features under transformations, downstream models too will respect invariances
    %\item \textbf{Explicit regularization}: Penalty for changes in model output under transformed inputs is added to loss
    \item \textbf{Data augmentation}: Extend training data by replicating inputs under invariant transformations (e.g., flipping/rotating images)
    \begin{figure}
    \includegraphics[width=0.75\textwidth]{figure_man/data-augmentation-cat.png}\\
    \end{figure}
    \item \textbf{Network architecture}: Build invariance property directly into network structure, e.g. CNNs \citebutton{Geometric DL (Bronstein et al., 2021)}{https://arxiv.org/pdf/2104.13478.pdf}
\end{enumerate}


\end{vbframe}

\begin{vbframe}{Recap: Overfitting}

Reducing overfitting is an important application of regularization, so let's first motivate it from that perspective.

\begin{itemize}
  \item Overfitting occurs when the model reflects noise or artifacts in training data which do not generalize (small train error, at cost of test high error)
  \item Hence, predictions of overfitting models cannot be trusted to generalize beyond the training data
\end{itemize}
\lz
\begin{columns}
\begin{column}{0.5\textwidth}
  \raggedright
  Overfitted model\\
  \includegraphics[width=0.85\textwidth]{figure/eval_ofit_1o}
\end{column}
\begin{column}{0.5\textwidth}
  \raggedright
    Appropriate model\\
  \includegraphics[width=0.85\textwidth]{figure/eval_ofit_1a}
\end{column}
\end{columns}
    
\end{vbframe}

\begin{vbframe}{Example I: Overfitting}

\begin{itemize}
\item Assume we want to predict the daily maximum \textbf{ozone level} in LA given a data set containing $50$ observations.
\item The data set contains $12$ features describing time conditions (e.g., weekday, month),
the weather (e.g., temperature at different weather stations, humidity, wind speed) or geographic variables (e.g., the pressure gradient).
\item We fit a linear regression model using \textbf{all} of the features

$$
\fxt = \thetab^T\xv = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_{12} x_{12}
$$

with the $L2$ loss.

\item We evaluate the performance with $10$ times $10$-fold CV.

\end{itemize}

\vfill

\begin{footnotesize} 
We use (a subset of) the \texttt{Ozone} data set from the \texttt{mlbench} package. This way, we artificially create a \enquote{high-dimensional} dataset by reducing the number of observations drastically while keeping the number of features fixed. 
\end{footnotesize}

\framebreak 


While our model fits the training data almost perfectly (left), it generalizes poorly
to new test data (right). We overfitted.

\lz 

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/ozone_mse_boxplot.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Example II: Overfitting}

We train a shallow neural network with one hidden layer and 100 hidden units as well as a SVM with RBF kernel (and $C=1e6, \gamma=10$) on a small regression task. No form of explicit regularization is imposed on the models. %The target variable is house price.
\vspace{0.2cm}
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & Neural Network & SVM \\ 
  \hline
Training MSE & 0.00 & 24.88 \\ 
  Test MSE & 0.72 & 86.15 \\ 
   \hline
\end{tabular}
\end{table}
\vspace{0.3cm}
\begin{itemize}
    \item Both neural network and SVM perform significantly better on the training set
    \item The shallow NN even achieves zero training error (interpolating)
    \item Test error is significantly higher for both models, indicating overfitting
\end{itemize}

\end{vbframe}

\begin{vbframe}{Avoid Overfitting} 

Why can \textbf{overfitting} happen in practice? And how to avoid it?
\lz
\lz
\begin{enumerate}
\item Not enough data \\
$\to$ collect \textbf{more data} 
\item Data is noisy \\
$\to$ collect \textbf{better data} (reduce noise) 
\item Models are too complex \\
$\to$ use \textbf{less complex models}
\item Aggressive loss optimization \\
$\to$ \textbf{optimize less}
\end{enumerate}


\framebreak 

\textbf{Approach 1: Collect more data}

\lz 

We explore our results for increased dataset size by $10$ times $10$-fold CV.
The fit worsens slightly, but the test error decreases.

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/avoid_overfitting_01.png}\\
\end{figure}

Good insight, but getting more data is often not feasible in practice.

\framebreak

\textbf{Approach 3: Reduce complexity}

\lz 

We try the simplest model we can think of: the constant model. For the $L2$ loss, the optimal constant model is the empirical mean

$$
\fxt = \frac{1}{n}\sumin \yi
$$

We then increase the complexity of the model step-by-step by adding one feature at a time.

\framebreak 

We can control the complexity of the model by including/excluding features.
We can try out all feature combinations and investigate the model fit.


\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/avoid_overfitting_02.png}\\
\end{figure}

\vfill

\begin{footnotesize}
Note: For simplicity, we added the features in one specific (clever) order, so we cheated a bit. Also note there are $2^{12} = 4096$ potential feature combinations.
\end{footnotesize}

\framebreak

\textbf{Approach 4: Optimize less}

\lz 

Now we use polynomial regression with temperature as the only feature to predict the ozone level, i.e.,

$$\fxt = \sum^{d}_{i=0} \theta_i (x_T)^{i} .$$
We choose $d = 15$, for which we get a very flexible model, which can be prone to overfitting for small data sets. \\
\medskip
In this example, we don't solve for $\hat\theta$ directly, but instead, we use the gradient descent algorithm to find $\hat\theta$ stepwise.

\framebreak

We want to stop the optimization early when the generalization error starts to degrade.


\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/early_stopping.png}\\
\end{figure}

\footnotesize{Note: For polynomial regression, gradient descent usually needs many iterations before it starts to overfit. Hence a very small training set was chosen to accelerate this effect.}

\framebreak 

We have contradictory goals

\begin{itemize}
\item \textbf{maximizing the fit} (minimizing the train loss)
\item \textbf{minimizing the complexity} of the model.
\end{itemize}

We need to find the \enquote{sweet spot}.

\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/complexity-vs-fit.png}
\end{figure}
\end{center}

\framebreak 

Until now, we can either include or exclude features in a binary fashsion.

\lz 

Instead of controlling the complexity in a discrete way by specifying the number of features,
we might prefer to control the complexity  \textbf{on a continuum} from simple to complex.

\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/complexity-vs-fit-continuous.png}
\end{figure}
\end{center}

\end{vbframe}

%\section{Regularized Empirical Risk Minimization}

\begin{vbframe}{Regularized Empirical Risk Minimization}

  
Recall, empirical risk minimization with a complex hypothesis set tends to overfit. A major tool for handling overfitting is \textbf{regularization}.
  
  \lz
  
In the broadest sense, regularization refers to any modification made to a learning algorithm that is intended to reduce its generalization error but not its training error.
  
  \lz
  
Explicitly or implicitly, such modifications represent the preferences we have regarding the elements of the hypothesis set. 

  \framebreak
  
  Commonly, regularization takes the following form:
  
  $$
  \riskrf = \riskef + \lambda \cdot J(f) = \sumin \Lxyi + \lambda \cdot J(f)
  $$
\begin{itemize}

  \item $J(f)$ is called \textbf{complexity penalty}, \textbf{roughness penalty} or \textbf{regularizer}.
  \item $\lambda > 0$ is called \textbf{complexity control} parameter. 
  \item It measures the \enquote{complexity} of a model and penalizes it in the fit.
  \item As for $\riske$, often $\riskr$ and $J$ are defined on $\thetab$ instead of $f$, so $\riskrt = \risket + \lambda \cdot J(\thetab)$. 
\end{itemize}

\framebreak

\textbf{Remarks:}

\begin{itemize}
  \item Note that we now face an optimization problem with two criteria: 
    \begin{enumerate}
      \item models should fit well (low empirical risk),
      \item but not be too complex (low $J(f)$). 
    \end{enumerate}
  \item We decide to combine the two in a weighted sum and to control
  the trade-off via the complexity control parameter $\lambda$.
  \item $\lambda$ is hard to set manually and is usually selected via cross-validation (see later).
  \item $\lambda = 0$: The regularized risk $\riskrf$ reduces to the simple empirical $\riskef$.

  \item If $\lambda$ goes to infinity, we stop caring about the loss/fit and models become as \enquote{simple} as possible.
\end{itemize}

\framebreak


\center
\vspace*{0.5cm}
\includegraphics[width=0.6\textwidth]{figure_man/biasvariance_scheme.png} \\
\footnotesize{Hastie, The Elements of Statistical Learning, 2009 (p. 225)}


\end{vbframe}



\endlecture
\end{document}
