\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Bias-variance Tradeoff
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/bv_anim_1.pdf
  }{
  \item Understand the bias-variance trade-off
  \item Know the definition of model bias, estimation bias, and estimation variance
}


%\section{Motivation for Regularization}

\begin{vbframe}{Bias-variance tradeoff}

In this slide set, we will visualize the bias-variance trade-off. \\
\lz 

We consider a DGP $\Pxy$ with $\Yspace \subset \R$ and the L2 loss $L$. We measure the distance between models $f:\Xspace\rightarrow\mathbb{R}^g$ via $$d(f, f^\prime) = \E_{\xv\sim\mathbb{P}_{\xv}}\left[L(f(\xv), f^\prime(\xv)\right].$$ \\
\lz
We define $\fbayes_0$ as the risk minimizer such that $$\fbayes_0 \in \argmin_{f \in \Hspace_0} \E_{\xy \sim \Pxy}\left[L(y, f(\xv))\right]$$

where $\Hspace_0 = \left\{f:\Xspace\rightarrow\mathbb{R}\vert\; d(\underline{0}, f) < \infty \right\}$ and $\underline{0}:\Xspace\rightarrow\{0\}$.

\framebreak

Our model space $\Hspace$ usually is a proper subset of $\Hspace_0$ and in general $\fbayes_0 \notin \Hspace.$\\
We define $\fbayes$ as the risk minimizer in $\Hspace,$ i.e.,
$$\fbayes \in \argmin_{f \in \Hspace} \E_{\xy \sim \Pxy}\left[L(f(\xv, y)\right].$$
$\fbayes \in \Hspace$ is closest to $\fbayes_0$, and we call $d(\fbayes_0, \fbayes)$ the model bias.

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/bv_anim_6.pdf}
\end{center}
\framebreak 
By regularizing our model, we further restrict the model space so that $\Hspace_R$ is a proper subset of $\Hspace.$
We define $\fbayes_R$ as the risk minimizer in $\Hspace_R,$ i.e.,
$$\fbayes_R \in \argmin_{f \in \Hspace_R} \E_{\xy \sim \Pxy}\left[L(f(\xv, y)\right].$$
$\fbayes_R \in \Hspace_R$ is closest to $\ftrue$, and we call $d(\fbayes_R, \fbayes)$ the estimation bias.
\begin{center}
\includegraphics[width=0.49\textwidth]{figure_man/bv_anim_5.pdf}
\end{center}
\framebreak

We sample a finite dataset $\D = \xyi^n \in \left(\Pxy\right)^n$ and find via ERM
$$\fh \in \argmin_{f\in\Hspace} \sumin L\left(\yi, \fh (\xi)\right).$$

\begin{columns}[onlytextwidth,T]
      \column{0.5\linewidth}

  \includegraphics[width=1.0\textwidth]{figure_man/bv_anim_4.pdf}

      \column{0.45\linewidth}
      \lz
      Note that the realization is only shown in the visualization for didactic purposes but is not an element of $\Hspace_0.$
    
    \end{columns}

\framebreak

Let's assume that $\fh$ is an unbiased estimate of $\fbayes$ (e.g., valid for linear regression), and we repeat the sampling process of $\fh$.

\begin{columns}[onlytextwidth,T]
      \column{0.5\linewidth}

  \includegraphics[width=1.0\textwidth]{figure_man/bv_anim_3.pdf}

      \column{0.5\linewidth}
      \lz 
      \begin{itemize}
          \item We can measure the spread of sampled $\fh$ around $\fbayes$ via $\delta = \var_\D\left[d(\fbayes, \fh)\right]$ which we call the estimation variance.
          \item We visualize this as a circle around $\fbayes$ with radius $\delta.$
      \end{itemize}
    \end{columns}

\framebreak

We repeat the previous construction in the restricted model space $\Hspace_R$ and sample $\fh_R$ such that 
$$\fh_R \in \argmin_{f\in\Hspace_R} \sumin L\left(\yi, \fh (\xi)\right).$$
\begin{columns}[onlytextwidth,T]
      \column{0.48\linewidth}

  \includegraphics[width=1.0\textwidth]{figure_man/bv_anim_2.pdf}

      \column{0.5\linewidth}
      \lz 
      \begin{itemize}
          \item We can measure the spread of sampled $\fh_R$ around $\fbayes_R$ via $\delta = \var_\D\left[d(\fbayes_R, \fh_R)\right]$ which we also call estimation variance.
          \item We observe that the increased bias results in a smaller estimation variance in $\Hspace_R$ compared to $\Hspace.$
      \end{itemize}
    \end{columns}

\end{vbframe}



\endlecture
\end{document}
