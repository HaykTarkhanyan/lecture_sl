\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/cv-error-lambda-path.png}
\newcommand{\learninggoals}{
  \item Know interpretation of $L2$ regularization as row-augmentation
  \item Know interpretation of $L2$ regularization as minimizing risk under feature noise
  %\item Derivation of the bias-variance tradeoff for Ridge regression
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Perspectives on Ridge Regression (Deep-Dive)}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Perspectives on $L2$ regularization}
We already saw two interpretations of $L2$ regularization. 
\begin{itemize}
    \item We know that it is equivalent to a constrained optimization problem:
  \begin{eqnarray*}  
  \thetah_{\text{Ridge}} &=& \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \lambda \|\thetab\|_2^2 = ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv\\
  %&=& \argmin_{\thetab} \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right) + \lambda \thetab^\top \thetab \\
  &=& \argmin_{\thetab} \sumin \left(\yi - \fxit\right)^2 \,
  \text{s.t. } \|\thetab\|_2^2  \leq t
  \end{eqnarray*}
  \item Bayesian interpretation of Ridge regression: For normal likelihood contributions $\mathcal{N}(\thetab^{\top}\xi,\sigma^2)$ and i.i.d. normal priors $\theta_j \sim \mathcal{N}(0,\tau^{2})$, the resulting MAP estimate is $\thetah_{\text{Ridge}}$ with $\lambda=\frac{\sigma^2}{\tau^2}$:
  $$\thetah_{\text{MAP}}=\argmax_{\theta} \log[p(\yv|\Xmat,\thetab)p(\thetab)] = \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \frac{\sigma^2}{\tau^2} \|\thetab\|_2^2$$
\end{itemize}

\end{vbframe}

\begin{vbframe}{$L2$ and row-augmentation}
We can also recover the Ridge estimator by performing least-squares on a \textbf{row-augmented} data set: Let $\tilde{\Xmat}:= \begin{pmatrix} \Xmat \\ \sqrt{\lambda} \id_{p} \end{pmatrix}$ and $\tilde{\yv} := \begin{pmatrix}
    \yv \\ \bm{0}_{p}
\end{pmatrix}$. Using the augmented data, the unregularized least-squares solution $\tilde{\thetab}$ can be written as
\begin{eqnarray*}
\tilde{\thetab} &=& \argmin_{\thetab} 
\sum_{i=1}^{n+p} \left(\tilde{\yi} - \thetab^T \tilde{\xi} \right)^2 \\ &=& \argmin_{\thetab} 
\sum_{i=1}^{n} \left(\yi - \thetab^T \xi \right)^2 + \sum_{j=1}^{p} \left(0 - \sqrt{\lambda} \theta_j \right)^2 \\ %= \thetah_{\text{Ridge}}
&=& \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \lambda \|\thetab\|_2^2
\end{eqnarray*}

$\Longrightarrow$ $\thetah_{\text{Ridge}}$ is the least-squares solution $\tilde{\thetab}$ but using $\tilde{\Xmat},\tilde{\yv}$ instead of $\Xmat, \yv$!
%$$\thetah_{\text{Ridge}} = ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv$$
\end{vbframe}

\begin{vbframe}{$L2$ and noisy features}
Now consider perturbed features $ \tilde{\xi}:= \xi + \bm{\delta}^{(i)}$ where $\bm{\delta}^{(i)} \overset{iid}{\sim} (\bm{0},\lambda \id_p)$. Note that no parametric family is assumed. We want to minimize the expected squared error taken w.r.t. the perturbations $\bm{\delta}$:
$$\riskt:= \mathbb{E}_{\bm{\delta}}\Big[\frac{1}{n}{\textstyle \sumin}\big((\yi-\thetab^{\top}\tilde{\xi})^2\big)\Big] = \mathbb{E}_{\bm{\delta}}\Big[\frac{1}{n}{\textstyle \sumin}\big((\yi-\thetab^{\top}(\xi+\bm{\delta}^{(i)}))^2\big)\Big]\,\,\Big|\, \text{expand}$$
\vspace{-0.2cm}
%Expanding, we obtain
$$\riskt = \mathbb{E}_{\bm{\delta}}\Big[\frac{1}{n}{\textstyle \sumin}\big((\yi-\thetab^{\top}\xi)^2 - 2 \thetab^{\top}\bm{\delta}^{(i)}(\yi-\thetab^{\top}\xi) + \thetab^{\top}\bm{\delta}^{(i)}\bm{{\delta}}^{(i) \top}\thetab\big)\Big]$$

By linearity of expectation, $\mathbb{E}_{\bm{\delta}}[\bm{\delta}^{(i)}]=\bm{0}_p$ and $\mathbb{E}_{\bm{\delta}}[\bm{\delta}^{(i)}\bm{\delta}^{(i)\top}]=\lambda \id_p$, this is
\vspace{-0.2cm}
%
\begin{align*}\riskt&=\frac{1}{n}{\textstyle \sumin}\big((\yi-\thetab^{\top}\xi)^2 - 2 \thetab^{\top}\mathbb{E}_{\bm{\delta}}[\bm{\delta}^{(i)}](\yi-\thetab^{\top}\xi) + \thetab^{\top}\mathbb{E}_{\bm{\delta}}[\bm{\delta}^{(i)}\bm{\delta}^{(i)\top}]\thetab \big) \\
&= \frac{1}{n}{\textstyle \sumin}(\yi-\thetab^{\top}\xi)^2+\lambda \Vert \thetab \Vert_2^2
\end{align*}
$\Longrightarrow$ Ridge regression on unperturbed features {\small $\xi$} turns out to be minimizing squared loss averaged over feature noise distribution!
\end{vbframe}






\endlecture
\end{document}
