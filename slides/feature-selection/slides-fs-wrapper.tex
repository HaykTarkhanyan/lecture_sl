
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\citebutton}[2]{%
\NoCaseChange{\resizebox{!}{9pt}{\protect\beamergotobutton{\href{#2}{#1}}}}%
}

\newcommand{\titlefigure}{figure_man/varsel_space.png}
\newcommand{\learninggoals}{
  \item Understand how wrapper methods work
  \item Understand how they could help in feature selection
  \item Know their advantages and disadvantages
}


\title{Supervised Learning}
\date{}

\begin{document}

  \lecturechapter{Wrapper methods}
  \lecture{Supervised Learning}

  \begin{vbframe}{Introduction}

    \begin{itemize}
      \item Wrapper methods emerged from the idea that different sets of features can be optimal for different learners
      \item Use the learner itself to assess the quality of the feature sets
      \item Evaluation on a test set or resampling techniques are used
      \item A wrapper is nothing else than a discrete search strategy for $S$, where the test error of a learner as a function of $S$ is now the objective criterion
    \end{itemize}

    \begin{center}
     \includegraphics[width = 0.3\textwidth]{figure/searchspace_binary.png}\\
     \scriptsize{Hasse diagram (source: Wikipedia)}
    \end{center}
    
\end{vbframe}

\begin{comment}

\begin{vbframe}{Introduction}

    Wrappers have the following components:


    \begin{itemize}
      \item A set of starting values
      \item Operators to create new points out of the given ones
      \item A termination criterion
    \end{itemize}


    \begin{figure}
      \includegraphics[width=8cm]{figure_man/varsel_space.png}
      % \caption{Space of all feature sets for 4 features.
      % The indicated relationships between the sets insinuate a greedy search strategy which either adds or removes a feature.}
      % Übersetzung von:
      % Raum aller Feature-Mengen bei 4 Features. Die eingezeichnete Nachbarschaftsbeziehung unterstellt eine Art ,,gierige'' Suchstrategie, bei der wir entweder ein Feature hinzufügen oder entfernen.}
    \end{figure}


  \end{vbframe}
\end{comment}

  \begin{vbframe}{Objective function}

    %\small

    Given $p$ features, \textbf{best-subset selection problem} is to find subset $S \subseteq \{ 1, \dots p \}$ optimizing objective $\Psi: \Omega \rightarrow \R$:
    \vspace{-0.15cm}
    %measuring the learner's generalization performance. The solution $S^*$ to the problem is
    $$S^{*}  \in \argmin_{{S \in \Omega}} \{ \Psi(S) \}$$
    %
    \vspace{-0.8cm}
    \begin{itemize}
    \setlength{\itemsep}{0.8em}
     \item $\Omega$  = search space of all feature subsets $S\subseteq\{ 1, \dots, p \}$. Usually we encode this by
      %(i.e., $\Omega \subseteq \mathcal{P}(\{ 1, \dots, p \})$, $\mathcal{P}$ denoting the power set) 
      bit vectors, i.e., $\Omega = \{0, 1\}^p$ (1 = feat. selected)
      %It will be clear from the context which variant we refer to.
     \item Objective $\Psi$ can be different functions, e.g., AIC/BIC for LM or cross-validated performance of a learner
     \item Poses a discrete combinatorial optimization problem over search space of size = $2^p$, i.e., grows exponentially in $p$ (power set)%as it is the power set of $\{1,\ldots,p\}$
      %also known as $L_0$ regularization.
     \item Unfortunately can not be solved efficiently in general (NP hard; see, e.g., \citebutton{Natarajan, 1995}{https://epubs.siam.org/doi/10.1137/S0097539792240406})
     \item Can avoid searching entire space by employing efficient search strategies, traversing search space in a ``smart" way %that finds performant feature subsets

    \end{itemize}

  \end{vbframe} 


\begin{comment}
 \begin{vbframe}{How difficult is best-subset selection?}

    \begin{itemize}
    \setlength{\itemsep}{1em}
      \item Size of search space = $2^p$, i.e., grows exponentially in $p$ as it is the power set of $\{1,\ldots,p\}$
      \item Finding best subset is discrete combinatorial optimization problem.
      %also known as $L_0$ regularization.
     \item It can be shown that this problem unfortunately can not be solved efficiently in general (NP hard; see, e.g., \citebutton{Natarajan, 1995}{https://epubs.siam.org/doi/10.1137/S0097539792240406})
     \item We can avoid having to search the entire space by employing efficient search strategies, moving through the search space in a smart way that finds performant feature subsets
     %\item By employing efficient search strategories, we can avoid searching the entire space.
    %\item Of course this does not mean that we have to search the entire space, since there are more efficient search strategies.
    %  \item Formally spoken: One can show that the problem is NP-hard!
    %  \item This means that the problem cannot be solved in polynomial (P) time: ${\mathcal{O}} (p^c)$, where $c \in \N$ indicates the degree o the polynom.
    % \end{blocki}
    % 
    % \framebreak
    % 
    % \begin{blocki}{How difficult is it to solve the introduced optimization problem, hence, to find the optimal feature set?}
    %  \item More precisely, the proof demonstrates that this problem cannot be approximated within any constant, unless P = NP.
    % 
    %   The latter means, that if you find an algorithm that solves a more difficult class of problems (than this optimization problem) in polynomial time, this implies that you found how to solve all easier problems (including our optimization problem) in polynomial time.
    % \item \textbf{Attention}: This does not imply that it is useless trying to construct strategies which work in practice!
    %\item Thus our problem now consists of moving through the search space in a smart and efficient way, thereby finding a particularly good set of features.
    \end{itemize}
  \end{vbframe}
  
\begin{frame}{Greedy forward search}

    \begin{blocki}{}
      \item Let $S \subset \{1, \dots, p \}$, where $\{1, \dots p \}$ are feature indices
      \item Start with the empty feature set $S = \emptyset$
      \item For a given set $S$, generate all $S_j = S \cup \{j\}$ with $j \notin S$.
      \item Evaluate the classifier on all $S_j$ and use the best $S_j$
      \item Iterate over this procedure
      \item Terminate if:
        \begin{enumerate}
          \item the performance measure doesn't improve enough
          \item a maximum number of features is used
          \item a given performance value is reached
        \end{enumerate}
    \end{blocki}

    \end{frame}
\end{comment}

\begin{frame}{Greedy forward search}
Let $S \subset \{1, \dots, p \}$ be subset of feature indices.
\vspace{-0.01cm}
    \begin{enumerate}
      %\item Let $S \subset \{1, \dots, p \}$, where $\{1, \dots p \}$ are feature indices
      \item Start with the empty feature set $S = \emptyset$
      \item For a given set $S$, generate all $S_j = S \cup \{j\}$ with $j \notin S$.
      \item Evaluate the classifier on all $S_j$ and use the best $S_j$
      \end{enumerate}
    %\vspace{-0.2cm}
    \textbf{Example for greedy forward search on bike sharing data:}
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{figure/fs-wrappers-powerset-tree-1.png}
    \end{center}

\end{frame}
    %\framebreak

\begin{frame}[noframenumbering]{Visualization of GFS}    
\begin{enumerate}
    \setcounter{enumi}{3}
    \item Iterate over this procedure
\end{enumerate}
    \begin{center}
      \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-powerset-tree-2.png}
      \end{center}
      %\framebreak
\end{frame}

\begin{frame}[noframenumbering]{Visualization of GFS}
\begin{enumerate}
    \setcounter{enumi}{3}
    \item Iterate over this procedure
\end{enumerate}

    \begin{center}
      \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-powerset-tree-3.png}
    \end{center}
\end{frame}
  
\begin{frame}[noframenumbering]{Visualization of GFS}
    \begin{center}
      \includegraphics[width = 0.6\textwidth]{figure/fs-wrappers-powerset-tree-4.png}
      \end{center}
      \vspace{-0.2cm}
 \begin{enumerate}
     \setcounter{enumi}{4}
     \item Terminate if performance does not improve further or max. number of features is used
 \end{enumerate}
\end{frame}
  
\begin{frame}[noframenumbering]{Visualization of GFS}
    \begin{center}
    \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-powerset-all-4.png}
    \end{center}
  \end{frame}


\begin{vbframe}{Greedy backward search}


    \begin{blocki}{}
      \item Start with the full index set of features $S = \{1, \ldots, p\}$.
      \item For a given set $S$ generate all

      $S_j = S \setminus\{j\}$ with $j \in S$.
      \item Evaluate the classifier on all $S_j$\
        and use the best $S_j$.
      \item Iterate over this procedure.
      \item Terminate if:
        \begin{itemize}
          \item the performance drops drastically, or
          \item a given performance value is undershot.
        \end{itemize}
      \end{blocki}

      \begin{itemize}
          \item GFS is much faster and generates sparser feature selections
          \item GBS much more costly and slower, but sometimes slightly better.
      \end{itemize}
    
  \end{vbframe}

  \begin{vbframe}{Extensions}

    \begin{itemize}
      \setlength{\itemsep}{1.0em}
      \item Eliminate or add several features at once to increase speed.
      \item Allow alternating forward and backward search.
      \item Randomly create candidate feature sets in each iteration.
      \item Continue search based on the set of features where an improvement is present.
      \item Use improvements of earlier iterations.
    \end{itemize}

    \framebreak

    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \State Start with a random set of features $S$ (bit vector $b$).
      \Repeat
      \State Flip a couple of bits in $b$ with probability $p$
      \State Generate set $S^\prime$ and bit vector $b^\prime$
      \State Measure the classifier's performance on $S^\prime$
      \State If $S^\prime$ performs better than $S$, update $S \leftarrow S^\prime$, otherwise $S \leftarrow S$.
      \Until One of the following conditions is met:
        \begin{itemize}
          \item A given performance value is reached.
          \item Budget is exhausted.
        \end{itemize}
        \caption{A simple 1+1 genetic algorithm}
    \end{algorithmic}
    \end{algorithm}

    %\framebreak
 \end{vbframe}   
%\begin{center}
%\begin{minipage}{0.48\textwidth}
%    \includegraphics[width=\textwidth]{figure/var-selection1.png}
%\end{minipage}\hfill
%\begin{minipage}{0.48\textwidth}
%    \includegraphics[width=\textwidth]{figure/var-selection2.png}
%\end{minipage}
%\end{center}
%Bit representation of the best variables included up to iteration $t$

% this is in the optim slides regarding featsel using EA
\begin{comment}
\textbf{Aim:} Use a $(\mu + \lambda)$ selection strategy for feature selection.\\
\vspace*{0.2cm}
Our iterative algorithm with $100$ iterations is as follows:
\begin{enumerate}
\item Initialize the population and evaluate it. Therefore, encode a chromosome of an individual as a bit string of length $p$, i.e. $\textbf{z} \in \{0, 1\}^p$. Where $z_j =1$ means that variable $j$ is included in the model.
\item Apply the variation and evaluate the fitness function. As fitness function, select BIC of the model belonging to the corresponding variable configuration $\textbf{z} \in \{0, 1\}^p$.
\item Finally, use $(\mu + \lambda)$-selection strategy as the survival selection with population size of $\mu = 100$ and $\lambda =50$ offspring.
\end{enumerate}

In addition:

\begin{itemize}
\item for the mutation, use bit flip with $p = 0.3$
\item for the recombination, use Uniform crossover with $p=0.5$
\end{itemize}

\lz

By exploiting \textbf{Greedy} as a selection strategy, ensure that you always choose individuals with the best fitness. 
\end{comment}



\begin{vbframe}{Extensions}
        %\vspace{1.3cm}
        \begin{center}
            \includegraphics[width=0.53\textwidth]{figure/var-selection1.png}
            %\hspace{0.2cm} \newline
            \includegraphics[width=0.53\textwidth]{figure/var-selection2.png}
     \end{center}
    \vspace{-0.5cm}
Top: BIC over number of iterations. Bottom: Bit representation of the best variables included up to iteration $t$.
\end{vbframe}


    %\framebreak

    %\vspace{0.9cm}
    %\begin{center}
    %\begin{figure}
    %    \includegraphics[height = 0.7\textheight]{figure/var-selection2.png}
    %\end{figure}
    %Bit representation of the best variables included up to iteration $t$
    %\end{center}


  \begin{vbframe}{Wrappers}

    \begin{blocki}{Advantages:}
      \item Can be combined with every learner
      \item Can be combined with every performance measure
      \item Optimizes the desired criterion directly
    \end{blocki}

    \lz

    \begin{blocki}{Disadvantages:}
      \item Evaluating the target function is expensive
      \item Does not scale well if number of features becomes large
      \item Does not use much structure or available information from our model
      \item Nested resampling becomes necessary
    \end{blocki}

  \end{vbframe}

    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # specify the search strategy.
    % # We want to use forward search:
    % ctrl = makeFeatSelControlSequential(method = "sfs")
    % ctrl
    %
    % # Selected features
    % sfeats = selectFeatures(learner = "regr.lm", task = bh.task,
    %   resampling = rdesc, control = ctrl, show.info = FALSE)
    % sfeats
    % @
    %
    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # Visualize optimization path
    % analyzeFeatSelResult(sfeats)
    % @
    %
    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # Fuse a base-learner with a search strategy (here: sfs)
    % lrn = makeFeatSelWrapper("classif.rpart", resampling = rdesc,
    %   control = ctrl, show.info = FALSE)
    % res = resample(lrn, iris.task, resampling = rdesc,
    %   show.info = FALSE, models = TRUE, extract = getFeatSelResult)
    % res$extract[1:5]
    % @

  \endlecture
\end{document}

