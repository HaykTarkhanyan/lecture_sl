
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/varsel_space.png}
\newcommand{\learninggoals}{
  \item Understand how wrapper methods work
  \item Understand how they coudl help in feature selection
  \item Know their advantages and disadvantages
}


\title{Supervised Learning}
\date{}

\begin{document}

  \lecturechapter{Wrapper methods}
  \lecture{Supervised Learning}

  \begin{vbframe}{Introduction}

    \begin{itemize}
      \item Wrapper methods emerged from the idea that different sets of features can be optimal for different classification learners.
      \item Use the classifier itself to assess the quality of the feature sets.
      \item Evaluation on a test set or resampling techniques are used.
      \item A wrapper is nothing else than a discrete search strategy for $S$, where the test error of a learner as a function of $S$ is now the objective criterion.

    \end{itemize}


    \framebreak

    Wrappers have the following components:


    \begin{itemize}
      \item A set of starting values
      \item Operators to create new points out of the given ones
      \item A termination criterion
    \end{itemize}


    \begin{figure}
      \includegraphics[width=8cm]{figure_man/varsel_space.png}
      % \caption{Space of all feature sets for 4 features.
      % The indicated relationships between the sets insinuate a greedy search strategy which either adds or removes a feature.}
      % Übersetzung von:
      % Raum aller Feature-Mengen bei 4 Features. Die eingezeichnete Nachbarschaftsbeziehung unterstellt eine Art ,,gierige'' Suchstrategie, bei der wir entweder ein Feature hinzufügen oder entfernen.}
    \end{figure}


  \end{vbframe}
  
  \begin{vbframe}{Greedy forward search}

    \begin{blocki}{}
      \item Let $S \subset \{1, \dots, p \}$, where $\{1, \dots p \}$ is an index set of all features.
      \item Start with the empty feature set $S = \emptyset$.
      \item For a given set $S$, generate all $S_j = S \cup \{j\}$ with $j \notin S$.
      \item Evaluate the classifier on all $S_j$ and use the best $S_j$.
      \item Iterate over this procedure.
      \item Terminate if:
        \begin{itemize}
          \item the performance measure doesn't improve enough.
          \item a maximum number of features is used.
          \item a given performance value is reached.
        \end{itemize}
    \end{blocki}

    \framebreak

    \textbf{Example for greedy forward search on iris data:}
    \begin{center}
    \includegraphics[width = 0.6\textwidth]{figure_man/wrapperanim1.png}
    \end{center}

    \framebreak

    \begin{center}
    \includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim2.png}
    \end{center}

    \framebreak

    \begin{center}
    \includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim3.png}
    \end{center}

    \framebreak

    \begin{center}
    \includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim4.png}
    \end{center}

    \framebreak

    \begin{center}
    \includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim5.png}
    \end{center}

    \framebreak

    \begin{center}
    \includegraphics[width = 0.65\textwidth]{figure_man/wrapperanim6.png}
    \end{center}
    
  \end{vbframe}


  \begin{vbframe}{Greedy backward search}


    \begin{blocki}{}
      \item Start with the full index set of features $S = \{1, \ldots, p\}$.
      \item For a given set $S$ generate all

      $S_j = S \setminus\{j\}$ with $j \in S$.
      \item Evaluate the classifier on all $S_j$\
        and use the best $S_j$.
      \item Iterate over this procedure.
      \item Terminate if:
        \begin{itemize}
          \item the performance drops drastically, or
          \item a given performance value is undershot.
        \end{itemize}
      \end{blocki}
    
  \end{vbframe}

  \begin{vbframe}{Extensions}

    \begin{itemize}
      \setlength{\itemsep}{1.0em}
      \item Eliminate or add several features at once to increase speed.
      \item Allow alternating forward and backward search.
      \item Randomly create candidate feature sets in each iteration.
      \item Continue search based on the set of features where an improvement is present.
      \item Use improvements of earlier iterations.
    \end{itemize}

    \framebreak

    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \State Start with a random set of features $S$ (bit vector $b$).
      \Repeat
      \State Flip a couple of bits in $b$ with probability $p$.
      \State Generate set $S^\prime$ and bit vector $b^\prime$.
      \State Measure the classifier's performance on $S^\prime$.
      \State If $S^\prime$ performs better than $S$, update $S \leftarrow S^\prime$, otherwise $S \leftarrow S$.
      \Until One of the following conditions is met:
        \begin{itemize}
          \item A given performance value is reached.
          \item Budget is exhausted.
        \end{itemize}
        \caption{A simple 1+1 genetic algorithm}
    \end{algorithmic}
    \end{algorithm}

    \end{vbframe}

  \begin{vbframe}{Wrappers}

    \begin{blocki}{Advantages:}
      \item Can be combined with every learner.
      \item Can be combined with every performance measure.
      \item Optimizes the desired criterion directly.
    \end{blocki}

    \lz

    \begin{blocki}{Disadvantages:}
      \item Evaluating the target function is expensive.
      \item Does not scale well if number of features becomes large.
      \item Does not use much structure or available information from our model.
    \end{blocki}

  \end{vbframe}

    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # specify the search strategy.
    % # We want to use forward search:
    % ctrl = makeFeatSelControlSequential(method = "sfs")
    % ctrl
    %
    % # Selected features
    % sfeats = selectFeatures(learner = "regr.lm", task = bh.task,
    %   resampling = rdesc, control = ctrl, show.info = FALSE)
    % sfeats
    % @
    %
    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # Visualize optimization path
    % analyzeFeatSelResult(sfeats)
    % @
    %
    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # Fuse a base-learner with a search strategy (here: sfs)
    % lrn = makeFeatSelWrapper("classif.rpart", resampling = rdesc,
    %   control = ctrl, show.info = FALSE)
    % res = resample(lrn, iris.task, resampling = rdesc,
    %   show.info = FALSE, models = TRUE, extract = getFeatSelResult)
    % res$extract[1:5]
    % @

  \endlecture
\end{document}

