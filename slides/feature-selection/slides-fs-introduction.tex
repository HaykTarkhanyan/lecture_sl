\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/feature_sel_vs_extr.png}
\newcommand{\learninggoals}{
  \item Adding more features can be detrimental to predictive performance.
  \item Benefits of keeping only informative features for the model
}

\newcommand{\citebutton}[2]{%
\NoCaseChange{\resizebox{!}{9pt}{\protect\beamergotobutton{\href{#2}{#1}}}}%
}

\title{Supervised Learning}
\date{}

\begin{document}

  \lecturechapter{Feature Selection}
  \lecture{Fortgeschrittene Computerintensive Methoden}

  \begin{vbframe}{Introduction}

    Feature selection deals with
    \begin{itemize}
     \item techniques for choosing a suitable subset of features
      \item evaluating the influence of features on the model
    \end{itemize}
    \vspace{0.5cm}
    \begin{center}
    \includegraphics{figure_man/varsel_overview.png}
    \end{center}

    \lz

    Feature selection can be performed relying on domain knowledge and expert input, or using a data-driven algorithmic approach.
  \end{vbframe}


%  \begin{vbframe}{Overview}
%    \begin{center}
%    \includegraphics{figure_man/varsel_overview.png}
%    \end{center}
%
%    \lz
%
%    It is the task of statisticians, data analysts and machine learners to filter out the relevant information which is \textbf{useful} for prediction!
%  \end{vbframe}


  \begin{vbframe}{Motivation}
    \begin{itemize}
    \setlength{\itemsep}{0.8em}
      \item Naive view:
        \begin{itemize}
          \item More features $\rightarrow$ more information $\rightarrow$ discriminant power $\uparrow$
          \item Model is not harmed by irrelevant features since their parameters can simply be estimated as 0.
        \end{itemize}
     %\item In practice there are many reasons why this is not the case!
     \item In practice, irrelevant and redundant features can \enquote{confuse} learners (see \textbf{curse of dimensionality}) and worsen performance.
     \item Example: In linear regression, $R^2$ is monotonically increasing in $p$, but adding irrelevant features leads to overfitting (capturing noise). %instead of underlying relationship.
    %\item Feature selection is critical for 
    %\begin{itemize}
    %    \item reducing noise and overfitting, 
    %    \item improving performance/generalization,
    %    \item enhancing interpretability by identifying most informative features.
    %\end{itemize}
  \end{itemize}

  \begin{center}
     \includegraphics[width = 0.5\textwidth]{figure/avoid_overfitting_02.png}\\
    \end{center}

  \framebreak

    \begin{itemize}
    \setlength{\itemsep}{1.0em}
      \item In high-dimensional data sets, we often have prior information that many features are either irrelevant or redundant.
      %many of which will be irrelevant or redundant, and multiple features of low quality.
      % \item This is due to the fact that there is an increasingly strong automation of measuring methods.
      % Especially the automatized collection of information by computers and the availability of information in the world wide web generates data sets with an extremely high dimensionality.
      %\item In domains with many features the underlying distribution function can be very complex and hard to estimate.
      \item Feature selection is critical for 
    \begin{itemize}
        \item reducing noise and overfitting, 
        \item improving performance/generalization,
        \item interpretability by identifying most informative features.
    \end{itemize}
      \item Feature selection can also remedy problems arising in small $n$ regimes or under limited computational resources. 
      %\item Training data are limited.
      %\item Computational resources are limited.
      \item Many models require $n > p$ data. Thus, we either need to
      %\item Thus, we either need
      \begin{itemize}
        \item adapt models to high-dimensional data (e.g. regularization),
        \item design entirely new procedures for $p>n$ data, or
        \item use the preprocessing methods addressed in this lecture.
      \end{itemize}
    \end{itemize}
  \end{vbframe}

  \begin{vbframe}{Size of datasets}
The increasingly automatized collection of information makes data sets with extremely high dimensionality available, while classical models were developed for small $p$ data.
\vspace{0.5cm}
    \begin{itemize}
    \setlength{\itemsep}{1.2em}
      \item \textbf{Classical setting}: Up to around $10^2$ features, feature selection might be relevant, but benefits often negligible.
      \item \textbf{Datasets of medium to high dimensionality}:
        At around $10^2$ to $10^3$ features, classical approaches can still work well, while principled feature selection helps in many cases.
      \item \textbf{High-dimensional data}: $10^3$ to $10^9$ or more features.
        Examples are e.g. micro-array / gene expression data and text categorization (bag-of-words features).
        If, in addition, observations are few, the scenario is called $p \gg n$.
    \end{itemize}

  \end{vbframe}

  \begin{vbframe}{Feature selection vs. extraction}

    \begin{columns}
      %Both graphs taken out from Tim Conrad's presentation for Novisad (see cim2/external_material/tim_conrad_novisad)

      \column{0.49\textwidth}
      %\textbf{Feature selection}

      \medskip

      \includegraphics{figure_man/feature_selection.png}

      \smallskip

      \begin{itemize}
        \item Creates a subset of original features $\xv$ by selecting $\tilde{p} < p$ features $\bm{f}$.
        %\item Selected features are subset of $\xv$.
        \item Retains information on selected individual features.
      \end{itemize}

      \column{0.49\textwidth}
      %\textbf{Feature extraction}

      \medskip

      \includegraphics{figure_man/feature_extraction.png}

      \smallskip

      \begin{itemize}
        \item Maps $p$ features in $\xv$ to $\tilde{p}$ extracted features $\bm{f}$.
        %\item Forms linear or nonlinear combinations of the original features.
        \item Info on individual features can be lost through (non-)linear combination.
      \end{itemize}

    \end{columns}

    % \vspace{0.3cm}
    % 
    % {\tiny{Source: Hsiao-Yun Huang. Regularized Double Nearest Neighbor Feature Extraction for Hyperspectral Image Classification \code{\url{https://dokumen.tips/documents/regularized-double-nearest-neighbor-feature-extraction-for-hyperspectral-image-5694e4566f1f3.html}}}\par}

    \framebreak

    
    %\framebreak
    \begin{itemize}
    \footnotesize
        \item Both FS and FE contribute to\\ 1) dimensionality reduction, and 2) simplicity of classification rules.
        \item FE can be unsupervised (PCA, Multidimensional Scaling, Manifold Learning) or supervised (supervised PCA, partial least squares).        
        \item FE can produce lower dim projections which can be more informative than FS.
    \end{itemize}

    \vspace{0.2cm}


    \begin{center}
     %feature_sel_vs_extr.R
     \includegraphics[width = 0.7\textwidth]{figure_man/feature_sel_vs_extr.png}\\
    \end{center}
    %\vspace{-0.2cm}
    %\scriptsize{Projection onto $x_1$ axis (feature selection) yields overlapping mixture components, while projection onto the hyperplane perpendicular to the first principal component (feature extraction) separates them.}
    %\normalsize


  \end{vbframe}

  \begin{vbframe}{Types of feature selection methods}
 
  In rest of the chapter, we introduce different types of methods for FS:

  \begin{itemize}
    \item Filters: evaluate relevance of features using statistical properties such as correlation with target variable.
    \item Wrappers: use a model to evaluate subsets of features. 
    \item Embedded methods: integrate FS directly into specific model - we look at them in their dedicated chapters.
  \end{itemize}

      \textbf{Example: embedded method (Lasso)} regularizing model params with $L1$ penalty %in the empirical risk 
      enables ``automatic" feature selection:
      \vspace{-0.28cm}
      $ \riskrt = \risket + \lambda \|\thetab\|_1 = \sumin \left(\yi - \thetab^\top \xi \right)^2 +\lambda \sum_{j=1}^p |\theta_j| $
      %are very popular for high-dimensional data.
      %\item The penalty shrinks the coefficients towards 0 in the final model.
      %\item Many (improved) variants: group LASSO, adaptive LASSO, ElasticNet, ...
      %\item Has some very nice optimality results: e.g., compressed sensing.
\vspace{0.1cm}
  \begin{center}
  \includegraphics[width=0.65\textwidth]{figure/regu_example_lasso_ridge.png}
  %\footnotesize{Lasso vs ridge regularization.}
  \end{center}


  \end{vbframe}



  \endlecture
\end{document}