\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/feature_sel_vs_extr.png}
\newcommand{\learninggoals}{
  \item Understand that adding more features can be detrimental to predictive performance.
  \item Understand the benefits of keeping only informative features for the model.
}

\newcommand{\citebutton}[2]{%
\NoCaseChange{\resizebox{!}{9pt}{\protect\beamergotobutton{\href{#2}{#1}}}}%
}

\title{Supervised Learning}
\date{}

\begin{document}

  \lecturechapter{Feature Selection}
  \lecture{Fortgeschrittene Computerintensive Methoden}

  \begin{vbframe}{Introduction}

    Feature selection deals with
    \begin{itemize}
     \item techniques for choosing a suitable subset of features
      \item evaluating the influence of features on the model
    \end{itemize}
    \vspace{0.5cm}
    \begin{center}
    \includegraphics{figure_man/varsel_overview.png}
    \end{center}

    \lz

    Feature selection can be performed relying on domain knowledge and expert input, or using a data-driven algorithmic approach.
  \end{vbframe}


%  \begin{vbframe}{Overview}
%    \begin{center}
%    \includegraphics{figure_man/varsel_overview.png}
%    \end{center}
%
%    \lz
%
%    It is the task of statisticians, data analysts and machine learners to filter out the relevant information which is \textbf{useful} for prediction!
%  \end{vbframe}


  \begin{vbframe}{Motivation}
    \begin{itemize}
    \setlength{\itemsep}{0.8em}
      \item Naive view:
        \begin{itemize}
          \item More features $\rightarrow$ more information $\rightarrow$ discriminant power $\uparrow$
          \item Model is not harmed by irrelevant features since their parameters can simply be estimated as 0.
        \end{itemize}
     %\item In practice there are many reasons why this is not the case!
     \item In practice, irrelevant and redundant features can \enquote{confuse} learners (see \textbf{curse of dimensionality}) and worsen performance.
     \item Example: In linear regression, $R^2$ is monotonically increasing in $p$, but adding irrelevant features leads to overfitting (capturing noise). %instead of underlying relationship.
    %\item Feature selection is critical for 
    %\begin{itemize}
    %    \item reducing noise and overfitting, 
    %    \item improving performance/generalization,
    %    \item enhancing interpretability by identifying most informative features.
    %\end{itemize}
  \end{itemize}

  \begin{center}
     \includegraphics[width = 0.5\textwidth]{figure/avoid_overfitting_02.png}\\
    \end{center}

  \framebreak

    \begin{itemize}
    \setlength{\itemsep}{1.0em}
      \item In high-dimensional data sets, we often have prior information that many features are either irrelevant or redundant.
      %many of which will be irrelevant or redundant, and multiple features of low quality.
      % \item This is due to the fact that there is an increasingly strong automation of measuring methods.
      % Especially the automatized collection of information by computers and the availability of information in the world wide web generates data sets with an extremely high dimensionality.
      %\item In domains with many features the underlying distribution function can be very complex and hard to estimate.
      \item Feature selection is critical for 
    \begin{itemize}
        \item reducing noise and overfitting, 
        \item improving performance/generalization,
        \item interpretability by identifying most informative features.
    \end{itemize}
      \item Feature selection can also remedy problems arising in small $n$ regimes or under limited computational resources. 
      %\item Training data are limited.
      %\item Computational resources are limited.
      \item Many models require $n > p$ data. Thus, we either need to
      %\item Thus, we either need
      \begin{itemize}
        \item adapt models to high-dimensional data (e.g. regularization),
        \item design entirely new procedures for $p>n$ data, or
        \item use the preprocessing methods addressed in this lecture.
      \end{itemize}
    \end{itemize}
  \end{vbframe}

  \begin{vbframe}{Size of datasets}
The increasingly automatized collection of information makes data sets with extremely high dimensionality available, while classical models were developed for small $p$ data.
\vspace{0.5cm}
    \begin{itemize}
    \setlength{\itemsep}{1.2em}
      \item \textbf{Classical setting}: Up to around $10^2$ features, feature selection might be relevant, but benefits often negligible.
      \item \textbf{Datasets of medium to high dimensionality}:
        At around $10^2$ to $10^3$ features, classical approaches can still work well, while principled feature selection helps in many cases.
      \item \textbf{High-dimensional data}: $10^3$ to $10^9$ or more features.
        Examples are e.g. micro-array / gene expression data and text categorization (bag-of-words features).
        If, in addition, observations are few, the scenario is called $p \gg n$.
    \end{itemize}

  \end{vbframe}

  \begin{vbframe}{Feature selection vs. extraction}

    \begin{columns}
      %Both graphs taken out from Tim Conrad's presentation for Novisad (see cim2/external_material/tim_conrad_novisad)

      \column{0.49\textwidth}
      %\textbf{Feature selection}

      \medskip

      \includegraphics{figure_man/feature_selection.png}

      \smallskip

      \begin{itemize}
        \item Creates a subset of original features $\xv$ by selecting $\tilde{p} < p$ features $\bm{f}$.
        %\item Selected features are subset of $\xv$.
        \item Retains information on selected individual features.
      \end{itemize}

      \column{0.49\textwidth}
      %\textbf{Feature extraction}

      \medskip

      \includegraphics{figure_man/feature_extraction.png}

      \smallskip

      \begin{itemize}
        \item Maps $p$ features in $\xv$ to $\tilde{p}$ extracted features $\bm{f}$.
        %\item Forms linear or nonlinear combinations of the original features.
        \item Info on individual features can be lost through (non-)linear combination.
      \end{itemize}

    \end{columns}

    % \vspace{0.3cm}
    % 
    % {\tiny{Source: Hsiao-Yun Huang. Regularized Double Nearest Neighbor Feature Extraction for Hyperspectral Image Classification \code{\url{https://dokumen.tips/documents/regularized-double-nearest-neighbor-feature-extraction-for-hyperspectral-image-5694e4566f1f3.html}}}\par}

    \framebreak

    
    %\framebreak
    \begin{itemize}
    \footnotesize
        \item Both FS and FE contribute to\\ 1) dimensionality reduction, and 2) simplicity of classification rules.
        \item FE can be unsupervised (PCA, Multidimensional Scaling, Manifold Learning) or supervised (supervised PCA, partial least squares).        
        \item FE can produce lower dim projections which can be more informative than FS.
    \end{itemize}

    \vspace{0.2cm}


    \begin{center}
     %feature_sel_vs_extr.R
     \includegraphics[width = 0.7\textwidth]{figure_man/feature_sel_vs_extr.png}\\
    \end{center}
    %\vspace{-0.2cm}
    %\scriptsize{Projection onto $x_1$ axis (feature selection) yields overlapping mixture components, while projection onto the hyperplane perpendicular to the first principal component (feature extraction) separates them.}
    %\normalsize


  \end{vbframe}



  \begin{vbframe}{Objective function}

    %\small

    Given $p$ features, the \textbf{best-subset selection problem} is to find a subset $S \subseteq \{ 1, \dots p \}$ optimizing objective $\Psi: \Omega \rightarrow \R$:
    \vspace{-0.1cm}
    %measuring the learner's generalization performance. The solution $S^*$ to the problem is
    $$S^{*}  \in \argmin_{{S \in \Omega}} \{ \Psi(S) \}$$
    %
    \vspace{-0.8cm}
    \begin{itemize}
    \setlength{\itemsep}{0.9em}
     \item $\Omega$  = search space of all feature subsets $S\subseteq\{ 1, \dots, p \}$. Usually we encode this by
      %(i.e., $\Omega \subseteq \mathcal{P}(\{ 1, \dots, p \})$, $\mathcal{P}$ denoting the power set) 
      bit vectors, i.e., $\Omega = \{0, 1\}^p$ (1 = feat. selected)
      %It will be clear from the context which variant we refer to.
     \item Objective $\Psi$ can be different functions, e.g., AIC/BIC for LM or cross-validated performance of a learner.
    \end{itemize}

    \begin{center}
     \includegraphics[width = 0.3\textwidth]{figure/searchspace_binary.png}\\
     \scriptsize{Hasse diagram (source: Wikipedia)}
    \end{center}
    %\normalsize
  \end{vbframe} 

  
 \begin{vbframe}{How difficult is best-subset selection?}

    \begin{itemize}
    \setlength{\itemsep}{1.2em}
      \item Size of search space = $2^p$, i.e., grows exponentially in $p$ as it is the power set of $\{1,\ldots,p\}$.
      \item Finding best subset is discrete combinatorial optimization problem also known as $L_0$ regularization.
     \item It can be shown that this problem unfortunately can not be solved efficiently in general (NP hard; see, e.g., \citebutton{Natarajan, 1995}{https://epubs.siam.org/doi/10.1137/S0097539792240406})
     \item We can avoid having to search the entire space by employing efficient search strategies, moving through the search space in a smart way that finds performant feature subsets.
     %\item By employing efficient search strategories, we can avoid searching the entire space.
    %\item Of course this does not mean that we have to search the entire space, since there are more efficient search strategies.
    %  \item Formally spoken: One can show that the problem is NP-hard!
    %  \item This means that the problem cannot be solved in polynomial (P) time: ${\mathcal{O}} (p^c)$, where $c \in \N$ indicates the degree o the polynom.
    % \end{blocki}
    % 
    % \framebreak
    % 
    % \begin{blocki}{How difficult is it to solve the introduced optimization problem, hence, to find the optimal feature set?}
    %  \item More precisely, the proof demonstrates that this problem cannot be approximated within any constant, unless P = NP.
    % 
    %   The latter means, that if you find an algorithm that solves a more difficult class of problems (than this optimization problem) in polynomial time, this implies that you found how to solve all easier problems (including our optimization problem) in polynomial time.
    % \item \textbf{Attention}: This does not imply that it is useless trying to construct strategies which work in practice!
    %\item Thus our problem now consists of moving through the search space in a smart and efficient way, thereby finding a particularly good set of features.
    \end{itemize}
  \end{vbframe}


  \endlecture
\end{document}