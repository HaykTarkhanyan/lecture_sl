\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/compboost-to-glm-iter10000.png}
\newcommand{\learninggoals}{
  \item Learn the relation of CWB to a GLM
  \item
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Componentwise Gradient Boosting}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Relation to GLM}

In the simplest case we use linear models (without intercept) on single features
as base learners:

$$
  b_j(x_j,\theta) = \theta x_j  \quad \text{for } j = 1, 2, \dots, p \quad
  \text{and with } b_j \in \mathcal{B}_j = \{\theta x_j  ~\rvert~ \theta \in
  \mathbb{R} \}.
$$


This definition will result in an ordinary \textbf{linear regression} model.

% .\footnote{Note: a linear model base learner without intercept only makes sense if the covariates are centered (see \texttt{mboost} tutorial, page7)}


\begin{itemize}
  \item Note that linear base learners without intercept only make sense for
  covariates that have been centered before.
  \item If we let the boosting algorithm converge, i.e., let it run for a really
  long time, the parameters will converge to the \textbf{same solution} as the
  maximum likelihood estimate.
  \item This means that, by specifying a loss function according to the negative
  likelihood of a distribution from an exponential family and defining a link
  function accordingly, this kind of boosting is equivalent to a (regularized)
  \textbf{generalized linear model (GLM)}.
\end{itemize}

\framebreak

% ------------------------------------------------------------------------------

But: We do not \emph{need} an exponential family and thus are able to fit models
to all kinds of other distributions and losses, as long as we can calculate (or
approximate) a derivative of the loss.
% Note, however, that this does not imply that the algorithm does something
% meaningful (e.g., non-convex loss functions would still require some
% additional effort).

\lz

Usually we do not let the boosting model converge fully, but \textbf{stop
early} for the sake of regularization and feature selection.

\lz

Even though the resulting model looks like a GLM, we do not have valid standard
errors for our coefficients,
so cannot provide confidence or prediction intervals or perform tests etc.
$\rightarrow$ post-selection inference.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Logistic regression with CWB}

Fitting a logistic regression (GLM with a Bernoulli distributed response) requires the specification of the loss as function as

$$
  L(y, \fx) = -y \cdot \fx + \log (1 + \exp(\fx)),\ y\in\{0, 1\}
$$

Note that CWB (as gradient boosting in general) predicts a score $\fx \in \R$. Squashing the score $\fx$ to $\pi(\xv) = s(\fx) \in [0,1]$ corresponds to transforming the linear predictor of a GLM to the response domain with a link function $s$:

\begin{itemize}
  \item $s(\fx) = (1 + \exp(-\fx))^{-1}$ for logistic regression.
  \item $s(\fx) = \Phi(\fx)$ for probit regression with $\Phi$ the CDF of the standard normal distribution.
\end{itemize}

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Example: CWB parameter convergence}

The following figure shows the parameter values for $m \leq 10000$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/compboost-to-glm-iter10000.png}
\end{center}

Throughout the fitting of CWB, the parameter estimated converges to the GLM solution. The used data set is \href{https://github.com/topepo/AmesHousing}{Ames Housing}.

\end{vbframe}


\endlecture
\end{document}
