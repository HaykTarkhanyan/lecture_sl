\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\usepackage{dsfont}
\usepackage{transparent}

\newcommand{\titlefigure}{figure/compboost-illustration-2.png}
\newcommand{\learninggoals}{
  \item Concept of of CWB and relation to GLM
  \item Built-in feature selection process
  \item Fair base learner selection
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Componentwise Gradient Boosting}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise gradient boosting}

GB (with trees), has strong predictive
performance but is difficult to interpret unless the base learners are stumps.

\lz

The aim of CWB is to find a model that:

\begin{itemize}
  \item
    has strong predictive performance,

  \item
    has components that are still interpretable,

  \item
    does automatic selection of components,

  \item
    is sparser than a model fitted with maximum-likelihood estimation.
\end{itemize}

\lz

This is achieved by using \enquote{nice} base learners which yield familiar
statistical models
in the end.

\lz

Because of this, CWB is also often referred to as \textbf{model-based boosting}.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Base learners}

In GB only one kind of base learner $\mathcal{B}$ is used, e.g., regression trees.

\lz

For CWB we generalize this to multiple base learner sets $\{ \mathcal{B}_1, ... \mathcal{B}_J \}$ with associated parameter spaces
$\{ \bm{\Theta}_1, ... \bm{\Theta}_J \}$,
% $$
%   % b_j^{[m]}(\xv,\pmb\theta^{[m]}) \quad j = 1,\dots, J\,,
%   \{ \bmm_j(\xv, \thetam): j = 1, 2, \dots, J \},
% $$
%
 where $j \in \{ 1, 2, \dots, J \}$ indexes the type of base learner.

\vspace*{0.2cm}

\[
\left\{
\begin{array}{c}
\includegraphics[width=2cm]{figure/boosting-cwb-bl1.png},
\includegraphics[width=2cm]{figure/boosting-cwb-bl2.png},
\cdots, 
\includegraphics[width=2cm]{figure/boosting-cwb-bl3.png},
\includegraphics[width=2cm]{figure/boosting-cwb-bl4.png}
\end{array}
\right\}
\]
\vspace*{0.2cm}

Different from GB, in each iteration multiple base learners 
$b_j\in\mathcal{B}_j$, $j = 1, \dots, J$, are fit to the 
pseudo residuals $\rmm$ and only the best-fitting one
% $\bmm_{\hat{j}}(\xv, \thetam)$
is selected and updated.


\framebreak

Common examples of base learners are

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-linear.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  linear effect
\end{minipage}

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-spline.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  non-linear (spline) effect
\end{minipage}

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-ridge.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  dummy encoded linear model of a categorical feature
\end{minipage}

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-tensor.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  product tensor of two base learners for interaction modelling (e.g. two splines)
\end{minipage}

\vspace{\baselineskip}

More advanced base learners could also be  Markov random fields, random effects, or trees.

\framebreak

We restrict these base learners to additive models, i.e., having a base learner $ b_j(\xv, \thetab^{[1]})$ and another base learner $b_j$ of the same type but with a different parameter vector $\thetab^{[2]}$, then it is possible to combine them to a new base learner of the same type:

$$
 b_j(\xv, \thetab^{[1]}) + b_j(\xv, \thetab^{[2]}) =
 b_j(\xv, \thetab^{[1]} + \thetab^{[2]}).
$$
\begin{center}
\begin{minipage}{.25\linewidth}
\includegraphics{figure/boosting-cwb-bl-add1.pdf}
\end{minipage}
\begin{minipage}{.05\linewidth}
\vspace*{-0.3cm}
$\bm{+}$
\end{minipage}
\begin{minipage}{.25\linewidth}
\includegraphics{figure/boosting-cwb-bl-add2.pdf} 
\end{minipage}
\begin{minipage}{.05\linewidth}
\vspace*{-0.3cm}
$\bm{=}$
\end{minipage}
\begin{minipage}{.25\linewidth}
\includegraphics{figure/boosting-cwb-bl-add3.pdf}
\end{minipage}
\end{center}
\vspace*{0.1cm}

Thus, if $\{ b_j(\xv, \thetab^{[1]}), b_j(\xv, \thetab^{[2]}) \} \in \mathcal{B}_j$, then $b_j(\xv, \thetab^{[1]} + \thetab^{[2]}) \in \mathcal{B}_j$.

%\lz
%
%Often base learners are not defined on the entire feature vector $\xv$ but on
%a single feature $x_j$:
%
%$$
%  b_j(x_j, \theta) \quad \text{for } j = 1, 2, \dots, p.
%$$
%
%This directly incorporates a variable selection mechanism into the fitting
%process, since in each iteration only the best base learner is selected in
%combination with the associated feature, and each base learner can be
%(substantially) more complex than a stump (e.g., univariate linear effects or
%splines).

\end{vbframe}


\begin{vbframe}{Feature selection in CWB}
Often base learners are not defined on the entire feature vector $\xv$ but on
a single feature $x_j$:

$$
  b_j(x_j, \theta) \quad \text{for } j = 1, 2, \dots, p.
$$
Hence, CWB naturally incorporates variable selection:
\begin{itemize}
    \item Only the best base learner with the associated feature is selected
    \item Due to the iterative nature of CWB, this is repeated $M$ times to obtain a selection of $M$ base learners by $j^{[1]}, \dots, j^{[m]}$ with $j^{[M]}\in\{1, \dots, p\}$
    \item The number $\sum_{m=1}^M \mathds{1}_{[j = j^{[m]}]}$ tells us how often the feature $x_j$ in $b_j$ was selected
\end{itemize}
%The iterative nature of CWB and the additivity of the base learners 
%directly incorporates a variable selection mechanism into the fitting
%process, since in each iteration only the best base learner is selected in
%combination with the associated feature, and each base learner can be
%(substantially) more complex than a stump (e.g., univariate linear effects or 
%splines).
\end{vbframe}



% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise boosting algorithm}
\definecolor{algocol}{rgb}{0, 0.125, 0.376}
\input{algorithms/componentwise_gradient_boosting.tex}

({\color{lightgray} Same as for GB, \color{algocol} New inner loop for CWB})

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise boosting algorithm}

\input{tex/cwb-algo-short.tex}


{\footnotesize Iteration $m$, $j = 1$, $\sum  \limits_{i=1}^n (\rmi - \hat{b}_1(x^{(i)}_1, \thetamh_1))^2 = 24.4$: \phantom{$\Rightarrow$ $j^{[m]} = 1$}}
\begin{center}
\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl1-points.png}
\hspace*{0.5cm}
{\transparent{0.3}\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl2-points.png}}
\hspace*{0.5cm}
{\transparent{0.3}\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl3-points.png}}
\end{center}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise boosting algorithm}

\input{tex/cwb-algo-short.tex}

{\footnotesize Iteration $m$, $j = 2$, $\sum  \limits_{i=1}^n (\rmi - \hat{b}_2(x^{(i)}_2, \thetamh_2))^2 = 43.2$: \phantom{$\Rightarrow$ $j^{[m]} = 1$}}
\begin{center}
{\transparent{0.3}\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl1.png}}
\hspace*{0.5cm}
\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl2-points.png}
\hspace*{0.5cm}
{\transparent{0.3}\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl3-points.png}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise boosting algorithm}

\input{tex/cwb-algo-short.tex}

{\footnotesize Iteration $m$, $j = 3$, $\sum  \limits_{i=1}^n (\rmi - \hat{b}_3(x^{(i)}_3, \thetamh_3))^2 = 35.2$: \phantom{$\Rightarrow$ $j^{[m]} = 1$}}
\begin{center}
{\transparent{0.3}\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl1-points.png}}
\hspace*{0.5cm}
{\transparent{0.3}\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl2-points.png}} 
\hspace*{0.5cm}
\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl3-points.png}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise boosting algorithm}

\input{tex/cwb-algo-short.tex}

{\footnotesize Iteration $m$: $\Rightarrow$ $j^{[m]} = 1$}
\begin{center}
\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl1-points.png}
\hspace*{0.5cm}
\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl2-points.png} 
\hspace*{0.5cm}
\includegraphics[width=0.25\textwidth]{figure/boosting-cwb-bl3-points.png}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}
% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------

\begin{vbframe}{Handling of categorical features}

Suppose a feature $x_j \in \{1, \dots, G\}$.
\begin{itemize}
  \item Two common options for encoding the categorical feature $x_j$ in a base learner $b_j$ are:
  \begin{itemize}
    \item 
        One base learner to simultaneously estimate all categories: 
        $$b_j(x_j | \thetab_j) = \sum_{g=1}^G \theta_{j,g}\mathds{1}_{\{g = x_j\}} = (\mathds{1}_{\{x_j = 1\}}, ..., \mathds{1}_{\{x_j = G\}}) \thetab_j$$
        Hence, $b_j$ incorporates a one-hot encoded feature with group means $\thetab\in\R^G$ as estimates. 
    
    \item 
        One binary base learner per category:
        $$b_{j,g}(x_j | \theta_{j,g}) = \theta_{j,g}\mathds{1}_{\{g = x_j\}}$$  
        Including all categories of the feature means adding $G$ base learners $b_{j,1}, \dots, b_{j,G}$ with each accounting for one specific class.
  \end{itemize}
  %\item The \texttt{compboost} package currently implements the first variant.
\end{itemize}

% ------------------------------------------------------------------------------
\framebreak

Advantages within CWB of simultaneously handling all categories: 
\begin{itemize}
    \item 
        Much faster estimation process compared to adding the categories as individual binary base learners.

    \item 
        Explicit solution of $\thetabh = \argmin_{\thetab\in\R^G}\sumin (\yi - b_j(x^{(i)}_j | \thetab))^2$ with:
        $$\thetabh = (\thetah_1, \dots, \thetah_G)^T,\ \thetah_g = n_g^{-1}\sumin \yi \mathds{1}_{\{x^{(i)}_j = g\}}$$

    \item 
        Adding regularization is easy by reframing the base learner as a linear model and using, e.g., Ridge regression for estimating $\thetabh$.  
\end{itemize}

Disadvantage within CWB of simultaneously handling all categories: 
\begin{itemize}
    \item 
        If class $g^\ast\in\{1, \dots, G\}$ is not informative, the respective parameter estimate $\thetah_{g^\ast}$ is (almost sure) not $0$.
\end{itemize}

% ------------------------------------------------------------------------------
\framebreak

Advantages within CWB of including categories individually: 
\begin{itemize}
    \item 
        A finer selection within the feature is possible since non-informative categories are simply not included in the model and hence have an effect of $0$.

    \item 
        Explicit solution of $\thetah_{j,g} = \argmin_{\theta\in\R}\sumin (\yi - b_g(x^{(i)}_j|\theta))^2$ with:
        $$\thetah_{j,g} = n_g^{-1}\sumin \yi \mathds{1}_{\{x^{(i)}_j = g\}}$$
\end{itemize}

Disadvantage within CWB of simultaneously handling all categories:
\begin{itemize}
    \item 
        Fitting CWB is much slower due to the overhead in each iteration (updating the model, pseudo residuals, and doing the base learner selection) after conducting one tiny step into the direction of the 1-dim parameter $\thetah_{j,g}$.

    \item 
        The base learner has exactly one degree of freedom which makes penalization and a fair selection very hard if not impossible.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{intercept handling}

\begin{itemize}
  \item The loss-optimal constant $\fm[0](\xv)$ is an initial model intercept.
  %\item An intercept is often referred to as part of a model which contains information independent of the features.
  \item Suppose linear base learners $b_j(\xv) = \theta_{j1} + \theta_{j2} x_j$ with one intercept $\theta_{j1}$ per base learner and slope $\theta_{j2}$.
  \item If base learner $\hat{b}_j$ with parameter estimates $\thetamh[1] = (\hat{\theta}_{j1}^{[1]}, \hat{\theta}_{j1}^{[1]})$ is selected in the first iteration consequently updates the model intercept to $\fm[0](\xv) + \hat{\theta}_{j1}^{[1]}$.
  \item Throughout the fitting process, the intercept is adjusted $M$ times to its final form:
    $$
    \fm[0](\xv) + \sum\limits_{m=1}^M \hat{\theta}^{[m]}_{j^{[m]}1}
    $$
\end{itemize}

% ------------------------------------------------------------------------------
\framebreak

Two possible options to handle the intercept in CWB are:

\begin{itemize}

\item Include an intercept base learner:
  \begin{itemize}
    \item Add base learner $b_{\text{int}} = \theta$ as potential candidate considered in each iteration.
    \item At the same time, remove the intercept from all linear base learners to only use $b_j(\xv) = \theta_j x_j$.
    \item The final intercept is given by $\fm[0](\xv) + \hat{\theta}$.
  \end{itemize}
  \item Include an intercept in each linear base learner $b_j(\xv) = \theta_{j1} + \theta_{j2} x_j$ and accumulate all intercepts to one global intercept after the fitting.

\end{itemize}

% ------------------------------------------------------------------------------

\framebreak

The following figure shows a comparison of the parameter updates with different intercept handlings:
\begin{center}
% Recreate figure: rsrc/fig-cwb-intercept-handling.R
\includegraphics[width = \textwidth]{figure/compboost-intercept-handling.png}
\end{center}
The parameter estimates converge to the same value. The used data set is \href{https://github.com/topepo/AmesHousing}{Ames Housing}.


\end{vbframe}

%\input{tex/cwb-bl-sel}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Life expectancy}

Consider the \texttt{life expectancy} (data set is provided by the WHO and is available on Kaggle: \url{https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who}) regression task, for which we fit a
CWB model with linear base learners (with intercept) to predict the life expectancy:

\begin{table}
\scriptsize
\begin{tabular}{l|l}
    \textbf{variable} & \textbf{description} \\
    \hline
    \texttt{Life.expectancy} & Life expectancy in years \\	
    \hline
    \texttt{Country}         & The country (just a selection GER, USE, SWE, ZAF, and ETH\\
    \texttt{Year}            & The recorded year\\
    \texttt{BMI}             & Average $\text{BMI} = \frac{\text{body weight in kg}}{(\text{Height in m})^2}$ in a year and country\\
    \texttt{Adult.Mortality} & Adult Mortality Rates of both sexes (probability of dying between 15 \\
                             & and 60 years per 1000 population)
\end{tabular}
\end{table}
Using \texttt{compboost} with $M = 150$ iterations, we can visualize which base learner was selected when and how the estimated feature effects evolve over time.
\end{vbframe}

% tex file and figures are created automatically by: rsrc/fig-cwb-anim.R
\input{tex/fig-cwb-anim}

%%%%% BLIND OUT, include with removing `\if1` and  \fi
\if1

\begin{vbframe}{example: boston housing}

\begin{minipage}[c]{0.4\textwidth}
  \small
  \raggedright
  Consider the \texttt{Boston housing} regression task, for which we fit a
  CWB model with linear base learners (with intercept) to predict median home
  value.
  Using \texttt{compboost} with $M = 100$ iterations, we can
  visualize which base learner was selected when:
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.55\textwidth}
  \tiny
  \begin{tabular}{l|l}
    \textbf{variable} & \textbf{description} \\
    \hline
    medv &	median value of owner-occupied homes in k USD \\
    \hline
    crim & per capita crime rate by town \\
    zn &	proportion of residential land zoned for lots $>$ 25k sq.ft \\
    indus &	proportion of non-retail business acres per town \\
    chas &	Charles River dummy (1 if tract bounds river, 0 otherwise) \\
    nox &	nitric oxides concentration (parts per 10m) \\
    rm &	average number of rooms per dwelling \\
    age &	proportion of owner-occupied units built prior to 1940 \\
    dis &	weighted distances to five Boston employment centres \\
    rad &	index of accessibility to radial highways \\
    tax &	full-value property-tax rate per USD 10k \\
    ptratio &	pupil-teacher ratio by town \\
    b &	$1000(B - 0.63)^2$,  $B$ as proportion of blacks by town \\
    lstat &	percentage of lower status of the population \\
  \end{tabular}
\end{minipage}%

\vfill

\begin{center}
\includegraphics[width = \textwidth]{figure/compboost-illustration-1.png}
\end{center}

\framebreak

% ------------------------------------------------------------------------------

The number of features effectively included in the final model depends on the
number of total iterations $M$.

\vfill

$\rightarrow$ A sparse linear regression is fitted.

\vfill

\includegraphics[width = \textwidth]{figure/compboost-illustration-2.png}

\end{vbframe}


\fi

\endlecture
\end{document}
