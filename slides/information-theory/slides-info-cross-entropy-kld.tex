\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/binary-ce.jpg}
\newcommand{\learninggoals}{
  \item Know the cross-entropy 
  \item Understand the connection between entropy, cross-entropy, and KL divergence
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Cross-Entropy and KL}
\lecture{Introduction to Machine Learning}


\begin{vbframe} {Cross-Entropy - Discrete Case}

\textbf{Cross-entropy} measures the average amount of information required to represent an event from one distribution $p$ using a predictive scheme based on another distribution $q$ (assume they have the same domain $\Xspace$ as in KL).
  $$ H(p \| q) = \sum_{x \in \Xspace} p(x) \log\left(\frac{1}{q(x)}\right) = - \sum_{x \in \Xspace} p(x) \log\left(q(x)\right) = - \mathbb{E}_{X\sim p}[\log(q(X))]$$

For now, we accept the formula as-is. More on the underlying intuition follows in the concent on inf. theory for ML and sourcecoding.
\begin{itemize}
\setlength{\itemsep}{0.9em}
\item Entropy = Avg. amount of information if we optimally encode $p$
\item Cross-Entropy = Avg. amount of information if we suboptimally encode $p$ with $q$
\item $DL_ {KL}(p \| q)$: Difference between the two
\item $H(p \| q)$ sometimes also denoted as $H_{q}(p)$ to set it apart from KL
\end{itemize}

\framebreak

We can summarize this also through this identity: 
\lz
$$
H(p \| q) = H(p) + D_{KL}(p \| q)
$$
This is because: 
\begin{eqnarray*}
H(p) + D_{KL}(p \| q) &=& - \sum_{x \in \Xspace} p(x) \log p(x) + \sum_{x \in \Xspace} p(x) \log \frac{p(x)}{q(x)} \\
                      &=& \sum_{x \in \Xspace} p(x) (-\log p(x) +  \log p(x) - \log q(x)) \\
&=& - \sum_{x \in \Xspace} p(x) \log q(x) = H(p \| q) \\
\end{eqnarray*}
   
\framebreak
\end{vbframe}

\begin{vbframe} {Cross-Entropy - Continuous Case}

For continuous density functions $p(x)$ and $q(x)$: 

$$ H(p \| q) = \int p(x) \log\left(\frac{1}{q(x)}\right) dx = - \int p(x) \log\left(q(x)\right) dx = - \mathbb{E}_{X \sim p}[\log(q(X))]$$

\begin{itemize}
\item It is not symmetric.
\item As for the discrete case, $H(p \| q) = h(p) + D_{KL}(p \| q)$ holds.
\item Can now become negative, as the $h(p)$ can be negative! 
\end{itemize}
\end{vbframe}
 
\begin{vbframe}{Proof: Maximum of Differential Entropy}
  \textbf{Claim}: For a given variance, the continuous distribution that maximizes differential entropy is the Gaussian.

  \lz

  \textbf{Proof}: Let $g(x)$ be a Gaussian with mean $\mu$ and variance $\sigma^2$ and $f(x)$ an arbitrary density function with the same variance. Since differential entropy is translation invariant, we can assume $f(x)$ and $g(x)$ have the same mean.

  \lz
  
  The KL divergence (which is non-negative) between $f(x)$ and $g(x)$ is:
  \begin{equation}
    \begin{aligned}
       0 \leq D_{KL}(f \| g) & = -h(f) + H_g(f) \\
                             & =-h(f)-\int_{-\infty}^{\infty} f(x) \log (g(x)) dx
    \end{aligned}
  \end{equation}
  
  \framebreak
  
The second term in (1) is, 
  
\begin{footnotesize}
\begin{eqnarray}
& & \int_{-\infty}^{\infty} f(x) \log (g(x)) d x =\int_{-\infty}^{\infty} f(x) \log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}\right) d x \nonumber\\ 
& &=\int_{-\infty}^{\infty} f(x) \log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right) d x+\log (e) \int_{-\infty}^{\infty} f(x)\left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) d x \nonumber\\ 
& &= -\frac{1}{2} \log \left(2 \pi \sigma^{2}\right)-\log (e) \frac{\sigma^{2}}{2 \sigma^{2}} = -\frac{1}{2} (\log \left(2 \pi \sigma^{2}\right)+\log (e) ) \nonumber\\ 
& &=-\frac{1}{2} \log \left(2 \pi e \sigma^{2}\right) = -h(g) \,,
\end{eqnarray}
\end{footnotesize}

where the last equality follows from the normal distribution example of the entropy chapter. Combining (1) and (2) results in
$$h(g) - h(f) \geq 0$$
with equality when $f(x) = g(x)$ (following from the properties of Kullback-Leibler divergence).
\end{vbframe}


% \begin{vbframe} {Cross-Entropy - Summary}
%   \begin{itemize}
%     \item For discrete probability distributions $p(x)$ and $q(x)$, the cross-entropy is: 
%      $$ H_p(q) = \sum_{x \in \Xspace} q(x) \log_2\left(\frac{1}{p(x)}\right) = - \sum_{x \in \Xspace} q(x) log_2(p(x))$$
%      \item For probability densities $p(x)$ and $q(x)$, it is: 
%      $$ H_p(q) = \int_{\Xspace} q(x) \ln\left(\frac{1}{p(x)}\right) dx = - \int_{\Xspace} q(x) \ln\left(p(x)\right) dx $$
%     \item It is not symmetric: $ H_p(q) \neq H(p \| q)$.
%     \item Relationship to KL divergence:
%       \begin{align*}
%         H(p \| q) &= H(p) + D_{KL}(p \| q) \\
%         H_p(q) &= H(q) + D_{KL}(q \| p)
%       \end{align*}
%     \item It is non-negative. If the two distributions are the same, cross-entropy equals entropy and KL divergence is zero. 
%     % \item Often used as a loss function for classification tasks. 
%   \end{itemize}

% \end{vbframe}
\endlecture
\end{document}
