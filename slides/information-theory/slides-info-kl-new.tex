\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/kl_log_diff_plot.png}
\newcommand{\learninggoals}{
  \item Know the KL divergence as distance between distributions
  \item Understand KL as expected log-difference 
  \item Understand how KL can be used as loss
  \item Understand that KL is equivalent to the expected likelihood ratio
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}



\lecturechapter{Kullback-Leibler Divergence}
\lecture{Introduction to Machine Learning}

\begin{vbframe} {Measuring Density Similarity in ML}
\begin{itemize}
    \item Information theory provides tools and concepts that can be used to quantify the similarity between probability distributions
    \begin{tikzpicture}
  % Define parameters for the first Gaussian curve
  \def\muA{0}
  \def\sigmaA{1}
  \def\scaleA{1.3}
  
  % Define parameters for the second Gaussian curve
  \def\muB{4}
  \def\sigmaB{1}
  \def\scaleB{1.3}
  
  % Plot the first Gaussian curve
  \draw[domain=-3:3, smooth, samples=100, variable=\x, blue] plot ({\x}, {\scaleA*exp(-(\x-\muA)^2/(2*\sigmaA^2))});
  
  % Plot the second Gaussian curve
  \draw[domain=1:7, smooth, samples=100, variable=\x, red] plot ({\x}, {\scaleB*exp(-(\x-\muB)^2/(2*\sigmaB^2))});
  
  % Add a question mark symbol above the curves
  \node at (2, 1.5) {?};
\end{tikzpicture}
\item In ML, measuring (and maximizing) the similarity between probability distributions is a ubiquitous concept, which will be shown in the following.
\end{itemize}
\framebreak
\begin{itemize}
    \item \textbf{Probabilistic model fitting}\\
Assume our learner is probabilistic, i.e., we model $p(y| \mathbf{x})$ for example (for example, ridge regression, logistic regression, ...).

\lz 

TODO: picture

\lz 

We want to minimize the difference between $p(y \vert \mathbf{x})$ and the conditional data generating process $\mathbb{P}_{y\vert\mathbf{x}}$ based on the data stemming from $\mathbb{P}_{y, \mathbf{x}}.$

\lz

Many losses can be derived this way. (e.g. cross-entropy loss)

\end{itemize}

\framebreak

\begin{itemize}
    \item \textbf{Feature selection}
In feature selection, we want to select features that strongly depend on our target. 

\lz 

TODO: picture

\lz 

We can measure dependency by measuring the similarity between $p(\mathbf{x}, y)$ and $p(\mathbf{x})\cdot p(y).$ (We will later see that this leads to the concept of mutual information)

\end{itemize}

\framebreak

\begin{itemize}
    \item \textbf{Variational inference (VI)}
Our data can also induce probability distributions: By Bayes' theorem it holds that the posterior density $$p(\bm{\theta}\vert \mathbf{X}, \mathbf{y}) = \frac{p(\mathbf{y}|\mathbf{X}, \bm{\theta})p(\bm{\theta})}{\int p(\mathbf{y}|\mathbf{X}, \bm{\theta})p(\bm{\theta})d\bm{\theta}}.$$ However, computing this density analytically is usually intractable.

\lz 

TODO: picture

\lz 

In VI, we want to fit a density $q_{\bm{\phi}}$ with parameters $\bm{\phi}$ to 
    $p(\bm{\theta}\vert \mathbf{X}, \mathbf{y}).$

\lz 

This scenario fundamentally differs from the previous ones because we can now generate samples.

\end{itemize}

\end{vbframe}

\begin{vbframe}{Constructing the KL divergence}
 
\end{vbframe}

\endlecture
\end{document}
