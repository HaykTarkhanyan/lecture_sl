\begin{enumerate}

  \item
    By definition, the entropy of a \textit{Bernoulli} random variable is :
  
    \begin{align*}
      H(\epsilon) &= - \sum_{\epsilon \in \Omega_\epsilon}  \P (\epsilon) log (\P(\epsilon)) \\
      &= - \theta_\epsilon log(\theta_\epsilon) - (1-\theta_\epsilon) log(1-\theta_\epsilon)
    \end{align*}
  \lz
  
  \item 
    The conditional entropy quantifies the uncertainty of $y$ if the outcome of $x$ is given. It is defined as the expected value of the entropies of the conditional distributions, averaged over the conditioning random variable.
    
    \begin{align*}
      H(y|x) &= \E_x [H(y|x=x)] = \sum_{x \in \Omega_x } \P(x=x) H(y|x=x) \\
      &= \theta_x H(y|x=1) + (1-\theta_x) H(y|x=0)
    \end{align*}
  

  Let's think about what happens with $y$ when the values of $x$ are given.
  
  \begin{itemize}
    \item
      If $x=1$, the maximum between $\epsilon$ and $x$ will always be 1 and $y$ will always be 1. As there is no uncertainty, the conditional entropy is 0.
    \item
      If $x=0$, the maximum between $\epsilon$ and $x$ has uncertainty, as it can be 0 or 1. The uncertainty is given only by $\epsilon$, because $x$ is not random in the conditional entropy.
  \end{itemize}
  
  \begin{align*}
      H(y|x) &= \theta_x \underbrace{H(y|x=1)}_{=0} + (1-\theta_x) \underbrace{H(y|x=0)}_{= H(\epsilon)} \\
      &= (1-\theta_x) H(\epsilon)
  \end{align*}
  
  \item
  
  Using the chain rule for entropy:
  
  \begin{align*}
    H(y,x) &= H(x) + H(y|x) \\
    &=  - \theta_x log(\theta_x) - (1-\theta_x) ( log(1-\theta_x) + H(\epsilon))
  \end{align*}
  
  
  
  \item
  
  Now $\epsilon$ has a deterministic relation with $x$. Let's see how our results change with this modification.
  
  \begin{itemize}
    \item
      As $\epsilon$ and $x$ have a deterministic relation, the uncertainty introduced by the $\psi$ function is 0. The entropy of $\epsilon$ is given only by the entropy of $x$
      \begin{align*}
        H(\epsilon) = H(x)
      \end{align*}
      
    \item
      $y$ is a function of $x$ and $\epsilon$, and now $\epsilon$ has a deterministic relation with $x$. Accordingly, we can conclude that $y$ is now a function of $x$.
      $$ y = 2  \ max  \{ x, \psi(x) \} -1 $$
      If the value of $x$ is given, there is no uncertainty associated.
      $$ H(y|x) = 0 $$
      
    \item
    
    Using the chain rule for the entropy:
    
    \begin{align*}
      H(y,x) &= H(x) + \underbrace{H(y|x)}_{=0}  \\
      &= H(x)
    \end{align*}
    
    This result is expected. As $x$ and $y$ have a deterministic relation, the uncertainty is only given by $x$.
  
  \end{itemize}

\end{enumerate}