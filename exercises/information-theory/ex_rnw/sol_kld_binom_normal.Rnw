\begin{enumerate}
\item Let $f$ be the pmf of the $Bin(n,p)$ distribution and $q$ the density of the $\mathcal{N}(\mu, \sigma^2)$. Recall for $X \sim Bin(n,p)$, we have $\mathbb{E}_{f}[X]=np$ and $\text{Var}_{f}[X]=np(1-p)$. We will first derive a simplified expression for the KL divergence and collect all terms that do not involve $\mu$ or $\sigma^2$ in a constant.
\begin{enumerate}
\item[]
        \begin{equation*}
            \begin{aligned}
                D_{KL}(f||q) &= \mathbb{E}_{f}\left[\log \frac{f(X)}{q(X,\theta)}\right] = \underbrace{\mathbb{E}_{f}[\log f(X)]}_{c_1} -\mathbb{E}_{f}[\log q(X|\theta)]\\\ 
                &= c_1 - \mathbb{E}_{f}\left[\log \left(\frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{1}{2 \sigma^2}(X-\mu)^2\right) \right) \right]\\\
                &= c_1 -\mathbb{E}_{f}\big[-\underbrace{\frac{1}{2}\log(2\pi)}_{c_2} -\frac{1}{2} \log(\sigma^2) - \frac{1}{2\sigma^2}(X-\mu)^2 \big]\\\
                &= \underbrace{c_1 + c_2}_{c_3} + \frac{1}{2} \log(\sigma^2) + \frac{1}{2 \sigma^2} \mathbb{E}_{f}\left[(X-\mu)^2 \right] \\\
                &= c_3 + \frac{1}{2} \log(\sigma^2) + \frac{1}{2 \sigma^2} \Big( \underbrace{\mathbb{E}_{f}[X^2]}_{\text{Var}_{f}[X]+\mathbb{E}_{f}[X]^2} -2\mu \mathbb{E}_{f}[X] + \mu^2 \Big) \\\
                &= c_3 + \frac{1}{2} \log(\sigma^2) + \frac{1}{2 \sigma^2} \big(np(1-p) +(np)^2-2\mu np + \mu^2 \big)\\\
                &= c_3 + \frac{1}{2} \log(\sigma^2) + \frac{1}{2 \sigma^2} \big(np(1-p) + (np-\mu)^2 \big)\\\
            \end{aligned}
        \end{equation*}
\item To obtain the gradient of the KLD with respect to the parameter vector $\bm{\theta}=(\mu,\sigma^2)$, we compute the partial derivatives with respect to the scalar components $\mu$ and $\sigma^2$.
\begin{itemize}
\item[] 
\begin{equation} \label{eq:I}
       \frac{\partial D_{KL}(f||q)}{\partial \mu} = \frac{\partial}{\partial \mu} \frac{(np-\mu)^2}{2\sigma^2}  = \frac{2(np-\mu)\cdot(-1)}{2\sigma^2} = \frac{\mu-np}{\sigma^2}
\end{equation}

\item[]
\begin{equation} \label{eq:II}
    \begin{aligned}
        \frac{\partial D_{KL}(f||q)}{\partial \sigma^2} &= \frac{\partial}{\partial \sigma^2} \left(\frac{1}{2} \log(\sigma^2) + \frac{1}{2 \sigma^2} \big(np(1-p) + (np-\mu)^2 \big) \right) \\\
        &= \frac{1}{2\sigma^2} - \frac{1}{2(\sigma^2)^2} \big(np(1-p) + (np-\mu)^2 \big) \\\
        &= \frac{1}{2\sigma^2} - \frac{1}{2\sigma^4} \mathbb{E}_{f}\left[(X-\mu)^2 \right]
    \end{aligned}
\end{equation}

\end{itemize}

\item Yes, there is. By setting the partial derivatives from before to zero we can identify the critical points. First, we set (\ref{eq:I}) to zero and get: 

$$ \frac{\partial D_{KL}(f||q)}{\partial \mu} \stackrel{!}{=} 0 \iff \frac{\hat{\mu}-np}{\hat{\sigma}^2} = 0 \iff \hat{\mu} = np = \mathbb{E}_{f}[X]$$

Next, we set equation (\ref{eq:II}) to zero and plug in our solution for $\hat{\mu}$:

\begin{equation*}
    \begin{aligned}
        \frac{\partial D_{KL}(f||q)}{\partial \sigma^2} \stackrel{!}{=} 0 &\iff \frac{1}{2\hat{\sigma}^2} = \frac{1}{2\hat{\sigma}^4} \mathbb{E}_{f}\left[(X-\hat{\mu})^2 \right] \quad \big\rvert \cdot 2\hat{\sigma}^4 \\\
        &\iff \hat{\sigma}^2 = \mathbb{E}_{f}\left[\left(X-\mathbb{E}_{f}[X]\right)^2 \right] = \text{Var}_{f}[X] = np(1-p) 
    \end{aligned}
\end{equation*}

Note that in order to prove that the critical point is a minimum, we would have to check that the Hessian of $D_{KL}(f||q)$ with respect to $(\mu, \sigma^2)$ is positive definite at $(\hat{\mu}, \hat{\sigma}^2)$. 

\item We could, alternatively, use the gradients and do gradient descent to find the optimal $\bm{\theta}$.
\end{enumerate}
\item 
<<fig.height=4, fig.width=7>>=
nr_points = 1000
p = 0.5
n = 100
# create data
X <- rbinom(nr_points, prob = p, size = n)

# define different Normal density functions
normal_optimal <- function(x) dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p)))
normal_shift <- function(x) dnorm(x, mean = n*p - 10, sd = sqrt(n*p*(1-p)))
normal_scale_increase <- function(x) dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p))*2)
normal_right_scale_decrease <- function(x) dnorm(x, mean = n*p + 20, sd = p*(1-p))

hist(X, breaks = 25, xlim = c(10, 100), freq = FALSE)
curve(normal_optimal, from = 10, to = 100, add = TRUE, col = "green")
curve(normal_shift, from = 10, to = 100, add = TRUE, col = "blue")
curve(normal_scale_increase, from = 10, to = 100, add = TRUE, col = "orange")
curve(normal_right_scale_decrease, from = 10, to = 100, add = TRUE, col = "red")
@

For these distributions, we get the following KL divergence values (up to an additive constant):

$$
D_{KL}(f||q) = c_3 + 0.5 \log \sigma^2 + \frac{1}{2\sigma^2} (\text{Var}_f(X) + (np-\mu)^2))
$$

<<>>=
kld_value <- function(mu,sigma2)
{
  0.5*log(sigma2) + 
    0.5 * (sigma2)^(-1) * (n*p*(1-p) + (n*p - mu)^2)
}
(optimal_green <- kld_value(n*p,n*p*(1-p)))
(shift_blue <- kld_value(n*p-10,n*p*(1-p)))
(scale_increase_orange <- kld_value(n*p,n*p*(1-p)*4))
(right_scale_decrease_red <- kld_value(n*p+20, (p*(1-p))^2))
@

\item Since we are now required to calculate the exact KLD values, we would also have to calculate $\mathbb{E}_f(\log f(X))$, which is somewhat more difficult. If you search the internet for a solution ($\rightarrow$ ``entropy of a binomial distribution''), you will find an approximate solution using the de-Moivre-Laplace theorem. Alternatively, we could make use of the central limit theorem, but then we would just approximate $f$ with a normal distribution with $\mu = np$ and $\sigma^2 = np(1-p)$, which would give us a constant KLD of zero (the very same happens if you use the first approximation using the de-Moivre-Laplace-theorem). We here instead will approximate the expectation using a large sample from the true underlying distribution:
$$
D_{KL}(f||q) \approx \frac{1}{B} \sum_{b=1}^B [\log f(X) - \log q(X|\mu = np, \sigma^2 = np(1-p))]
$$
<<warning=FALSE, message=NA, fig.height=4, fig.width=7>>=
p_seq <- seq(0.01, 0.99, l = 100)
n_seq <- seq(10, 500, by = 100)
B <- 10000

kld_value_approx <- function(n,p){ 
  
  # sample a large number of data points from true distribution
  x <- rbinom(B, prob = p, size = n)
  
  # approximate the mean; threshold values to 0 if < 0 due
  # to the approximation
  pmax(
    mean(
      dbinom(x, prob = p, size = n, log = TRUE) - 
        dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p)), log = TRUE),
      na.rm = TRUE
    ),
    0)
  
}

kld_val <- sapply(n_seq, function(this_n) 
  sapply(p_seq, function(this_p) kld_value_approx(this_n, this_p)))

cols = rev(colorRampPalette(c('darkred','red','blue','lightblue'))(50))

filled.contour(x = p_seq, y = n_seq, z = kld_val, 
               xlab = "p", ylab = "n", 
               col = cols
               )
@
\item Based on the previous result, one can see that the KLD is very close to zero but has larger values for very small or very large values of $p$ and / in combination with a small number of experiments $n$. These are exactly the cases where the normal approximation of a binomial distribution does not work so well. 
\end{enumerate}
