\begin{enumerate}
        
    \item 

        First of all, we will use the  fact that $\Xmat$ has orthonormal columns to show that:

        \begin{equation}\label{eq:solve_b}
            \begin{aligned}
                \thetabh &= \left(\underbrace{{\Xmat}^T \Xmat}_{\id}\right)^{-1} \Xmat^T\yv \\
                \thetabh &= \Xmat^T \yv  
            \end{aligned}
        \end{equation}

        We will now use the result of eq. \ref{eq:solve_b} to show that:
        \begin{equation}
            \begin{aligned}
                \argmin_{\thetab}  \riskrt &= \argmin_{\thetab} \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \Xmat^{(i)} \right)^2 + \lambda \sum_{i=1}^p  |\theta_i| \\
                &= \argmin_{\thetab}  \frac12  \| \yv - \Xmat \thetab  \|_2^2 + \lambda \|\thetab\|_1 \\
                &= \argmin_{\thetab} \frac12 ( \yv - \Xmat \thetab)^T ( \yv - \Xmat \thetab) + \lambda \|\thetab\|_1 \\ 
                &= \argmin_{\thetab}  \frac12 \left(\underbrace{\yv^T \yv}_{indep. \  of \  \thetab} - 2 \underbrace{\yv^T \Xmat}_{\thetab^T} \thetab + \thetab^T \underbrace{\Xmat^T \Xmat}_{\id} \thetab \right) + \lambda \|\thetab\|_1 \\
                &= \argmin_{\thetab} - \thetabh^T \thetab  + \frac{\thetab^T \thetab}{2} + \lambda \|\thetab\|_1 \\
                &= \argmin_{\thetab} \sum_{i=1}^p	- \thetah_{i}\theta_i + \frac{\theta_i^2}{2} + \lambda|\theta_i|.
            \end{aligned}
        \end{equation}

    \item

        The advantage of this representation if we are interested in finding $\thetab$ is that we can optimize each $g_i(\theta)$ separately to get optimal entries for $\theta_1,\dots,\theta_p$.

    \item 

        Let's use the hint and compare $g_i(\theta)$ with $g_i(-\theta)$, for the case where $\theta > 0$ :

        \begin{equation} \label{eq:compare_gi}
            \begin{aligned}
                g_i(\theta) &= - \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda|\theta_i| \\
                g_i(-\theta) &= + \thetah_{i}\theta + \frac{(-\theta)^2}{2} + \lambda|\theta_i| \\
            \end{aligned}
        \end{equation}

        We also know that non-positive $\theta$ have always a greater or equal value for $g_i(\theta)$ than than their positive counterpart:
        \begin{equation} \label{eq:compare_gi_2}
            \begin{aligned}
                \theta > 0 \longrightarrow g_i(\theta) \leq g_i(-\theta)
            \end{aligned}
        \end{equation}

        The second and third term of both equations in eq. \ref{eq:compare_gi} are equivalent, so we can ignore them. 
        Accordingly, to comply with the condition from  eq. \ref{eq:compare_gi_2}, the minimizer $\theta_i^*$   must be non-negative.

        \begin{equation}
            \begin{aligned}
                - \thetah_{i}\theta \leq \thetah_{i}\theta \longrightarrow \theta_i^* \geq 0
            \end{aligned}
        \end{equation}
    
    \item 

        To calculate the minimizer $\theta_i^*$, we will derive with respect to the parameter and set the derivative to zero:

        \begin{equation}
            \begin{aligned}
                \frac{\partial g_i(\theta)}{\partial \theta} &= - \thetah_{i} + \theta + \underbrace{\lambda}_{\theta_i > 0} \\
                &= - \thetah_{i} + \theta + \lambda  \stackrel{!}{=} 0  \longrightarrow \theta = \underbrace{\theta_i -\lambda}_{ \geq 0} \\
                \theta_i^* &=  \underbrace{\thetah_i - \lambda}_{ \geq 0} \\
                &= max(0,  \thetah_i  - \lambda) \\
            \end{aligned}
        \end{equation}
    
    \item 

        Let's use the hint again and compare $g_i(\theta)$ with $g_i(-\theta)$, for the case where $\theta < 0$ :

        \begin{equation} \label{eq:compare_gi_negative}
            \begin{aligned}
                g_i(\theta) &= - \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda|\theta_i| \\
                g_i(-\theta) &= + \thetah_{i}\theta + \frac{(-\theta)^2}{2} + \lambda|\theta_i| \\
            \end{aligned}
        \end{equation}

        \begin{equation} \label{eq:compare_gi_negative_2}
            \begin{aligned}
                \theta < 0 \longrightarrow g_i(\theta) \geq g_i(-\theta)
            \end{aligned}
        \end{equation}

        The second and third term in both equations of \ref{eq:compare_gi_negative} are equivalent. Using the condition from eq. \ref{eq:compare_gi_negative_2}, we can conclude that the minimizer $\theta_i^*$ must be non-positive.

        \begin{equation}
            \begin{aligned}
                - \thetah_{i}\theta \geq \thetah_{i}\theta \longrightarrow \theta_i^* \leq 0
            \end{aligned}
        \end{equation}

    \item 

        Calculating the derivative again and setting it to zero, we get:

        \begin{equation} \label{eq:minimizer_theta_less_0}
            \begin{aligned}
                \frac{\partial g_i(\theta)}{\partial \theta} &= - \thetah_{i} + \theta \underbrace{- \lambda}_{\theta_i < 0}  \stackrel{!}{=} 0  \longrightarrow \theta = \underbrace{\theta_i +\lambda}_{ \leq 0} \\
                \theta_i^* &=  \underbrace{\thetah_i + \lambda}_{\leq0} \\
                &= min(0, \thetah_i  + \lambda) \\
            \end{aligned}
        \end{equation}

    \item 

        \begin{equation}
            \begin{aligned}
                \begin{cases}
                    \theta_i > 0 \longrightarrow \theta_i^* = \overbrace{1}^{\sign (\thetah_i)} \cdot \  max(0, \overbrace{\thetah_i}^{= |\thetah_i|} - \lambda) \\
                    \theta_i \leq 0 \longrightarrow \theta_i^* = min(0, \thetah_i + \lambda) 
                        = \underbrace{-1}_{\sign (\thetah_i)} \cdot \  max(0, \overbrace{-\thetah_i}^{ = |\thetah_i|}-\lambda)
                \end{cases}
            \end{aligned}
        \end{equation}

        By transforming the min function from \ref{eq:minimizer_theta_less_0} to a max function, we can combine the two cases in only one expression:

        \begin{equation}
        \begin{aligned}
            \theta_i^* = \sign(\thetah_i) max (|\thetah_i| - \lambda, 0)
        \end{aligned}
        \end{equation}

\end{enumerate}


