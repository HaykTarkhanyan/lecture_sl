\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}}}
\makeatother
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{
\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
}}
\makeatother
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}
	
\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopfsl{4}

\loesung{}{

\begin{enumerate}
\item 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{set.seed}\hlstd{(}\hlnum{42}\hlstd{)}
\hlstd{n} \hlkwb{=} \hlnum{100}
\hlstd{p_add} \hlkwb{=} \hlnum{100}
\hlcom{# create matrix of features}
\hlstd{X} \hlkwb{=} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n} \hlopt{*} \hlstd{(p_add} \hlopt{+} \hlnum{1}\hlstd{)),} \hlkwc{ncol} \hlstd{= p_add} \hlopt{+} \hlnum{1}\hlstd{)}

\hlstd{Y} \hlkwb{=} \hlkwd{sin}\hlstd{(X[,}\hlnum{1}\hlstd{])} \hlopt{+} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{sd} \hlstd{=} \hlnum{0.5}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\item Demonstration of

\begin{itemize}

\item underfitting:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(X[,}\hlnum{1}\hlstd{], Y)}
\hlkwd{points}\hlstd{(}\hlkwd{sort}\hlstd{(X[,}\hlnum{1}\hlstd{]),} \hlkwd{sin}\hlstd{(}\hlkwd{sort}\hlstd{(X[,}\hlnum{1}\hlstd{])),} \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"blue"}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{])),} \hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-4-1} 
\end{knitrout}
\item overfitting:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(X[,}\hlnum{1}\hlstd{], Y)}
\hlstd{sX1} \hlkwb{<-} \hlkwd{sort}\hlstd{(X[,}\hlnum{1}\hlstd{])}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{sin}\hlstd{(sX1),} \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"blue"}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{3}\hlstd{)} \hlopt{+}
                        \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{4}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{5}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{6}\hlstd{)} \hlopt{+}
                        \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{7}\hlstd{)))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-5-1} 
\end{knitrout}

\item $L1$ penalty:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(glmnet)}
\hlkwd{plot}\hlstd{(}\hlkwd{glmnet}\hlstd{(X, Y),} \hlkwc{xvar} \hlstd{=} \hlstr{"lambda"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-6-1} 
\end{knitrout}
\item $L2$ penalty
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{glmnet}\hlstd{(X, Y,} \hlkwc{alpha} \hlstd{=} \hlnum{0}\hlstd{),} \hlkwc{xvar} \hlstd{=} \hlstr{"lambda"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-7-1} 
\end{knitrout}
\item elastic net regularization:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{glmnet}\hlstd{(X, Y,} \hlkwc{alpha} \hlstd{=} \hlnum{0.3}\hlstd{),} \hlkwc{xvar} \hlstd{=} \hlstr{"lambda"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-8-1} 
\end{knitrout}

\item the underdetermined problem:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{try}\hlstd{(ls_estimator} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X),} \hlkwd{crossprod}\hlstd{(X,Y)))}
\end{alltt}
\begin{verbatim}
## Error in solve.default(crossprod(X), crossprod(X, Y)) : 
##   system is computationally singular: reciprocal condition number = 5.84511e-18
\end{verbatim}
\end{kframe}
\end{knitrout}
\item the bias-variance trade-off:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(X[,}\hlnum{1}\hlstd{], Y,} \hlkwc{col}\hlstd{=}\hlkwd{rgb}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0.2}\hlstd{))}
\hlstd{sX1} \hlkwb{<-} \hlkwd{sort}\hlstd{(X[,}\hlnum{1}\hlstd{])}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{sin}\hlstd{(sX1),} \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"blue"}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{2}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{)))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"magenta"}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{3}\hlstd{)))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"orange"}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{3}\hlstd{)} \hlopt{+}
                        \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{4}\hlstd{)))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"purple"}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{3}\hlstd{)} \hlopt{+}
                        \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{4}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{5}\hlstd{)))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"green"}\hlstd{)}
\hlkwd{points}\hlstd{(sX1,} \hlkwd{fitted}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X[,}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{3}\hlstd{)} \hlopt{+}
                        \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{4}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{5}\hlstd{)} \hlopt{+} \hlkwd{I}\hlstd{(X[,}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{6}\hlstd{)))[}\hlkwd{order}\hlstd{(X[,}\hlnum{1}\hlstd{])],}
       \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"brown"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-10-1} 
\end{knitrout}
\item early stopping (use a simple neural network as in Exercise 2):
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(dplyr)}
\hlkwd{library}\hlstd{(keras)}

\hlstd{neural_network} \hlkwb{<-} \hlkwd{keras_model_sequential}\hlstd{()}

\hlstd{neural_network} \hlopt{%>%}
  \hlkwd{layer_dense}\hlstd{(}\hlkwc{units} \hlstd{=} \hlnum{50}\hlstd{,} \hlkwc{activation} \hlstd{=} \hlstr{"relu"}\hlstd{)} \hlopt{%>%}
  \hlkwd{layer_dense}\hlstd{(}\hlkwc{units} \hlstd{=} \hlnum{50}\hlstd{,} \hlkwc{activation} \hlstd{=} \hlstr{"relu"}\hlstd{)} \hlopt{%>%}
  \hlkwd{layer_dense}\hlstd{(}\hlkwc{units} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{activation} \hlstd{=} \hlstr{"relu"}\hlstd{)} \hlopt{%>%}
  \hlkwd{compile}\hlstd{(}
    \hlkwc{optimizer} \hlstd{=} \hlstr{"adam"}\hlstd{,}
    \hlkwc{loss}      \hlstd{=} \hlstr{"mse"}\hlstd{,}
    \hlkwc{metric} \hlstd{=} \hlstr{"mse"}
  \hlstd{)}

\hlstd{history_minibatches} \hlkwb{<-} \hlkwd{fit}\hlstd{(}
  \hlkwc{object}           \hlstd{= neural_network,}
  \hlkwc{x}                \hlstd{= X,}
  \hlkwc{y}                \hlstd{= Y,}
  \hlkwc{batch_size}       \hlstd{=} \hlnum{24}\hlstd{,}
  \hlkwc{epochs}           \hlstd{=} \hlnum{100}\hlstd{,}
  \hlkwc{validation_split} \hlstd{=} \hlnum{0.2}\hlstd{,}
  \hlkwc{callbacks} \hlstd{=} \hlkwd{list}\hlstd{(}\hlkwd{callback_early_stopping}\hlstd{(}\hlkwc{patience} \hlstd{=} \hlnum{50}\hlstd{)),}
  \hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlcom{# set this to TRUE to get console output}
  \hlkwc{view_metrics} \hlstd{=} \hlnum{FALSE} \hlcom{# set this to TRUE to get a dynamic graphic output in RStudio}
\hlstd{)}
\hlkwd{plot}\hlstd{(history_minibatches)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-11-1} 
\end{knitrout}
\end{itemize}
\end{enumerate}
}
\newpage

\loesung{}{

\begin{enumerate}
  \item The Taylor approximation of first order of a function $f(x)$ at point $x_0$ is $$f(x) \approx f(x_0) + f^\prime(x_0)(x-x_0).$$ On the other hand, a differentiable function $f$ is said to be convex on an interval $\mathcal{I}$ if and only if $$f(x) \geq f(x_0) + f^\prime(x_0)(x-x_0)$$ for all points $x,x_0 \in \mathcal{I}$.
  \begin{enumerate}
  \item  If we approximate a convex function with a Taylor approximation of first order, we will always get a lower bound at the given point as the second equation states. 
  \item Visualization of such an approximation for $2x^2$ on $\mathcal{I} = [-2,2]$ (we will only later see how to calculate a derivative(-like) measure for the non-differentiable functions). The approximation in this case is $f(x) \approx 2 x_0^2 + 4 x_0 (x-x_0) = -2x_0^2 + 4 x_0 x$. We can plot this for several values of x:
  \end{enumerate}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{xx} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlopt{-}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{0.01}\hlstd{)}
\hlstd{yy} \hlkwb{<-} \hlnum{2}\hlopt{*}\hlstd{xx}\hlopt{^}\hlnum{2}
\hlcom{# this will give us the approximation function for x=0}
\hlcom{# and what happens if we vary x (its slope)}
\hlcom{# for given x0}
\hlstd{approx_fun} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x0}\hlstd{)} \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{2}\hlopt{*}\hlstd{x0}\hlopt{^}\hlnum{2}\hlstd{,} \hlnum{4}\hlopt{*}\hlstd{x0)}

\hlkwd{plot}\hlstd{(xx, yy,} \hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{"x"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"f(x)"}\hlstd{,} \hlkwc{ylim}\hlstd{=}\hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{4}\hlstd{,}\hlnum{10}\hlstd{),} \hlkwc{col} \hlstd{=}\hlstr{"red"}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{2.5}\hlstd{)}
\hlkwa{for}\hlstd{(x0} \hlkwa{in} \hlkwd{seq}\hlstd{(}\hlopt{-}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{,}\hlkwc{by}\hlstd{=}\hlnum{0.5}\hlstd{))}
  \hlkwd{abline}\hlstd{(}\hlkwd{approx_fun}\hlstd{(x0),} \hlkwc{col} \hlstd{=} \hlkwd{rgb}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0.5}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-12-1} 
\end{knitrout}
  \item A subdifferential of $f$ is a set of values $\breve{\nabla}_{x_0} f$ defined as $$\breve{\nabla}_{x_0} f = \{ g: f(x) \geq f(x_0) + g \cdot (x-x_0) \, \forall x \in \mathcal{I} \}.$$ Every scalar value $g \in \breve{\nabla}_{x_0}$ is said to be a subgradient of $f$.  A subdifferential thus generalizes the idea of a lower approximation from before by replacing $f^\prime(x_0)$ with any constant $g$ for which the approximation is still strictly below the objective function $f$.
  \item We can make use of subdifferentials for convex but non-differentiable loss functions like the one induced by the Lasso, because we are now not restricted to cases where we can compute $f^\prime(x_0)$. It holds that:\\
  \begin{center}
  A point $x_0$ is the global minimum of a convex function $f$ $\Leftrightarrow$ $0$ is contained in the subdifferential $\breve{\nabla}_{x_0} f$.\\
  \end{center}
  We can define a subdifferential at point $x_0$ also as a non-empty interval $[x_l,x_u]$ where the lower and upper limit is defined by $$x_l = \lim_{x \to x_0^{-}} \frac{f(x)-f(x_0)}{x-x_0}, \quad x_u = \lim_{x \to x_0^{+}} \frac{f(x)-f(x_0)}{x-x_0}.$$ These resemble the limits of the derivative $\partial f / \partial x$ evaluated at a point very close to $x_0$ when coming from the left or right side, respectively. 
  \begin{enumerate}
  \item In the case for $f(x) = |x|$, $\lim_{x\to 0^{\pm}} |x|/x = \pm 1$ and thus $\breve{\nabla}_{x_0} f = [-1,1]$ at $x_0 = 0$.
  \item $x_0$ is a global minimum as $0 \in \breve{\nabla}_{x_0} f$
  \item The $L1$ penalty has no derivative at $\theta_k = 0$ for all $\theta_k$ with $k\in \{1,\ldots,p\}$. Thus we are particularly interested in the subdifferential at this point, which is  $$\breve{\nabla}_{\theta_k} \lambda \sum_{j=1}^p |\theta_j| = \sum_{j=1}^p \breve{\nabla}_{\theta_k} \lambda |\theta_j| = \breve{\nabla}_{\theta_k} \lambda |\theta_k| = [-\lambda, \lambda],$$ where in the second equation we use that the subdifferential of a constant function is zero. For a (sub-) gradient at any other differentiable point, we get the conventional gradient using the given hint, which is $-\lambda$ for $\theta_k < 0$ and $\lambda$ for $\theta_k > 0$.
  \end{enumerate}
  \item The subdifferential for the Lasso w.r.t. $\theta_2$ is then simply the combination of the standard gradient for the unregularized risk $\nabla_{emp} := n^{-1} \sum_{i=1}^n -2 x^{(i)}_{2} (y^{(i)} - x^{(i)}_{1}\theta_1 - x^{(i)}_{2}\theta_2)$ plus the subdifferential for the penalty: $$\breve{\nabla}_{\theta_2} \mathcal{R}_{reg} =  
  \begin{cases} 
  \nabla_{emp} - \lambda & \text{if } \theta_2 < 0\\
  [\nabla_{emp} - \lambda, \nabla_{emp}  + \lambda] & \text{if } \theta_2 = 0\\
  \nabla_{emp} + \lambda & \text{if } \theta_2 > 0.
  \end{cases}
  $$
  \end{enumerate}
  % \textbf{Extra}: So far we only have derived the subdifferential for a given $\theta_k$ (or here $\theta_2$) but no update routine yet. Being able to derive a closed form solution in each step, we use this solution to update $\theta_k$ in each step. Here, we set $\breve{\nabla}_{\theta_2} \mathcal{R}_{reg}$ to zero and rewritte the results in terms of $\theta_2$ to get a corresponding update policy. First, note that $$\nabla_{emp} = 2n^{-1} (-c + \sum_i (x_2^{(i)})^2 \theta_2),$$ where $c$ is constant term w.r.t. $\theta_2$ and we know that $\sum_i (x_2^{(i)})^2 > 0$. For the first and the third case from the subdifferential above $\breve{\nabla}_{\theta_2} \mathcal{R}_{reg} = 0$ implies that our updated parameter 
  % $$\theta_2=\begin{cases} 
  % (c + 0.5n\lambda)/ \sum_i (x_2^{(i)})^2 & \text{for } c < - 0.5n\lambda  \\
  % (c - 0.5n\lambda)/ \sum_i (x_2^{(i)})^2 & \text{for } c > 0.5n\lambda.
  % \end{cases}
  % $$
  % For the special second case, setting the interval to zero implies we use $\theta_2 = 0$ if $- 0.5n\lambda \leq c \leq 0.5n\lambda$ holds.



}
\end{document}
