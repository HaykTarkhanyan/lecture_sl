\begin{enumerate}
    \item 
        The initial weights all three points in the dataset is $\frac{1}{3}$. A decision boundary for the first decision stump could be the line $x=2$.
        
        
        <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
            mpoints = list(c(1, 0.1),c(3, 0.1),c(5, 0.1))
            x1 = sapply(mpoints,function(x) x[1])
            x2 = sapply(mpoints,function(x) x[2])
            y = c(-1, 1, -1)
            
            par(mar = c(4,4,1,1), pin = c(3,3))
            plot(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2,
            xlim = c(-1,6), ylim = c(0,1),
            ylab = "x2", xlab = "x1")
			      grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = 1)
            points(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2)
			      text(x1, x2, labels = c("y=-1", "y=-2", "y=-3"), pos = 3, cex = 1)
			      abline(v=2,lty=2,lwd=2,col='red')
			      arrows(2,0.3,3,0.3,col='red')
			      text(x=2.3, y=0.35, label="+",cex=2,col='red')
          @

    \item 

        The first stump makes two correct predictions and one incorrect prediction,
         we can then calculate the  weighted in-sample misclassification rate,
          the weight for the stump and the new data points weights

        
        \begin{equation}
            \begin{aligned}
                \errm[0] &= \sumin \wmi[0] \cdot \mathds{1}_{\{\yi[0] \,\neq\, \blh[0](\xi)\}} = 0.33 \\
                \betamh[0] &= \frac{1}{2} \log \left( \frac{1 - \errm[0]}{\errm[0]}\right) = 0.5 \cdot log \left(\frac{0.66}{0.33} \right)  \approx 0.35 \\
                w^{[1](1)} &= w^{[0](1)} \cdot \exp \left( -\betamh[0] \cdot \yi[1] \cdot \blh[0](\xi[1]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot -1 \cdot -1 \right) \approx 0.23 \\
                w^{[1](2)} &= w^{[0](2)} \cdot \exp \left( -\betamh[0] \cdot \yi[2] \cdot \blh[0](\xi[2]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot 1 \cdot 1 \right) \approx 0.23 \\
                w^{[1](3)} &= w^{[0](3)} \cdot \exp \left( -\betamh[0] \cdot \yi[3] \cdot \blh[0](\xi[3]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot 1 \cdot -1 \right) \approx 0.47 \\ 
            \end{aligned}
        \end{equation}

        We need to normalize the weights so that they sum up to one: 

        \begin{equation}
            \begin{aligned}
                w^{[1](1)} &= \frac{w^{[1](1)}}{\sumin w^{[1](i)}} = \frac{0.23}{0.23 + 0.23 + 0.47} \approx 0.25 \\
                w^{[1](2)} &= \frac{w^{[1](2)}}{\sumin w^{[1](i)}} = \frac{0.23}{0.23 + 0.23 + 0.47} \approx 0.25 \\
                w^{[1](3)} &= \frac{w^{[1](3)}}{\sumin w^{[1](i)}} = \frac{0.47}{0.23 + 0.23 + 0.47} \approx 0.50 \\
            \end{aligned}
        \end{equation}

    \item 

         As the training error is not yet 0, we do a second stump using the new weights, The decision boundary is x=4. 
         The stump makes two correct predictions and one incorrect prediction:

      <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
            mpoints = list(c(1, 0.1),c(3, 0.1),c(5, 0.1))
            x1 = sapply(mpoints,function(x) x[1])
            x2 = sapply(mpoints,function(x) x[2])
            y = c(-1, 1, -1)
            
            par(mar = c(4,4,1,1), pin = c(3,3))
            plot(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2,
            xlim = c(-1,6), ylim = c(0,1),
            ylab = "x2", xlab = "x1")
			      grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = 1)
            points(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2)
			      text(x1, x2, labels = c("y=-1", "y=-2", "y=-3"), pos = 3, cex = 1)
			      abline(v=2,lty=2,lwd=2,col='red')
			      arrows(2,0.3,3,0.3,col='red')
			      text(x=2.3, y=0.35, label="+",cex=2,col='red')
			      abline(v=4,lty=2,lwd=2,col='blue')
			      arrows(4,0.4,3,0.4,col='blue')
			      text(x=3.6, y=0.45, label="+",cex=2,col='blue')
          @

        We calculate the  weighted in-sample misclassification rate and the weight for the stump.

        \begin{equation}
            \begin{aligned}
                \errm[1] &= \sumin \wmi[1] \cdot \mathds{1}_{\{\yi[1] \,\neq\, \blh[1](\xi)\}} = 0.25 \\
                \betamh[1] &= \frac{1}{2} \log \left( \frac{1 - \errm[1]}{\errm[1]}\right) = 0.5 \cdot log \left(\frac{0.75}{0.25} \right)  \approx 0.54 \\
            \end{aligned}
        \end{equation}

        We can see that for the left-most point and the right-most point, there is a disagreement between our two stumps. 
        In the case of the right-most point, the second stump has a bigger weight than the first one, so we will classify it correctly as $y=-1$. 
        Unfortunately, in the case of the left-most point, we will classify it incorrectly as $y=-1$. We will need a third iteration that will correct this mistake.

        <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
            mpoints = list(c(1, 0.1),c(3, 0.1),c(5, 0.1))
            x1 = sapply(mpoints,function(x) x[1])
            x2 = sapply(mpoints,function(x) x[2])
            y = c(-1, 1, -1)
            
            par(mar = c(4,4,1,1), pin = c(3,3))
            plot(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2,
            xlim = c(-1,6), ylim = c(0,1),
            ylab = "x2", xlab = "x1")
			      grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = 1)
            points(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2)
			      text(x1, x2, labels = c("y=-1", "y=-2", "y=-3"), pos = 3, cex = 1)
			      abline(v=2,lty=2,lwd=2,col='red')
			      arrows(2,0.3,3,0.3,col='red')
			      text(x=2.3, y=0.35, label="+",cex=2,col='red')
			      abline(v=4,lty=2,lwd=2,col='blue')
			      arrows(4,0.4,3,0.4,col='blue')
			      text(x=3.6, y=0.45, label="+",cex=2,col='blue')
			      abline(v=0.5,lty=2,lwd=2,col='darkgreen')
			      arrows(0.5,0.4,-0.5,0.4,col='darkgreen')
			      text(x=-0.2, y=0.45, label="+",cex=2,col='darkgreen')
          @


\end{enumerate}