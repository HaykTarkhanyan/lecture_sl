\begin{enumerate}
    \item 
        The initial weights for all three points in the dataset is $\frac{1}{3}$. A decision boundary for the first decision stump could be $x=2$.
        
        
        <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
          # Define x 
          x<-c(1,3,5)
          # Define y
          y = c(-1, 1, -1)
          ## Make y-value=0
          x<-cbind(x,0)
          ## Plotting without box or axis with dot, representing data points                  
          plot(x, col=ifelse(y==1,'red','blue')
                  ,bty='n',xaxt='n',yaxt='n',ylab='',xlab='',pch=16,cex=2)
          # add axis with label to the right
          axis(1, at=1:5, labels=1:5, las=1,pos=-0.12)
          # add axis labels
          mtext("x", side=1, line=-2, cex=1.5)
          # add a legend
          legend("topright", legend=c("y=1","y=-1"), col=c("red","blue"), pch=16, cex=0.7)
          
          # write a segment in x = 2
          segments(2,-0.1,2,0.3,lty=2,lwd=2,col='darkgreen')
          arrows(2,0.1,2.5,0.1,col='darkgreen',lwd=2,length = 0.1)
          text(x=2.3, y=0.25, label="+",cex=1.5,col='darkgreen')
        @

    \item 

        The first stump makes two correct predictions and one incorrect prediction,
         we can then calculate the  weighted in-sample misclassification rate,
          the weight for the stump and the new data points weights.

        
        \begin{equation}
            \begin{aligned}
                \errm[0] &= \sumin \wmi[0] \cdot \mathds{1}_{\{\yi[i] \,\neq\, \blh[0](\xi)\}} = 0.33 \\
                \betamh[0] &= \frac{1}{2} \log \left( \frac{1 - \errm[0]}{\errm[0]}\right) = 0.5 \cdot log \left(\frac{0.67}{0.33} \right)  \approx 0.35 \\
                w^{[1](1)} &= w^{[0](1)} \cdot \exp \left( -\betamh[0] \cdot \yi[1] \cdot \blh[0](\xi[1]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot -1 \cdot -1 \right) \approx 0.23 \\
                w^{[1](2)} &= w^{[0](2)} \cdot \exp \left( -\betamh[0] \cdot \yi[2] \cdot \blh[0](\xi[2]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot 1 \cdot 1 \right) \approx 0.23 \\
                w^{[1](3)} &= w^{[0](3)} \cdot \exp \left( -\betamh[0] \cdot \yi[3] \cdot \blh[0](\xi[3]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot 1 \cdot -1 \right) \approx 0.47 \\ 
            \end{aligned}
        \end{equation}

        We need to normalize the weights so that they sum up to one: 

        \begin{equation}
            \begin{aligned}
                w^{[1](1)} &= \frac{w^{[1](1)}}{\sumin w^{[1](i)}} = \frac{0.23}{0.23 + 0.23 + 0.47} \approx 0.25 \\
                w^{[1](2)} &= \frac{w^{[1](2)}}{\sumin w^{[1](i)}} = \frac{0.23}{0.23 + 0.23 + 0.47} \approx 0.25 \\
                w^{[1](3)} &= \frac{w^{[1](3)}}{\sumin w^{[1](i)}} = \frac{0.47}{0.23 + 0.23 + 0.47} \approx 0.50 \\
            \end{aligned}
        \end{equation}

    \item 

         As the training error is not yet 0, we do a second stump using the new weights, The decision boundary is $x=4$. 
         The stump makes two correct predictions and one incorrect prediction:

      <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
          # Define x 
          x<-c(1,3,5)
          # Define y
          y = c(-1, 1, -1)
          ## Make y-value=0
          x<-cbind(x,0)
          ## Plotting without box or axis with dot, representing data points                  
          plot(x, col=ifelse(y==1,'red','blue')
                  ,bty='n',xaxt='n',yaxt='n',ylab='',xlab='',pch=16,cex=2)
          # add axis with label to the right
          axis(1, at=1:5, labels=1:5, las=1,pos=-0.12)
          # add axis labels
          mtext("x", side=1, line=-2, cex=1.5)
          # add a legend
          legend("topright", legend=c("y=1","y=-1"), col=c("red","blue"), pch=16, cex=0.7)
          
          # write a segment in x = 2
          segments(2,-0.1,2,0.3,lty=2,lwd=2,col='darkgreen')
          arrows(2,0.1,2.5,0.1,col='darkgreen',lwd=2,length = 0.1)
          text(x=2.3, y=0.25, label="+",cex=1.5,col='darkgreen')
          # write a segment in x =4 
          segments(4,-0.1,4,0.3,lty=2,lwd=2,col='darkviolet')
          arrows(4,0.1,3.5,0.1,col='darkviolet',lwd=2,length = 0.1)
          text(x=3.7, y=0.25, label="+",cex=1.5,col='darkviolet')
          @

        We calculate the  weighted in-sample misclassification rate and the weight for the stump.

        \begin{equation}
            \begin{aligned}
                \errm[1] &= \sumin \wmi[1] \cdot \mathds{1}_{\{\yi[i] \,\neq\, \blh[1](\xi)\}} = 0.25 \\
                \betamh[1] &= \frac{1}{2} \log \left( \frac{1 - \errm[1]}{\errm[1]}\right) = 0.5 \cdot log \left(\frac{0.75}{0.25} \right)  \approx 0.54 \\
            \end{aligned}
        \end{equation}

        We can see that for the left-most point and the right-most point, there is a disagreement between our two stumps. 
        In the case of the right-most point, the second stump has a bigger weight than the first one, so we will classify it correctly as $y=-1$. 
        Unfortunately, in the case of the left-most point, we will classify it incorrectly as $y=1$. Let's calculate the new weights:

        \begin{equation}
            \begin{aligned}
                w^{[2](1)} &= w^{[1](1)} \cdot \exp \left( -\betamh[0] \cdot \yi[1] \cdot \blh[1](\xi[1]) \right) = 0.25 \cdot \exp \left( -0.54 \cdot 1 \cdot -1 \right) \approx 0.42 \\
                w^{[2](2)} &= w^{[1](2)} \cdot \exp \left( -\betamh[0] \cdot \yi[2] \cdot \blh[1](\xi[2]) \right) = 0.25 \cdot \exp \left( -0.54 \cdot 1 \cdot 1 \right) \approx 0.15 \\
                w^{[2](3)} &= w^{[1](3)} \cdot \exp \left( -\betamh[0] \cdot \yi[3] \cdot \blh[1](\xi[3]) \right) = 0.5 \cdot \exp \left( -0.54 \cdot -1 \cdot -1 \right) \approx 0.29 \\ 
            \end{aligned}
        \end{equation}

        We need to normalize the weights so that they sum up to one: 

        \begin{equation}
            \begin{aligned}
                w^{[2](1)} &= \frac{w^{[2](1)}}{\sumin w^{[2](i)}} = \frac{0.42}{0.42 + 0.15 + 0.29} \approx 0.5 \\
                w^{[2](2)} &= \frac{w^{[2](2)}}{\sumin w^{[2](i)}} = \frac{0.23}{0.42 + 0.15 + 0.29} \approx 0.17 \\
                w^{[2](3)} &= \frac{w^{[2](3)}}{\sumin w^{[2](i)}} = \frac{0.47}{0.42 + 0.15 + 0.29} \approx 0.33 \\
            \end{aligned}
        \end{equation}
        
        
        
         We will start now with the third iteration. We will add a new stump on $x=2$.

        <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
          # Define x 
          x<-c(1,3,5)
          # Define y
          y = c(-1, 1, -1)
          ## Make y-value=0
          x<-cbind(x,0)
          ## Plotting without box or axis with dot, representing data points                  
          plot(x, col=ifelse(y==1,'red','blue')
                  ,bty='n',xaxt='n',yaxt='n',ylab='',xlab='',pch=16,cex=2)
          # add axis with label to the right
          axis(1, at=1:5, labels=1:5, las=1,pos=-0.12)
          # add axis labels
          mtext("x", side=1, line=-2, cex=1.5)
          # add a legend
          legend("topright", legend=c("y=1","y=-1"), col=c("red","blue"), pch=16, cex=0.7)
          
          # write a segment in x = 2
          segments(2,-0.1,2,0.3,lty=2,lwd=2,col='darkgreen')
          arrows(2,0.1,2.5,0.1,col='darkgreen',lwd=2,length = 0.1)
          text(x=2.3, y=0.25, label="+",cex=1.5,col='darkgreen')
          # write a segment in x =4 
          segments(4,-0.1,4,0.3,lty=2,lwd=2,col='darkviolet')
          arrows(4,0.1,3.5,0.1,col='darkviolet',lwd=2,length = 0.1)
          text(x=3.7, y=0.25, label="+",cex=1.5,col='darkviolet')
          # write a segment in x =2 
          segments(2,-0.1,2,0.6,lty=3,lwd=2,col='brown')
          arrows(2,0.4,2.5,0.4,col='brown',lwd=2,length = 0.1)
          text(x=2.3, y=0.55, label="+",cex=1.5,col='brown')
          @
          

          We calculate the weighted in-sample misclassification error for this stump:

          \begin{equation}
            \begin{aligned}
                \errm[2] &= \sumin \wmi[2] \cdot \mathds{1}_{\{\yi[i] \,\neq\, \blh[2](\xi)\}} = 0.33 \\
                \betamh[2] &= \frac{1}{2} \log \left( \frac{1 - \errm[2]}{\errm[2]}\right) = 0.5 \cdot log \left(\frac{0.67}{0.33} \right)  \approx 0.35 \\
            \end{aligned}
        \end{equation}
        
        We can see that we are going back to a similar case as the first iteration, it seems like we will keep creating new stumps on $x=2$ and $x=4$ and the model will never reach a training error of zero. Let's implement Adaboost with stumps in R to find out:

        
<<echo=TRUE, fig.align='center', fig.height=4, fig.width=4>>=
library(rpart)
library(rpart.plot)
# Define x
x <- c(1, 3, 5)
# Define y
y = as.factor(c('negative', 'positive', 'negative'))
df = data.frame(x, y)

w <- rep(1 / 3, 3)
M <- 100
err <- rep(0,M)
beta <- rep(1, M)
b_list <- list()
f <- rep(0,3)
m <- 1
while (m < M) {
  cat("m =", m, ":\n")
  # 3: Fit classifier with weights
  tree = rpart(y ~ x,
               data = df,
               weights = w,
               control = rpart.control(
                 minsplit = 0,
                 maxdepth = 1,
                 cp = -1
               ))
  prp(tree,main = paste("Stump for iteration: ",m))
  cat("split point:", tree$splits[,"index"], "\n")
  # 4: Calculate error
  b <- predict(tree, df, type="class")
  indicator <- b!=y
  err[m] <- sum(w*indicator)
  cat("error unweighted:", sum(indicator)/3, "\n")
  cat("error weighted:", err[m], "\n")
  
  # 5: Calculate beta
  beta[m] <- 0.5 * log((1-err[m])/err[m])
  
  # Update weigths
  y_mal_b <- -(indicator*2-1)
  w_unnormalized <- w * exp (-beta[m]*y_mal_b)
  w_normalized <- w_unnormalized/sum(w_unnormalized)
  w <- w_normalized
  cat("new weights after this iteration:", w, "\n")
  b_list[[m]] <- b
  f <- f + beta[m]*(as.numeric(b)*2-3)
  cat("f:", f, "\n")
  h <- f>0
  err_ens <- sum(h!=(y=="positive"))/3
  cat("error of ensemble:", err_ens, "\n \n")
  if(err_ens==0){
    cat( "We reached 0% error rate on the training dataset!")
    m=M
  }else{
    m <- m+1
  }
}
@
          
          
        The code shows that we reached convergence in only 3 steps. Why our results are not the same? The reason behind the difference is that rpart allows stumps where all the leaves lead to the same group, as in the 1st iteration. If we take this into account, we could build again our third stump:
      
        
         <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
          # Define x 
          x<-c(1,3,5)
          # Define y
          y = c(-1, 1, -1)
          ## Make y-value=0
          x<-cbind(x,0)
          ## Plotting without box or axis with dot, representing data points                  
          plot(x, col=ifelse(y==1,'red','blue')
                  ,bty='n',xaxt='n',yaxt='n',ylab='',xlab='',pch=16,cex=2)
          # add axis with label to the right
          axis(1, at=1:5, labels=1:5, las=1,pos=-0.12)
          # add axis labels
          mtext("x", side=1, line=-2, cex=1.5)
          # add a legend
          legend("topright", legend=c("y=1","y=-1"), col=c("red","blue"), pch=16, cex=0.7)
          
          # write a segment in x = 2
          segments(2,-0.1,2,0.3,lty=2,lwd=2,col='darkgreen')
          arrows(2,0.1,2.5,0.1,col='darkgreen',lwd=2,length = 0.1)
          text(x=2.3, y=0.25, label="+",cex=1.5,col='darkgreen')
          # write a segment in x =4 
          segments(4,-0.1,4,0.3,lty=2,lwd=2,col='darkviolet')
          arrows(4,0.1,3.5,0.1,col='darkviolet',lwd=2,length = 0.1)
          text(x=3.7, y=0.25, label="+",cex=1.5,col='darkviolet')
          # write a segment in x =2 
          segments(2,-0.1,2,0.6,lty=3,lwd=2,col='brown')
          arrows(2,0.4,2.5,0.4,col='brown',lwd=2,length = 0.1)
          arrows(2,0.4,1.5,0.4,col='brown',lwd=2,length = 0.1)
          text(x=2.3, y=0.55, label="-",cex=1.5,col='brown')
          text(x=1.7, y=0.55, label="-",cex=1.5,col='brown')
          @


       The weighted in-sample misclassification error and voting power of the stump are calculated:
        
        \begin{equation}
            \begin{aligned}
                \errm[2] &= \sumin \wmi[2] \cdot \mathds{1}_{\{\yi[i] \,\neq\, \blh[2](\xi)\}} = 0.17 \\
                \betamh[2] &= \frac{1}{2} \log \left( \frac{1 - \errm[2]}{\errm[2]}\right) = 0.5 \cdot log \left(\frac{0.83}{0.17} \right)  \approx 0.79 \\
            \end{aligned}
        \end{equation}
        
        Let's check how the points are classified:
        
        
        \begin{equation}
            \begin{aligned}
              \hat{y_1}  &= \sign \left( \sumim \betamh[i]  \blh[i] ( \xi[1]) \right) = \sign( -0.35 + 0.54 - 0.79) = -1\\
              \hat{y_2}  &= \sign \left( \sumim \betamh[i]  \blh[i] ( \xi[2]) \right) = \sign( +0.35 + 0.54 - 0.79) = 1 \\
              \hat{y_3}  &= \sign \left( \sumim \betamh[i]  \blh[i] ( \xi[3]) \right) = \sign( +0.35 - 0.54 - 0.79) = -1 \\
            \end{aligned}
        \end{equation}
        
        In this case, we reached zero error in training in the third iteration.

\end{enumerate}