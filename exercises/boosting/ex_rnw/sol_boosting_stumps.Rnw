\begin{enumerate}
    \item 
        The initial weights all three points in the dataset is $\frac{1}{3}$. A decision boundary for the first decision stump could be the line $x=2$.
        
        
        <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
            mpoints = list(c(1, 0.1),c(3, 0.1),c(5, 0.1))
            x1 = sapply(mpoints,function(x) x[1])
            x2 = sapply(mpoints,function(x) x[2])
            y = c(-1, 1, -1)
            
            par(mar = c(4,4,1,1), pin = c(3,3))
            plot(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2,
            xlim = c(-1,6), ylim = c(0,1),
            ylab = "x2", xlab = "x1")
			      grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = 1)
            points(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2)
			      text(x1, x2, labels = c(expression(y[1]), expression(y[2]), expression(y[3])), pos = 3, cex = 1)
			      abline(v=2,lty=2,lwd=2,col='red')
			      arrows(2,0.3,3,0.3,col='red')
			      text(x=2.3, y=0.35, label="+",cex=2,col='red')
          @

    \item 

        The first stump makes two correct predictions and one incorrect prediction,
         we can then calculate the  weighted in-sample misclassification rate,
          the weight for the stump and the new data points weights

        
        \begin{equation}
            \begin{aligned}
                \errm[0] &= \sumin \wmi[0] \cdot \mathds{1}_{\{\yi[0] \,\neq\, \blh[0](\xi)\}} = 0.33 \\
                \betamh[0] &= \frac{1}{2} \log \left( \frac{1 - \errm[0]}{\errm[0]}\right) = 0.5 \cdot log \left(\frac{0.67}{0.33} \right)  \approx 0.35 \\
                w^{[1](1)} &= w^{[0](1)} \cdot \exp \left( -\betamh[0] \cdot \yi[1] \cdot \blh[0](\xi[1]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot -1 \cdot -1 \right) \approx 0.23 \\
                w^{[1](2)} &= w^{[0](2)} \cdot \exp \left( -\betamh[0] \cdot \yi[2] \cdot \blh[0](\xi[2]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot 1 \cdot 1 \right) \approx 0.23 \\
                w^{[1](3)} &= w^{[0](3)} \cdot \exp \left( -\betamh[0] \cdot \yi[3] \cdot \blh[0](\xi[3]) \right) = 0.33 \cdot \exp \left( -0.35 \cdot 1 \cdot -1 \right) \approx 0.47 \\ 
            \end{aligned}
        \end{equation}

        We need to normalize the weights so that they sum up to one: 

        \begin{equation}
            \begin{aligned}
                w^{[1](1)} &= \frac{w^{[1](1)}}{\sumin w^{[1](i)}} = \frac{0.23}{0.23 + 0.23 + 0.47} \approx 0.25 \\
                w^{[1](2)} &= \frac{w^{[1](2)}}{\sumin w^{[1](i)}} = \frac{0.23}{0.23 + 0.23 + 0.47} \approx 0.25 \\
                w^{[1](3)} &= \frac{w^{[1](3)}}{\sumin w^{[1](i)}} = \frac{0.47}{0.23 + 0.23 + 0.47} \approx 0.50 \\
            \end{aligned}
        \end{equation}

    \item 

         As the training error is not yet 0, we do a second stump using the new weights, The decision boundary is x=4. 
         The stump makes two correct predictions and one incorrect prediction:

      <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
            mpoints = list(c(1, 0.1),c(3, 0.1),c(5, 0.1))
            x1 = sapply(mpoints,function(x) x[1])
            x2 = sapply(mpoints,function(x) x[2])
            y = c(-1, 1, -1)
            
            par(mar = c(4,4,1,1), pin = c(3,3))
            plot(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2,
            xlim = c(-1,6), ylim = c(0,1),
            ylab = "x2", xlab = "x1")
			      grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = 1)
            points(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2)
			      text(x1, x2, labels = c(expression(y[1]), expression(y[2]), expression(y[3])), pos = 3, cex = 1)
			      abline(v=2,lty=2,lwd=2,col='red')
			      arrows(2,0.3,3,0.3,col='red')
			      text(x=2.3, y=0.35, label="+",cex=2,col='red')
			      abline(v=4,lty=2,lwd=2,col='blue')
			      arrows(4,0.4,3,0.4,col='blue')
			      text(x=3.6, y=0.45, label="+",cex=2,col='blue')
          @

        We calculate the  weighted in-sample misclassification rate and the weight for the stump.

        \begin{equation}
            \begin{aligned}
                \errm[1] &= \sumin \wmi[1] \cdot \mathds{1}_{\{\yi[1] \,\neq\, \blh[1](\xi)\}} = 0.25 \\
                \betamh[1] &= \frac{1}{2} \log \left( \frac{1 - \errm[1]}{\errm[1]}\right) = 0.5 \cdot log \left(\frac{0.75}{0.25} \right)  \approx 0.54 \\
            \end{aligned}
        \end{equation}

        We can see that for the left-most point and the right-most point, there is a disagreement between our two stumps. 
        In the case of the right-most point, the second stump has a bigger weight than the first one, so we will classify it correctly as $y=-1$. 
        Unfortunately, in the case of the left-most point, we will classify it incorrectly as $y=1$. We will now calculate the new weights:

        \begin{equation}
            \begin{aligned}
                w^{[2](1)} &= w^{[1](1)} \cdot \exp \left( -\betamh[0] \cdot \yi[1] \cdot \blh[0](\xi[1]) \right) = 0.25 \cdot \exp \left( -0.54 \cdot 1 \cdot -1 \right) \approx 0.42 \\
                w^{[2](2)} &= w^{[1](2)} \cdot \exp \left( -\betamh[0] \cdot \yi[2] \cdot \blh[0](\xi[2]) \right) = 0.25 \cdot \exp \left( -0.54 \cdot 1 \cdot 1 \right) \approx 0.15 \\
                w^{[2](3)} &= w^{[1](3)} \cdot \exp \left( -\betamh[0] \cdot \yi[3] \cdot \blh[0](\xi[3]) \right) = 0.5 \cdot \exp \left( -0.54 \cdot -1 \cdot -1 \right) \approx 0.29 \\ 
            \end{aligned}
        \end{equation}

        We need to normalize the weights so that they sum up to one: 

        \begin{equation}
            \begin{aligned}
                w^{[2](1)} &= \frac{w^{[2](1)}}{\sumin w^{[2](i)}} = \frac{0.42}{0.42 + 0.15 + 0.29} \approx 0.49 \\
                w^{[2](2)} &= \frac{w^{[2](2)}}{\sumin w^{[2](i)}} = \frac{0.23}{0.42 + 0.15 + 0.29} \approx 0.17 \\
                w^{[2](3)} &= \frac{w^{[2](3)}}{\sumin w^{[2](i)}} = \frac{0.47}{0.42 + 0.15 + 0.29} \approx 0.34 \\
            \end{aligned}
        \end{equation}
        
        
        
         We will start with a third iteration that will correct this mistake. Unfortunately, we can't have open end notes in a stump during training. The best we can do add a new stump in the same place as the first one: 

        <<echo=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
            mpoints = list(c(1, 0.1),c(3, 0.1),c(5, 0.1))
            x1 = sapply(mpoints,function(x) x[1])
            x2 = sapply(mpoints,function(x) x[2])
            y = c(-1, 1, -1)
            
            par(mar = c(4,4,1,1), pin = c(3,3))
            plot(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2,
            xlim = c(-1,6), ylim = c(0,1),
            ylab = "x2", xlab = "x1")
			      grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = 1)
            points(x1, x2, pch = ifelse(y == -1, "-", "+"), cex = 2)
			      text(x1, x2, labels = c(expression(y[1]), expression(y[2]), expression(y[3])), pos = 3, cex = 1)
			      abline(v=2,lty=2,lwd=2,col='red')
			      arrows(2,0.3,3,0.3,col='red')
			      text(x=2.3, y=0.35, label="+",cex=2,col='red')
			      abline(v=4,lty=2,lwd=2,col='blue')
			      arrows(4,0.4,3,0.4,col='blue')
			      text(x=3.6, y=0.45, label="+",cex=2,col='blue')
			      abline(v=2,lty=2,lwd=1,col='darkgreen')
			      arrows(2,0.5,3,0.5,col='darkgreen')
			      text(x=2.3, y=0.55, label="+",cex=2,col='darkgreen')
          @

          We calculate the weighted in-sample misclassification error for this stump:

          \begin{equation}
            \begin{aligned}
                \errm[1] &= \sumin \wmi[1] \cdot \mathds{1}_{\{\yi[1] \,\neq\, \blh[1](\xi)\}} = 0.34 \\
                \betamh[1] &= \frac{1}{2} \log \left( \frac{1 - \errm[1]}{\errm[1]}\right) = 0.5 \cdot log \left(\frac{0.66}{0.34} \right)  \approx 0.54 \\
            \end{aligned}
        \end{equation}
        
        We can see that we are going back to a similar case as the first iteration and now the rounding errors could play a role in our calculations. Let's build a model to see what happens:
<<echo=FALSE,message=FALSE, warning=FALSE, include=FALSE, fig.align='center', fig.height=4, fig.width=4>>=
  devtools::install_github("jadonwagstaff/sboost")
  library(sboost)
@
			      
<<echo=TRUE, fig.align='center', fig.height=4, fig.width=4>>=
  df = data.frame(x1,x2,y)
  for (i in 1:6){
    # Train the model with the amount of iterations
    model= sboost::sboost(df[1:2],df$y,iterations = i)
    # Assess the performance 
    model_assessment=sboost::assess(model,features = df[1:2],outcomes = df$y)
    cat("Iteration number :" , i,
        ". model accuracy: ", model_assessment$performance[[6]],"\n"  )
    }
  cat("Voting power of each stump: \n", model$classifier$vote)
  
  cat("Where each stump is located: ", model$classifier$split )
@

\end{enumerate}