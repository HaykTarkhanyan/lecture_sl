\begin{enumerate}
  \item 
  	As seen in the \href{https://github.com/slds-lmu/lecture_sl/blob/main/slides-pdf/slides-advriskmin-classification-01.pdf}{0-1-Loss presentation, slide 2}, the discrete classifier that minimizes the risk $\hbayes$ (The Bayes optimal classifier) is:
  	
  	\begin{align*}
  	  \hxbayes &=  \argmax_{l \in \Yspace } \ \ \underbrace{\P(y = l~|~ _{\xv = \xv})}_{\sim \mathrm{Unif} \{1,\ldots,x\} } \\
  	  &=  \argmax \frac{1}{x} \cdot \I_{[1\leq l\leq x]}
  	\end{align*}

    As the distribution of $y$ given $x$ is uniform , any value between 1 and $x$ is optimal.

  	\begin{align*}
  	  \hxbayes = \{1,\dots,x\}
  	\end{align*}
    \lz
  \item
    The Bayes risk for the 0-1-loss, also known as the Bayes error rate, is defined as :
    
      \begin{align*}
        \riskbayes &= 1 - \E_x \left[\max_{l \in \Yspace} \P(y = l~|~ _{\xv = \xv})\right] \\
        &= 1 - \underbrace{\E_x \left[ \frac{1}{x} \right]}_{p_x \sim \mathrm{Unif} \{1,\ldots,10\}} \\
        &= 1 - \sum_{x=1}^{10} \frac{1}{x} \frac{1}{10} \\
        &= 1- \frac{7381}{25200}
  	  \end{align*}
    \lz
  \item
  
    The point-wise optimizer for the 0-1 loss over all discrete classifiers $\hxbayes$ is:
  
    \begin{align*}
      \hxbayes = \argmax_{l \in \Yspace } \ \ \P(y = l~|~ _{\xv = \xv})
    \end{align*}
    
    The optimal constant model can be obtained by forgetting the conditioning on x, leading to:
    
    \begin{align*}
      \hxh = \argmax_{l \in \Yspace } \ \ \P(y = l)
    \end{align*}
    
    Using the law of total probability:
    \begin{align*}
      \hxh &= \argmax_{l \in \Yspace }  \sum_{x=1}^{10}  \P(y = l~|~ _{\xv = \xv}) \cdot \P(\xv = \xv) \\
      &= \argmax_{l \in \Yspace }  \sum_{x=1}^{10}  \frac{1}{x} \cdot  \I_{[1\leq l \leq x]} \cdot \frac{1}{10} \\
      &= \argmax_{l \in \Yspace } \begin{cases}
            \frac{7381}{25200}, & l=1 \\
            \frac{7381}{25200} - \frac{1}{10}, & l=2 \\ 
            \frac{7381}{25200} - \frac{1}{10} - \frac{1}{20}, & l=3 \\ 
            \hfil \vdots & \hfil \vdots \\
            \frac{7381}{25200} - \sum_{z=1}^{l-1} \frac{1}{10\cdot z},& l=10
        \end{cases}
    \end{align*}
    
    As the probability is monotonically decreasing with $l$, we can conclude that the optimal constant model is :
    
    \begin{align*}
      \hxh = 1
    \end{align*}
    \lz
  \item
    
    The Risk is calculated by:
    
    \begin{align*}
      \R_L(\hh) &= 1 - \max \P ( y =l) \\
      &= 1 -  \P (y=1) \\
      &= 1 - \frac{7381}{25200} 
    \end{align*}
    
    
\end{enumerate}
