\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}}}
\makeatother
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{
\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
}}
\makeatother
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}
	
\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopfsl{2}

\loesung{}{

\begin{enumerate}

\item Read in the MNIST data set 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(keras)}
\hlstd{mnist} \hlkwb{<-} \hlkwd{dataset_mnist}\hlstd{()}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loaded Tensorflow version 2.9.0}}\end{kframe}
\end{knitrout}

\item Visualize the data like

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(keras)}
\hlstd{mnist} \hlkwb{<-} \hlkwd{dataset_mnist}\hlstd{()}
\hlstd{x_train} \hlkwb{<-} \hlstd{mnist}\hlopt{$}\hlstd{train}\hlopt{$}\hlstd{x}
\hlstd{y_train} \hlkwb{<-} \hlstd{mnist}\hlopt{$}\hlstd{train}\hlopt{$}\hlstd{y}
\hlstd{x_test} \hlkwb{<-} \hlstd{mnist}\hlopt{$}\hlstd{test}\hlopt{$}\hlstd{x}
\hlstd{y_test} \hlkwb{<-} \hlstd{mnist}\hlopt{$}\hlstd{test}\hlopt{$}\hlstd{y}

\hlcom{# visualize the digits}
\hlkwd{par}\hlstd{(}\hlkwc{mfcol}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{6}\hlstd{))}
\hlkwd{par}\hlstd{(}\hlkwc{mar}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{0}\hlstd{),} \hlkwc{xaxs}\hlstd{=}\hlstr{'i'}\hlstd{,} \hlkwc{yaxs}\hlstd{=}\hlstr{'i'}\hlstd{)}
\hlkwa{for} \hlstd{(idx} \hlkwa{in} \hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlkwd{NROW}\hlstd{(x_train),} \hlnum{6}\hlstd{)) \{}
    \hlstd{im} \hlkwb{<-} \hlstd{x_train[idx,,]}
    \hlstd{im} \hlkwb{<-} \hlkwd{t}\hlstd{(}\hlkwd{apply}\hlstd{(im,} \hlnum{2}\hlstd{, rev))}
    \hlkwd{image}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{28}\hlstd{,} \hlnum{1}\hlopt{:}\hlnum{28}\hlstd{, im,} \hlkwc{col}\hlstd{=}\hlkwd{gray}\hlstd{((}\hlnum{0}\hlopt{:}\hlnum{255}\hlstd{)}\hlopt{/}\hlnum{255}\hlstd{),}
          \hlkwc{xaxt}\hlstd{=}\hlstr{'n'}\hlstd{,} \hlkwc{main}\hlstd{=}\hlkwd{paste}\hlstd{(y_train[idx]),}
          \hlkwc{yaxt}\hlstd{=}\hlstr{'n'}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-4-1} 

}


\end{knitrout}
\item Convert the features to a (\texttt{pandas}) data frame, by flattening the 28x28 images to a 784-entry-long vector, which represents one row in your data frame. Divide the intensity values of each pixel (each column) by 255 to get a value between 0 and 1. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(tibble)}
\hlcom{# reshape}
\hlkwd{dim}\hlstd{(x_train)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{nrow}\hlstd{(x_train),} \hlnum{784}\hlstd{)}
\hlkwd{dim}\hlstd{(x_test)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{nrow}\hlstd{(x_test),} \hlnum{784}\hlstd{)}
\hlcom{# rescale}
\hlstd{x_train} \hlkwb{<-} \hlstd{x_train} \hlopt{/} \hlnum{255}
\hlstd{x_test} \hlkwb{<-} \hlstd{x_test} \hlopt{/} \hlnum{255}
\hlcom{# convert to data.frame}
\hlstd{x_train} \hlkwb{<-} \hlkwd{as_tibble}\hlstd{(}\hlkwd{as.data.frame}\hlstd{(x_train))}
\hlstd{x_test} \hlkwb{<-} \hlkwd{as_tibble}\hlstd{(}\hlkwd{as.data.frame}\hlstd{(x_test))}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Softmax regression

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(nnet)}
\hlstd{data} \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{as.factor}\hlstd{(y_train), x_train)}
\hlcom{# note: takes some time and requires quite some memory}
\hlcom{# also you need to set the maximum number of weights to get it running}
\hlcom{# we will further restrict the maximum number of iterations}
\hlcom{# to avoid overfitting (explanation is given later)}
\hlstd{model} \hlkwb{<-} \hlkwd{multinom}\hlstd{(y} \hlopt{~ -}\hlnum{1} \hlopt{+} \hlstd{.,} \hlkwc{data} \hlstd{= data,} \hlkwc{MaxNWts} \hlstd{=} \hlnum{7860}\hlstd{,} \hlkwc{maxit} \hlstd{=} \hlnum{20}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## # weights:  7850 (7056 variable)
## initial  value 138155.105580 
## iter  10 value 30790.232712
## iter  20 value 23682.853837
## final  value 23682.853837 
## stopped after 20 iterations
\end{verbatim}
\end{kframe}
\end{knitrout}


Look at the larger weights:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(model}\hlopt{$}\hlstd{wts)}
\end{alltt}
\begin{verbatim}
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -0.754937 -0.036317  0.000000 -0.003706  0.009061  0.842105
\end{verbatim}
\begin{alltt}
\hlkwd{which.max}\hlstd{(}\hlkwd{abs}\hlstd{(model}\hlopt{$}\hlstd{wts))}
\end{alltt}
\begin{verbatim}
## [1] 1192
\end{verbatim}
\begin{alltt}
\hlkwd{dim}\hlstd{(}\hlkwd{coef}\hlstd{(model))}
\end{alltt}
\begin{verbatim}
## [1]   9 784
\end{verbatim}
\end{kframe}
\end{knitrout}

There seem to be a few very large coefficients

\item Use \texttt{keras}:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(dplyr)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# Attaching package: 'dplyr'}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# The following objects are masked from 'package:stats':\\\#\# \\\#\# \ \ \ \ filter, lag}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# The following objects are masked from 'package:base':\\\#\# \\\#\# \ \ \ \ intersect, setdiff, setequal, union}}\begin{alltt}
\hlkwd{library}\hlstd{(keras)}

\hlcom{# convert outcome using one-hot encoding}
\hlstd{y_train_one_hot} \hlkwb{<-} \hlkwd{to_categorical}\hlstd{(y_train)}
\hlstd{y_test_one_hot} \hlkwb{<-} \hlkwd{to_categorical}\hlstd{(y_test)}

\hlstd{neural_network} \hlkwb{<-} \hlkwd{keras_model_sequential}\hlstd{()}

\hlstd{neural_network} \hlopt{%>%}
  \hlkwd{layer_dense}\hlstd{(}\hlkwc{units} \hlstd{=} \hlnum{10}\hlstd{,} \hlcom{# corresponding to the number of classes}
              \hlkwc{activation} \hlstd{=} \hlstr{"softmax"}\hlstd{,}
              \hlkwc{input_shape} \hlstd{=} \hlkwd{list}\hlstd{(}\hlnum{784}\hlstd{))} \hlopt{%>%}
  \hlkwd{compile}\hlstd{(}
    \hlkwc{optimizer} \hlstd{=} \hlstr{"adam"}\hlstd{,}
    \hlkwc{loss}      \hlstd{=} \hlstr{"categorical_crossentropy"}\hlstd{,}
    \hlkwc{metric} \hlstd{=} \hlstr{"accuracy"}
  \hlstd{)}

\hlstd{history_minibatches} \hlkwb{<-} \hlkwd{fit}\hlstd{(}
  \hlkwc{object}           \hlstd{= neural_network,}
  \hlkwc{x}                \hlstd{=} \hlkwd{as.matrix}\hlstd{(x_train),}
  \hlkwc{y}                \hlstd{= y_train_one_hot,}
  \hlkwc{batch_size}       \hlstd{=} \hlnum{24}\hlstd{,}
  \hlkwc{epochs}           \hlstd{=} \hlnum{80}\hlstd{,}
  \hlkwc{validation_split} \hlstd{=} \hlnum{0.2}\hlstd{,}
  \hlkwc{callbacks} \hlstd{=} \hlkwd{list}\hlstd{(}\hlkwd{callback_early_stopping}\hlstd{(}\hlkwc{patience} \hlstd{=} \hlnum{10}\hlstd{)),}
  \hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlcom{# set this to TRUE to get console output}
  \hlkwc{view_metrics} \hlstd{=} \hlnum{FALSE} \hlcom{# set this to TRUE to get a dynamic graphic output in RStudio}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

Look at the network weights

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(tensorflow)}
\hlstd{tensor_weights} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(tf}\hlopt{$}\hlkwd{add}\hlstd{(neural_network}\hlopt{$}\hlstd{weights[[}\hlnum{1}\hlstd{]],}\hlnum{0}\hlstd{))}
\hlkwd{summary}\hlstd{(}\hlkwd{c}\hlstd{(tensor_weights))}
\end{alltt}
\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -4.4055 -0.4624 -0.0740 -0.2411  0.0864  1.9105
\end{verbatim}
\end{kframe}
\end{knitrout}

and compare to the ones from multinomial logistic regression:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{c}\hlstd{(tensor_weights[,}\hlopt{-}\hlnum{1}\hlstd{])} \hlopt{~} \hlkwd{c}\hlstd{(}\hlkwd{t}\hlstd{(}\hlkwd{coef}\hlstd{(model))),}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Weights Softmax Regression"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Weights Neural Network"}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-11-1} 
\end{knitrout}

As both models de facto are based on neural networks (here the implementation of the softmax regression is actually done by fitting a neural network with the very same network structure), their similarity depends on how the network is trained. While clearly the implementation calling Python with backend \texttt{TensorFlow} (the \texttt{keras} fit) is much much faster, the network also converges more quickly due to a small batch size while the multinomial logistic regression calls a network fitting algorithm that uses batch size equal to the number of observations (which is usually a bad idea).

\item 

First define the metrics

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Classification error (how many of the predictions are wrong)}
\hlstd{classiferror} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{actual}\hlstd{,} \hlkwc{predicted}\hlstd{) \{}
    \hlkwd{return}\hlstd{(}\hlkwd{mean}\hlstd{(actual} \hlopt{!=} \hlstd{predicted))}
\hlstd{\}}

\hlcom{# Accuracy (how many of the predictions are correct)}
\hlstd{accuracy} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{actual}\hlstd{,} \hlkwc{predicted}\hlstd{) \{}
    \hlkwd{return}\hlstd{(}\hlnum{1} \hlopt{-} \hlkwd{classiferror}\hlstd{(actual, predicted))}
\hlstd{\}}

\hlcom{# As we will usually have probabilistic predictions, }
\hlcom{# we need to convert those to classes for the above }
\hlcom{# metrics using the class with the max probability}
\hlstd{probs_to_class} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{probvec}\hlstd{) \{}
  \hlkwd{which.max}\hlstd{(probvec)}\hlopt{-}\hlnum{1}
\hlstd{\}}

\hlcom{#' MC Brier score}
\hlstd{mcbrier} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{actual_one_hot}\hlstd{,} \hlkwc{prob}\hlstd{) \{}
  \hlkwd{rowSums}\hlstd{((actual_one_hot}\hlopt{-}\hlstd{prob)}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{\}}

\hlcom{# Cross-Entropy loss}
\hlstd{crossentropy} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{actual_one_hot}\hlstd{,} \hlkwc{prob}\hlstd{) \{}
  \hlkwd{rowSums}\hlstd{(} \hlopt{-}\hlkwd{log}\hlstd{(prob)} \hlopt{*} \hlstd{actual_one_hot )}
\hlstd{\}}

\hlcom{# negative log-likelihood of multinomial distribution}
\hlstd{loglikmultinom} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{actual_one_hot}\hlstd{,} \hlkwc{prob}\hlstd{) \{}
  \hlkwd{sapply}\hlstd{(}\hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(actual_one_hot),} \hlkwa{function}\hlstd{(}\hlkwc{i}\hlstd{)}
    \hlkwd{dmultinom}\hlstd{(actual_one_hot[i,],}
              \hlkwc{size} \hlstd{=} \hlnum{1}\hlstd{, prob[i,],} \hlkwc{log} \hlstd{=} \hlnum{TRUE}\hlstd{))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Now we get the predictions:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pred_multinom} \hlkwb{<-} \hlkwd{predict}\hlstd{(model, x_test,} \hlkwc{type} \hlstd{=} \hlstr{"probs"}\hlstd{)}
\hlstd{pred_nn} \hlkwb{<-} \hlkwd{predict}\hlstd{(neural_network,} \hlkwd{as.matrix}\hlstd{(x_test))}
\hlkwd{str}\hlstd{(pred_multinom,} \hlnum{1}\hlstd{)}
\end{alltt}
\begin{verbatim}
##  num [1:10000, 1:10] 0.001922 0.019599 0.000202 0.98602 0.00456 ...
##  - attr(*, "dimnames")=List of 2
\end{verbatim}
\begin{alltt}
\hlkwd{str}\hlstd{(pred_nn,} \hlnum{1}\hlstd{)}
\end{alltt}
\begin{verbatim}
##  num [1:10000, 1:10] 1.68e-08 2.29e-05 7.43e-07 1.00 6.32e-04 ...
\end{verbatim}
\end{kframe}
\end{knitrout}

Let's first look at the confusion matrix (in this case for the multinomial regression):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{table}\hlstd{(y_test,} \hlkwd{apply}\hlstd{(pred_multinom,}\hlnum{1}\hlstd{,probs_to_class))}
\end{alltt}
\begin{verbatim}
##       
## y_test    0    1    2    3    4    5    6    7    8    9
##      0  904    0    0    3    1   55   12    2    3    0
##      1    0 1108    2   10    1    1    5    2    6    0
##      2    8   11  874   34    9    6   20   15   47    8
##      3    6    0   11  926    3   15    5    9   26    9
##      4    0    5    2    4  901    2   13    4    3   48
##      5   11    1    3   54   13  703   23   10   62   12
##      6    9    3    4    1    8   16  913    2    2    0
##      7    4   14    9   11    8    1    1  938    3   39
##      8   10   14    5   48   12   22   16   12  813   22
##      9    8    5    1   19   41    6    1   19    7  902
\end{verbatim}
\end{kframe}
\end{knitrout}

Now the metrics. Classification error:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cbind}\hlstd{(}\hlkwc{multinom} \hlstd{=} \hlkwd{classiferror}\hlstd{(y_test,} \hlkwd{apply}\hlstd{(pred_multinom,}\hlnum{1}\hlstd{,probs_to_class)),}
      \hlkwc{neural} \hlstd{=} \hlkwd{classiferror}\hlstd{(y_test,} \hlkwd{apply}\hlstd{(pred_nn,}\hlnum{1}\hlstd{,probs_to_class))}
\hlstd{)}
\end{alltt}
\begin{verbatim}
##      multinom neural
## [1,]   0.1018 0.0754
\end{verbatim}
\end{kframe}
\end{knitrout}

Accuracy:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cbind}\hlstd{(}\hlkwc{multinom} \hlstd{=} \hlkwd{accuracy}\hlstd{(y_test,} \hlkwd{apply}\hlstd{(pred_multinom,}\hlnum{1}\hlstd{,probs_to_class)),}
      \hlkwc{neural} \hlstd{=} \hlkwd{accuracy}\hlstd{(y_test,} \hlkwd{apply}\hlstd{(pred_nn,}\hlnum{1}\hlstd{,probs_to_class))}
\hlstd{)}
\end{alltt}
\begin{verbatim}
##      multinom neural
## [1,]   0.8982 0.9246
\end{verbatim}
\end{kframe}
\end{knitrout}

MC Brier score (note that we look at the mean, because the definition of the loss is on an observation basis):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cbind}\hlstd{(}\hlkwc{multinom} \hlstd{=} \hlkwd{mean}\hlstd{(}\hlkwd{mcbrier}\hlstd{(y_test_one_hot, pred_multinom)),}
      \hlkwc{neural} \hlstd{=} \hlkwd{mean}\hlstd{(}\hlkwd{mcbrier}\hlstd{(y_test_one_hot, pred_nn))}
\hlstd{)}
\end{alltt}
\begin{verbatim}
##       multinom    neural
## [1,] 0.1658749 0.1155362
\end{verbatim}
\end{kframe}
\end{knitrout}

Cross-entropy (mean):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cbind}\hlstd{(}\hlkwc{multinom} \hlstd{=} \hlkwd{mean}\hlstd{(}\hlkwd{crossentropy}\hlstd{(y_test_one_hot, pred_multinom)),}
      \hlkwc{neural} \hlstd{=} \hlkwd{mean}\hlstd{(}\hlkwd{crossentropy}\hlstd{(y_test_one_hot, pred_nn))}
\hlstd{)}
\end{alltt}
\begin{verbatim}
##      multinom    neural
## [1,] 0.383929 0.2785728
\end{verbatim}
\end{kframe}
\end{knitrout}

Mean negative log-likelihood of multinomial distribution:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cbind}\hlstd{(}\hlkwc{multinom} \hlstd{=} \hlkwd{mean}\hlstd{(}\hlkwd{loglikmultinom}\hlstd{(y_test_one_hot, pred_multinom)),}
      \hlkwc{neural} \hlstd{=} \hlkwd{mean}\hlstd{(}\hlkwd{loglikmultinom}\hlstd{(y_test_one_hot, pred_nn))}
\hlstd{)}
\end{alltt}
\begin{verbatim}
##       multinom     neural
## [1,] -0.383929 -0.2785728
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{enumerate}
}
\newpage

\loesung{}{

\begin{enumerate}
\item The logistic function is a special case of the softmax for two classes. We have

$$\pi_1(x)=\frac{\exp({\theta_1^\top x})}{\exp({\theta_1^\top x}) + \exp({\theta_2^\top x})}$$

and

$$\pi_2(x)=\frac{\exp({\theta_2^\top x})}{\exp({\theta_1^\top x}) + \exp({\theta_2^\top x})}.$$

We get:

$$\pi_1(x)=\frac{1}{ (\exp({\theta_1^\top x}) + \exp({\theta_2^\top x}) )/ \exp({\theta_1^\top x})} = \frac{1}{\exp((\theta_1 - \theta_1)^\top x) +\exp((\theta_2-\theta_1)^\top x)} = \frac{1}{1+\exp({\theta^\top x})}$$ where $\theta =\theta_2 - \theta_1 $ and $\pi_2(x) = 1 - \pi_1(x)$.

\item For $g$ classes and $n=1$ trials (actually we are dealing with a multinoulli or categorial distribution), the likelihood $l(\bm{\pi})$ of a single observation $y$ is given by 
$$l(\bm{\pi}) = \prod_{k=1}^g \pi_k^{\mathds{1}_{\{y = k\}}}.$$ 

Now let's look at the logarithmic loss in softmax regression:

$$\mbox{MC logloss} = - \sum_{k=1}^g {\mathds{1}_{\{y = k\}}} \log \pi_{k}.$$

This is in fact just the negative logarithm of our likelihood: $-\log l(\bm{\pi}) = - \sum_{k=1}^g {\mathds{1}_{\{y = k\}}} \log \pi_{k}.$
\end{enumerate}

}
\end{document}
