\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}}}
\makeatother
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{
\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
}}
\makeatother
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[utf8]{inputenc}
\pagenumbering{arabic}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}

\input{../../style/common}

\tcbset{enhanced}

%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}

\font \sfbold=cmssbx10
\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}

\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
% \pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
	{\sf \bf \huge Exercise Collection -- #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\exlect}
  {\color{black} \hrule \section{Lecture exercises}}
  
\newcommand{\exexams}
  {\color{black} \hrule \section{Further exercises}}
  % rename so it is not immediately clear these are from past exams
  
\newcommand{\exinspo}
  {\color{black} \hrule \section{Ideas \& exercises from other sources}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1} 
	\noindent}
	{\vspace{0.5cm}}
	
\newenvironment{aufgabeexam}[3] % semester, first or second, question number
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1, #2, question #3}
	\noindent}
	{\vspace{1.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\color{gray} \refstepcounter{loes}\textbf{Solution \arabic{loes}:}
	\\ \noindent}
	{\bigskip}

\setcounter{secnumdepth}{0}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\kopf{Advanced Risk Min}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{Risk Minimization and Gradient Descent (Part 1)}{

You want to estimate the relationship between a continuous response variable $\bm{y} \in \mathbb{R}^n$ and some feature $\bm{X} \in \mathbb{R}^{n \times p}$ using the linear model with an appropriate loss function $L$.

\begin{enumerate}

  \item Describe the model $f$ used in this case, its hypothesis space $\mathcal{H}$ and the theoretical risk function.
  \item Given $f \in \mathcal{H}$, explain the different parts of the Bayes regret if (i) $f^\ast \in \mathcal{H}$; if (ii) $f^\ast \notin \mathcal{H}$.
  \item Define the empirical risk and derive the gradients of the empirical risk.
  \item Show that the empirical risk is convex in the model coefficients. Why is convexity a desirable property? Hint: Compute the Hessian matrix $\bm{H} \in \mathbb{R}^{p \times p}$ and show that $\bm{z}^\top \bm{H} \bm{z} \geq 0 \, \forall \bm{z} \in \mathbb{R}^p$, i.e., show that the Hessian is positive semi-definite (psd).  
  \item Write a function implementing a gradient descent routine for the optimization of this linear model. Start with:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#' @param step_size the step_size in each iteration}
\hlcom{#' @param X the feature input matrix X}
\hlcom{#' @param y the outcome vector y}
\hlcom{#' @param beta a starting value for the coefficients}
\hlcom{#' @param eps a small constant measuring the changes in each update step. }
\hlcom{#' Stop the algorithm if the estimated model parameters do not change}
\hlcom{#' more than \textbackslash{}code\{eps\}.}

\hlcom{#' @return a set of optimal coefficients beta}
\hlstd{gradient_descent} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{step_size}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{beta}\hlstd{,} \hlkwc{eps} \hlstd{=} \hlnum{1e-8}\hlstd{)\{}

  \hlcom{# >>> do something <<<}

  \hlkwd{return}\hlstd{(beta)}

\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Run a small simulation study by creating 20 data sets as indicated below and test different step sizes $\alpha$ (fixed across iterations) against each other and against the state-of-the-art routine for linear models (in R, using the function \texttt{lm}, in Python, e.g., \texttt{sklearn.linear\_model.LinearRegression}).
  \begin{itemize}
    \item Compare the difference in estimated coefficients $\beta_j, j=1,\ldots,p$ using the mean squared error, i.e. $$p^{-1} \sum_{j=1}^p (\beta^{truth}_j-\hat{\beta}_j)^2$$ and summarize the difference over all 100 simulation repetitions.
    \item Compare the run times of your implementation and the one given by the state-of-the-art method by wrapping the function calls into a timer (e.g., \texttt{system.time()} in R).
    \end{itemize}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# settings}
\hlstd{n} \hlkwb{<-} \hlnum{10000}
\hlstd{p} \hlkwb{<-} \hlnum{100}
\hlstd{nr_sims} \hlkwb{<-} \hlnum{20}

\hlcom{# create data (only once)}
\hlstd{X} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlstd{p),} \hlkwc{ncol}\hlstd{=p)}
\hlstd{beta_truth} \hlkwb{<-} \hlkwd{runif}\hlstd{(p,} \hlopt{-}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{)}
\hlstd{f_truth} \hlkwb{<-} \hlstd{X}\hlopt{%*%}\hlstd{beta_truth}

\hlcom{# create result object}
\hlstd{result_list} \hlkwb{<-} \hlkwd{vector}\hlstd{(}\hlstr{"list"}\hlstd{, nr_sims)}

\hlkwa{for}\hlstd{(sim_nr} \hlkwa{in} \hlstd{nr_sims)}
\hlstd{\{}

  \hlcom{# create response}
  \hlstd{y} \hlkwb{<-} \hlstd{f_truth} \hlopt{+} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{sd} \hlstd{=} \hlnum{2}\hlstd{)}

  \hlcom{# >>> do something <<<}


  \hlcom{# save results in list (performance, time)}
  \hlstd{result_list[[sim_nr]]} \hlkwb{<-} \hlstd{add_something_meaningful_here}

\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
 \item Why is gradient descent maybe not the best option for optimization of a linear model with $L2$ loss? What other options exist? Name at least one and describe how you would apply this for the above problem. 
 \item Can we say something about the algorithm's consistency w.r.t. $\Pxy$, if $f^\ast \notin \Hspace$?
\end{enumerate}
}

\dlz

\aufgabe{Risk Minimization and Gradient Descent (Part 2)}{

This exercise builds upon the previous exercise sheet.

\begin{enumerate}
  \item Write a function implementing a gradient descent routine for the optimization of the linear model defined in the previous exercise sheet. Start with:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#' @param step_size the step_size in each iteration}
\hlcom{#' @param X the feature input matrix X}
\hlcom{#' @param y the outcome vector y}
\hlcom{#' @param beta a starting value for the coefficients}
\hlcom{#' @param eps a small constant measuring the changes in each update step. }
\hlcom{#' Stop the algorithm if the estimated model parameters do not change}
\hlcom{#' more than \textbackslash{}code\{eps\}.}

\hlcom{#' @return a set of optimal coefficients beta}
\hlstd{gradient_descent} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{step_size}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{beta}\hlstd{,} \hlkwc{eps} \hlstd{=} \hlnum{1e-8}\hlstd{)\{}

  \hlcom{# >>> do something <<<}

  \hlkwd{return}\hlstd{(beta)}

\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Run a small simulation study by creating 20 data sets as indicated below and test different step sizes $\alpha$ (fixed across iterations) against each other and against the state-of-the-art routine for linear models (in R, using the function \texttt{lm}, in Python, e.g., \texttt{sklearn.linear\_model.LinearRegression}).
  \begin{itemize}
    \item Compare the difference in estimated coefficients $\beta_j, j=1,\ldots,p$ using the mean squared error, i.e. $$p^{-1} \sum_{j=1}^p (\beta^{truth}_j-\hat{\beta}_j)^2$$ and summarize the difference over all 20 simulation repetitions.
    \item Compare the run times of your implementation and the one given by the state-of-the-art method by wrapping the function calls into a timer (e.g., \texttt{system.time()} in R).
    \end{itemize}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# settings}
\hlstd{n} \hlkwb{<-} \hlnum{10000}
\hlstd{p} \hlkwb{<-} \hlnum{100}
\hlstd{nr_sims} \hlkwb{<-} \hlnum{20}

\hlcom{# create data (only once)}
\hlstd{X} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlstd{p),} \hlkwc{ncol}\hlstd{=p)}
\hlstd{beta_truth} \hlkwb{<-} \hlkwd{runif}\hlstd{(p,} \hlopt{-}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{)}
\hlstd{f_truth} \hlkwb{<-} \hlstd{X}\hlopt{%*%}\hlstd{beta_truth}

\hlcom{# create result object}
\hlstd{result_list} \hlkwb{<-} \hlkwd{vector}\hlstd{(}\hlstr{"list"}\hlstd{, nr_sims)}

\hlkwa{for}\hlstd{(sim_nr} \hlkwa{in} \hlstd{nr_sims)}
\hlstd{\{}

  \hlcom{# create response}
  \hlstd{y} \hlkwb{<-} \hlstd{f_truth} \hlopt{+} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{sd} \hlstd{=} \hlnum{2}\hlstd{)}

  \hlcom{# >>> do something <<<}


  \hlcom{# save results in list (performance, time)}
  \hlstd{result_list[[sim_nr]]} \hlkwb{<-} \hlstd{add_something_meaningful_here}

\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
 \item Why is gradient descent maybe not the best option for optimization of a linear model with $L2$ loss? What other options exist? Name at least one and describe how you would apply this for the above problem. 
 \item Can we say something about the algorithm's consistency w.r.t. $\Pxy$, if $f^\ast \notin \Hspace$?
\end{enumerate}
}

\dlz

\aufgabe{Risk Minimizers for 0-1-Loss}{


Consider the classification learning setting, i.e., $\mathcal{Y}=\{1,\ldots,g\},$ and the hypothesis space is $\Hspace = \{ h:\Xspace \to \Yspace  \}.$
%
The loss function of interest is the 0-1-loss:
%
$$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1, \quad \text{ if } y \ne \hx, \\ 0, \quad    \text{ if } y = \hx.  \end{cases}
  $$
%
%
\begin{enumerate}
	%
	\item Consider the hypothesis space of constant models 
	%
	$\Hspace = \{ h:\Xspace \to \Yspace \, | \,  \hx = \bm{\theta}\in \Yspace   \ \forall \xv  \in \Xspace  \}.$
	%
	where $\Xspace$ is the feature space.
	%
	Show that 
	%
	$$\hxh = \text{mode} \left\{\yi\right\} $$
	%
	is the empirical risk minimizer for the 0-1-loss in this case.
	%
	\item What is the optimal constant model in terms of the (theoretical) risk for the 0-1-loss and what is its risk?
	%\bar{h}(\xv ) = \argmax_{l \in \Yspace} \P(y = l ),
	% 1 - \max_{l \in \Yspace} \P(y = l)
	\item Derive the approximation error if the hypothesis space $\Hspace$ consists of the constant models. 
	% 1 - \max_{l \in \Yspace} \P(y = l) - (1 - \E_x \left[\max_{l \in \Yspace} \P(y = l~|~ \xv = \xv)\right])
	\item Assume now $g=2$ (binary classification) and consider now the hypothesis space of probabilistic classifiers 
	%
	$\Hspace = \{ \pi:\Xspace \to [0,1]    \},$
  %
  that is, $\pi(\xv )$ (or $1-\pi(\xv )$) is an estimate of the posterior distribution $p_{y|x}(1 ~|~ \xv )$ (or $p_{y|x}(0 ~|~ \xv )$).
  %
  Further, consider the probabilistic 0-1-loss 
  %
  \begin{align*}
%  	
%  	L: \Yspace \times \Hspace &\to \{0,1\}  \\
%  	
%  	(y,\pix) &\mapsto 
  	\Lpixy = \begin{cases}
  		1, & \mbox{if ($\pix\geq 1/2$ and $y=0$) or ($\pix< 1/2$ and $y=1$), } \\
  		0, & \mbox{else. }
  	\end{cases}
%  	
  \end{align*}
%
Is the minimum of $\E_{xy}[\Lpixy ]  $ unique over $\pi \in \Hspace$\footnote{If it is unique, then the loss is a strictly proper scoring rule.}? 
%
Is the posterior distribution $p_{y|x}$ a resp.\ \emph{the} minimizer of $\E_{xy}[\Lpixy ]?$
%
Discuss the corresponding (dis-)advantages of your findings.
%

\emph{Hint:} First note that we can write $\Lpixy = \mathds{1}_{\{ \pix\geq 1/2  \}}  \mathds{1}_{\{ y=0 \}} + \mathds{1}_{\{ \pix< 1/2  \}} \mathds{1}_{\{ y=1 \}}  $ and then consider the ``unraveling trick'': $  \E_{xy}[\Lpixy ]  = \E_x \left[\E_{y|x}[\Lpixy ~|~ \xv =\xv ] \right]. $ 

\end{enumerate}
  
}

\dlz

\aufgabe{Risk Minimizers for Generalized L2-Loss}{




Consider the regression learning setting, i.e., $\mathcal{Y} = \R,$
and assume that your loss function of interest is $\Lxy= \big(m(y)-m(\fx)\big)^2,$ where $m:\R \to \R$ is a continuous strictly monotone function.
%

\textbf{Disclaimer:} In the following we always assume that $ \var(m(Y))$ exists.
%
\begin{enumerate}
	%
	\item Consider the hypothesis space of constant models 
	%
	$\Hspace = \{ f:\Xspace \to \R \, | \,  \fx = \bm{\theta}  \ \forall \xv  \in \Xspace  \},$
	%
	where $\Xspace$ is the feature space.
	%
	Show that 
	%
	$$\fxh = m^{-1}\left(  \frac1n \sum_{i=1}^n m(\yi)  \right)$$
	%
	is the optimal constant model for the loss function above, where $m^{-1}$ is the inverse function of $m.$
	
	\emph{Hint:} We can obtain several different notions of a mean value by using a specific function $m,$ e.g., the arithmetic mean by $m(x)=x,$ the harmonic mean by $m(x)=1/x$ (if $x>0$) or the geometric mean by $m(x)=\log(x)$ (if $x>0$).
	
	%
	\item Verify that the risk of the optimal constant model is $\risk_L\left(\fh \right) = \left( 1 + \frac1n \right) \var(m(y)).$
	%
	\item Derive that the risk minimizer (Bayes optimal model) $\fbayes$ is given by $\fxbayes=m^{-1} \left(  \E_{y|x}\left[ m(y) ~|~ \xv \right] \right).$
	%
	\item What is the optimal constant model in terms of the (theoretical) risk for the loss above and what is its risk?
	%
	\item Recall the decomposition of the Bayes regret into the estimation and the approximation error.
	%
	Show that the former is $\frac1n \var(m(y)),$ while the latter is $\var\Big(  \E_{y|x}\left[ m(y) ~|~ \xv \right] \Big)$ for the optimal constant model $\fxh$ if the hypothesis space $\Hspace$ consists of the constant models. 
	
	\emph{Hint:} Use the law of total variance, which states that $ \var(Y) = \E_X\left[\var(Y~|~X)\right] + \var(  \E_{Y|X} \left[ Y ~|~ X \right] ),$ where the conditional variance is defined as $\var(Y~|~X) = \E_X \left[  \big(Y - \E_{Y|X}(Y ~|~X)\Big)^2 ~|~X \right].$
  
\end{enumerate}
  
}

\dlz

\aufgabe{Connection between MLE and ERM 1}{

Imagine you work at a car dealer and are tasked with predicting the monthly number of cars
that will be sold within the next year. You decide to 
address this challenge in a data-driven manner and develop a model that predicts 
the number of cars from data regarding vehicles’ properties from sold cars of previous years, 
current competitor and market data.

\begin{enumerate}[a)]
  \item Let $x_1$ and $x_2$ measure the number of sold cars of the previous month and of the previous year, respectively. 
  Both features and target are numeric and discrete. You choose to use a generalized linear model (GLM) for this task.
  For this, you assume the targets to be conditionally independent given the 
  features, i.e., $\yi|\xi \perp \yi[j]|\xi[j]$ for all $i,j \in 
  \{1, 2, \dots, n\}, i \neq j$, with sample size $n$.
  \begin{itemize}
  \item Argue which of the following distributions from the one-parametric exponential family is most suitable for the underlying use case: normal, Bernoulli, gamma or Poisson.
  \item Write down the probability distribution of the chosen distribution depending on $\thetab$ assuming a log link function.
  \end{itemize}
  
  %The GLM models the target as a linear function of the features 
  %with Gaussian error term: $\ydat = \Xmat \thetab + \epsilon$, \\ 
  %$\epsilon \sim N(\bm{0}, \mathit{diag}(\sigma^2)), ~~ \sigma > 0$.
  % Furthermore, you have reason to believe that the effect of mileage might be 
  % non-linear, so you decide to include this quantity logarithmically (using the 
  % natural logarithm).
  
  \item State the hypothesis space for the corresponding model class.
  For this, assume the parameter vector $\thetab$ to include the intercept 
  coefficient.
  \item Which parameters need to be learned?
  Define the corresponding parameter space $\Theta$.
  %\item State the loss function for the $i$-th observation using $L2$ loss. 
  \item In classical statistics, you would estimate the parameters via maximum 
  likelihood estimation (MLE). 
  %The likelihood for the LM is given by:
  % \[
  % \ell(\thetab) = - \frac{n}{2} \log(2 \sigma^2 \pi) - \frac{1}{2 \sigma^2} 
  % (\ydat - \Xmat \thetab)^T(\ydat - \Xmat \thetab)
  % \]
  
  % \\
  % &= \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{n}{2}} \exp \left(- 
  % \frac{1}{2 \sigma^2} \sumin \left(\yi - \thetat \xi \right)^2  \right) \\ 
  % &= \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{n}{2}} \exp \left(- 
  % \frac{1}{2 \sigma^2} \| \ydat - \Xmat \thetab \|^2 \right)
  Describe how you can make use of the likelihood in empirical risk minimization 
  (ERM) and write down the likelihood as well as the resulting empirical risk.
  %\item Now you need to optimize this risk to find the best parameters, 
  %and hence the best model, via empirical risk minimization. 
  %State the optimization problem formally and list the necessary steps to solve 
  %it. 

\end{enumerate}



}

\dlz

\aufgabe{Connection between MLE and ERM 2}{


Suppose we are facing a regression task, i.e., $\mathcal{Y} = \R,$ and the feature space is $\Xspace \subseteq \R^p.$ 
%	
Let us assume that the relationship between the features and labels is specified by  
%
\begin{align} \label{eq_relationship}
%	
	y = m^{-1} \left( m(\ftrue(\xv)) + \eps \right),
%	
\end{align}
%
where $m:\R \to \R$ is a continuous strictly monotone function with $m^{-1}$ being its inverse function, and the errors are Gaussian, i.e. $\eps \sim \mathcal{N}(0, \sigma^2)$. 
%
In particular, for the data points $(\xv^{(1)},y^{(1)}),\ldots,(\xv^{(n)},y^{(n)})$ it holds that
%
\begin{align} \label{eq_relationship_data}
	%	
	\yi = m^{-1} \left( m(\ftrue(\xi)) + \epsi \right),
	%	
\end{align}
%
where $\eps^{(1)},\ldots,\eps^{(n)}$ are iid with distribution $\mathcal{N}(0, \sigma^2)$. 
%Then $$m(y)~|~\xv \sim N\left( m(\ftrue(\xv)), \sigma^2\right).$$

\textbf{Disclaimer:} We assume in the following that $m(y)$ and $m(f(\xv))$ is well-defined for any $y\in \Yspace,$ $f\in \Hspace$ and $\xv \in \Xspace.$

\begin{enumerate}
%	
	\item  How can we transform the labels $y^{(1)},\ldots,y^{(n)}$ to ``new'' labels $z^{(1)},\ldots,z^{(n)}$ such that $z^{(i)}~|~\xv$ is normally distributed? What are the parameters of this normal distribution?
%  
	\item Assume that the hypothesis space is 
	%  
	\begin{equation*}
    \begin{split}
      \Hspace = \{f(\cdot~|~ \thetab): \Xspace \to \R \ ~|~   & f(\cdot~|~ \thetab) \text{ belongs to a certain
       functional family parameterized by } \thetab \in \Theta \},
    \end{split}
  \end{equation*}
  %
  where $\thetab = (\theta_1, \theta_2, \ldots, \theta_d)$ is a parameter vector, which is an element of a \textbf{parameter space} 
  $\Theta$.
	%
	Based on your findings in (a), establish a relationship between minimizing the negative log-likelihood for  $(\xv^{(1)},z^{(1)}),\ldots,(\xv^{(n)},z^{(n)})$ and empirical loss minimization over $\Hspace$ of the generalized L2-loss function of Exercise sheet 1, i.e., $\Lxy= \big(m(y)-m(\fx)\big)^2.$ 
%   
	\item In many practical applications such as biology, medicine, physics or social sciences one often observed statistical property is that the label $y$ given a feature $\xv$ follows a \emph{log-normal distribution}\footnote{The Wikipedia article on the log-normal distribution has quite a large part about the occurrence of the log-normal distribution.}.
%	
	Note that we can obtain such a relationship by using $m(x)=\log(x)$ above.
%  
	In the following we want to consider the conjecture of the Scottish physician James D.\ Forbes, who conjectured in the year 1857 that the relationship between the air pressure (in inches of mercury) $y$ and the boiling point of water $x$ (in degrees Farenheit) is given by
%	
	$$  y = \theta_1 \exp(\theta_2 x + \eps),$$
%	
	for some specific values $\theta_1 \in \R_+,\theta_2\in \R$ and some error term $\eps$ (of course, we assume that this error term is stochastic and normally distributed).
%	
	\begin{itemize}
%		
		\item What would be a suitable hypothesis space $\Hspace$ if this conjecture holds?
%		
		\item The dataset \texttt{forbes} in the R-package \texttt{MASS} contains 17 different observations of $y$ and $x$ at different locations in the Alps and Scotland, i.e., the data set is $(x^{(i)},\yi)_{i=1}^{17}.$
%		
  Analyze whether his conjecture was reasonable by using the following code snippet:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#' @param X the feature input matrix X}
\hlcom{#' @param y the outcome vector y}
\hlcom{#' @param theta parameter vector for the model (2-dimensional)}

\hlcom{# Load MASS and data set forbes}
\hlkwd{library}\hlstd{(MASS)}
\hlkwd{data}\hlstd{(forbes)}
\hlkwd{attach}\hlstd{(forbes)}

\hlcom{# initialize the data set}
\hlstd{X} \hlkwb{=} \hlkwd{cbind}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{17}\hlstd{),bp)}
\hlstd{y} \hlkwb{=} \hlstd{pres}

\hlcom{#' function to represent your models via the parameter vector theta = c(theta_1, theta_2)}
\hlcom{#' @return a predicted label y_hat for x}
\hlstd{f} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{theta}\hlstd{)\{}

  \hlcom{# >>> do something <<<}

  \hlkwd{return}\hlstd{(y_hat)}

\hlstd{\}}

\hlcom{#' @return a vector consisting of the optimal  parameter vector }
\hlstd{optim_coeff} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{X}\hlstd{,}\hlkwc{y}\hlstd{)\{}

  \hlcom{# >>> do something <<<}

  \hlkwd{return}\hlstd{(theta)}

\hlstd{\}}

  \hlcom{# >>>  Do something here to check Forbes' conjecture <<<}
\end{alltt}
\end{kframe}
\end{knitrout}
%
	\emph{Hint:} As a sanity check whether your function to find the optimal coefficients work, it should hold that $\hat\theta_1 \approx 0.3787548$ 	and $\hat\theta_2 \approx 0.02062236.$
	%		
	\end{itemize}
%
\end{enumerate}
}


% % ------------------------------------------------------------------------------
% % PAST EXAMS
% % ------------------------------------------------------------------------------
% 
% \dlz
% \exexams
% \lz
% 
% % ------------------------------------------------------------------------------
% % INSPO
% % ------------------------------------------------------------------------------
% 
% \dlz
% \exinspo
\end{document}
